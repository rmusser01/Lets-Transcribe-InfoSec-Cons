{
  "webpage_url": "https://www.youtube.com/watch?v=DT3eUlt379E",
  "title": "TROOPERS24: Open Source Vulnerability Management for Software Development and Cloud Environments",
  "description": "Talk by Stefan Fleckenstein - June 26th, 2024 at TROOPERS24 IT security conference in Heidelberg, Germany hosted by @ERNW_ITSec\n\n#TROOPERS24 #ITsecurity \nhttps://troopers.de/troopers24/talks/vkfkeu/\n\nMore impressions:\nhttps://twitter.com/WEareTROOPERS\nhttps://twitter.com/ERNW_ITSec\nhttps://infosec.exchange/@WEareTROOPERS\nhttps://infosec.exchange/@ERNW https://ernw.de",
  "channel_url": "https://www.youtube.com/channel/UCPY5aUREHmbDO4PtR6AYLfQ",
  "duration": 3523,
  "channel": "TROOPERS IT Security Conference",
  "uploader": "TROOPERS IT Security Conference",
  "upload_date": "20240902"
}

This text was transcribed using whisper model: large-v2

 Thank you very much.  People are still a bit tired maybe after the lunch break.  I always get this lots after the lunch break and I hope it will be interesting for you.  So we'll talk about open source vulnerability management.  And if you read the synopsis in the agenda, it's about software development and cloud  environments mostly, where what we don't touch will be like on-premise classical infrastructure.  So it will be mainly software development and cloud.  Let me start who am I?  Stefan Fleckenstein.  I work for Mayborn Wolf.  Mayborn Wolf is a German software consultancy and software development company.  We build software, individual software for clients in Germany.  Lots of well-known, some not so well-known companies in Germany and their customers.  And we do this with a full-stack approach.  We have data, we have mobile applications, we have web applications, we have IoT, we  have smart devices.  We start doing a lot of AI.  And there are a lot of cybersecurity is needed these days as well.  And so I'm head of the cybersecurity department at Mayborn Wolf.  So we don't provide cybersecurity for our own company, but for our customers where we  do consultancy about cybersecurity or implement security-related software.  What are my passions?  I have three main passions which are relevant today here.  So this is, I did a lot of software development in my life, more than 25 years.  In different consultancies, I started as a software developer, software architect, managed  software projects, stuff like this.  Then the last six years I've done mostly cybersecurity.  It started in an internal role at Mayborn Wolf where we started to do a lot of cybersecurity  there and passed the ISO 2701 certification, which I was responsible for.  And there I really developed a passion for cybersecurity, did a lot there, which then  led to founding the department for our customers four years ago, in the middle of the pandemic  it was.  And the third passion is open source.  I think the open source stuff is really great.  There are lots of tools and it's a lot of fun.  Work with open source communities because you give something and you get a lot back.  And this I really like.  And because I'm not doing software development for money anymore in my company, I do it as  an open source developer these days.  And so I was two years a maintainer of Defect Dojo, that's an open source vulnerability  management with an OWASP flagship project.  And then roughly two years ago, I started my own project, SecObserve, which we will  see later a little bit more.  All right, what we'll do in the next roughly an hour.  So we start with some motivation.  Why are we here?  Why are we listening to this talk?  We'll talk a bit about how do we do security testing in software development and then see  how does this open source approach work there and ask the question, well, does it really  work?  To give you some confidence there.  So let me start with four cases from the last years.  I think a lot of you heard about Lock4Shell.  This was, I think, December 2021, short before Christmas, where Lock4J, which is one of the  most popular Java libraries, which is used for logging, became known that attacker can  load and execute malicious code via the JNDI interface.  And there was immediately a wide range exploitation of this.  So there was Bitcoin mining, there were ransomware attacks, and everyone was running around and  not having a nice Christmas time, but running around.  Are we affected?  Is the software affected that we build?  Is the software affected that we use?  Are systems affected that we use, like the locking system of the doors, maybe?  Nobody knew.  And this is really bad in a way.  I'll come to this later.  Moveit last year, Moveit is a tool that does file transfer, is often used for sensitive  data like HR systems.  And there was an SQL injection in the software.  And there's a blog which describes how that happened.  And it's a really classical SQL injections, where you think these days, can this really  happen anymore?  And yes, it can.  And so there were ransomware attacks like on the BBC, the British Airways, the Nova  Scotia Bank and others, where a lot of data was given to ransomware attackers.  Not good as well?  Detectable?  Yes.  And I found an interesting case in the research, there were four airports in Colombia and Peru,  which had three terabytes of data in an S3 bucket at Amazon, which was publicly available.  Everyone who knew it, who found it, could access it.  And there were personal data of employees in there.  But other sensitive data about layout of the airports, where are fuel lines and stuff like  this in there, was completely public.  Luckily, it was discovered by security researchers.  If you search for it, you can find it.  It was not known that it was actually exploited, but imagine what could have happened there  for these airports.  Fourth case, from this year, quite new, a Microsoft employee published credentials in  a Git repository, I think it was on GitHub, that granted privileged access to an internal  Azure Container Registry, where they have internally, or should have been internally,  I mean, it's in the cloud, but nobody should have access, where they had their containers  for several of their software, and it was available.  I think no exploitation here as well.  But what if some attacker uploads a malicious container of teams or something like this?  Not good as well, and all of these cases are avoidable, and we come to these later.  How does actually testing work?  In general, we have four ways how we test software.  For security, we can do code reviews.  So for every merge request, we do often code reviews anyway, for software quality, that  it adheres to the architecture, that it works, stuff like that.  Or when we do pair programming, the two pair programmers do it against themselves.  And so we should look for possible security vulnerabilities as well there.  But there's a good chance to actually miss some problems, because it's cumbersome, you  have to look at a lot of things.  There is a guide from OWASP, a secure code review guide, it has more than 200 pages.  I don't know who writes so long documents, and pretty sure more or less nobody reads  these long documents.  So it is important to do it, to look at the code, but yeah, you will miss things.  Then there are functional tests, stuff like authorization, authentication, input validation,  that can really be tested very well with unit tests, integration tests.  We do system tests on larger levels, load tests to see how it behaves.  So this is really good to do, but it doesn't show that you have vulnerabilities, it just  shows what you have implemented, you have implemented the right way, which is good.  Automation tests.  In our company, we see that our customers are not doing them really regularly.  It's typical for an initial release, not for a lot of systems we build, the customer says  no, we don't really need it.  So it's more the critical systems, and then, yeah, when we start, when we go into production  the first time, when there are larger changes, but it's not regularly.  And it's good, again, outside view of the system, you find things, but some things still  you might not find.  And so the final piece in the puzzle is the vulnerability management, which typically  runs in the CICD pipeline, or you run it regularly, every night or stuff like that, and where  tools are used to cover different perspectives, which we will see later in this talk.  And I think if you have all these four things together, then you have a really good chance  to build a secure product, and it gets more and more important, not only because the software  we build, the software we are using gets attacked, but there are a lot of regulations now.  We have heard about from the guys from Audi here before, about regulations, they have  a tier 2, we have the Cyber Resilience Act in the European Union soon, we have NIST 2,  and we see that lots of areas are now covered by different regulations.  And they all say something about vulnerabilities, that you shouldn't ship your products with  known vulnerabilities.  So how does it work?  An overview for the start, we have a developer, he writes source code, develops things, commits  them and pushes it into a source code repository.  A CICD pipeline runs, does tests, builds the software, deploys the software, and as part  of that, there should be vulnerability scanners running to see if they find anything in the  code, in the libraries, and other stuff there.  They get imported into a vulnerability management tool, so that people can actually have a look  at it, like the developer or other stakeholders, and can see, oh well, there is stuff in there  and we should do something about it.  And optionally, you could push it into an issue tracker, so when you use an issue tracker  like Jira, GitLab, GitHub, or anything else, push it in there, so it's part of your product  backlog to say, okay, these things need to be cared for.  This goes back to the developer, and then you have a workflow where you find things,  where you can detect things and care for things.  So we have five steps that I think are really important, and we start with number one, very  important, avoid vulnerabilities.  Because every vulnerability that's not in the system will not be found and must not  be cared for, because you will see later, every vulnerability requires some work that  needs to be done, and we don't want to do that.  So security by design, really important to start it very early in the process, to see,  to have your, I don't know a proper English term for it, in German it's Schutzbedarfsanalyse,  couldn't really find a proper English word, so I use protection needs, where you say,  what kind of data do I have in my system, and what will be the impact if you breach  confidentiality, integrity, or availability, what will really happen?  Is it just a minor thing that you can easily mitigate, or is it an existential threat to  the company, maybe, if this data gets lost or is not available again?  So then you know the impact from a threat modeling, threat analysis, you find out where  is my system, where can it be attacked, what kind of attackers could there be, and what  is the probability of, is it really easy, because there might be things that a script  QD can do, or do we really need to get prepared if some state actors might attack us?  And so with risks, so you can have risks from impact and probability, and so you know your  risks, then you can find your security requirements and the measures that need to be done to build  a secure system, they can be part of the architecture, how you build things, use OIDC for authentication  and stuff like this, they can be part, they can be in user stories, like I have a user  story, write an authorization role based with these requirements, or they can be in the  definition of done, saying yeah every service must have authentication and every service  must be TLS encrypted and stuff like this, so then all the requirements are there, and  you can start building a secure system.  So next, automated page management, keep your things updated, you have libraries, you have  Docker images, keep them really updated regularly, and there are tools for it, like the Renovate  Bot or the Dependabot to automate this, they look at your package.json, poetry.log, stuff  like this, in your Docker files and say there's a new version of this library, this library,  generate a merger push request for it, and then you can deal with it, and this is really  important to do it really regularly, because then you are used to doing it, and if there  is something where you have to react very fast, like with look4shell, then you're really  used to do it.  You have the process, it's not that you haven't changed anything the last three months and  have problems doing it, and do the major changes as well, they are a bit hard often, then you  have something that's in version 4 and now it gets to version 5, that's not just accepting  the push request, it's much more work, still often really helpful, because if in an old  version there is a vulnerability detected and then you have to go to version 5 in a  hurry and there's a lot of work involved, have a problem again.  Keep it simple, as few libraries as possible, as many libraries as necessary, there are  some things you shouldn't build yourself, like authentication, you should always have  libraries for this, people have made it better than you can, but not everything needs a library,  some things might be easier to do it yourself and then you know they are fine, and use small  base images, as small as possible, like Alpine, Distroless stuff, when you have a normal Debian  based image, they often contain a lot of vulnerable things that might not be relevant for you,  but you have to look at it.  This helps, and last in this list, security champions, this is a concept, if you don't  know it, where you have one person in your team who really cares about security, who  knows what needs to be done and cares that things get done concerning cyber security,  the tests are done, that all the processes have been followed, they must not do everything  themselves, but they really care about and see and have a look at it, that there are  some things in the product backlog, they push the product owner not to only do functional  things but care for these things as well, really helpful, and training of programmers  really helps that they avoid many pitfalls they could do from the beginning, just because  they know how to do things, and I think with these four things, you will already build  a very good and secure system from the start.  So what needs to be checked?  So yeah, we have the vulnerabilities and dependencies like Docker images, your NPM, Maven, other  stuff, you have the cloud configuration in your cloud, hopefully you have it all as infrastructure  as code, so this can be checked.  The vulnerabilities that might be in the application code you should know, and if you have any  secrets in the source code repository, so I want to introduce some names here, so this  all checking of dependencies is often called SCA, software composition analysis, we will  see this on later slides, this abbreviation.  We have SAST, what is that, static application security testing, these are tools that basically  read the source code and do a code review, they have a lot of rules there and can detect  things then in the code that something is missing, or dynamic application security testing  where the system really runs and there are test cases often automatically generated that  run to see if it's secure from this point of view.  And yeah, log4shell was a good example for the first one because there are always, in  your dependencies, there are always things you will find and log4shell was a good example  for this to say you should know about them and react as quick as possible or as quick  as necessary depending on the risk.  The airport example of the Colombian and Peruvian airports was a good thing for cloud  configurations if you use infrastructure as code and there are some dynamic tools as well  we can see later, then you would find there's an open S3 bucket, you will find that and  say this should be closed, this can be easily detected actually.  Yeah, vulnerabilities in the source code, the moveit example and SQL injection is really  easy to find, so this SAS tools, they find this stuff very easily typically and so would  be completely avoidable to have this in the source code.  And secrets, yeah, the container registry, the guy who accidentally checked something  in into the git repository, this stuff happens all the time and so there are tools to detect  this and that you don't have anything in there and even if you say well it's my company  git repository, is it that important?  But you don't know how many people have access to this and maybe they read something, leave  the company and then can exploit it, so even if in your own internal repositories there  should nothing be in there what is really a secret.  So step two, use a curated list of vulnerability scanners, we've done this, we used a lot of  tool in our company and have a list, this is mainly the list of things that really work,  so for to check libraries, there's dependency check for mainly Maven, there's dependency  tracker tool that works on S-bombs with its own user interface, scribe and trivia, this  typical kind of tools that go over your package.json and other stuff and docker containers, look  at the docker containers, see what's in there and where are newer versions and where do  you have any CVEs or other known vulnerabilities in there.  SAST is often specific for the programming language, so bandit checks Python code, ASLint,  JavaScript, TypeScript, findsecbox, Java, there are tools for Go and others as well,  typically you will find an open source tool for every programming language or most of  them.  SMgrab is a tool that there's an open source version and a paid version, but the open source  version is quite good, that can work with a lot of programming languages and test and  special rule sets for different kind of environments, SAST, there are some tools to check stuff  like terraform, your docker files, docker compose files, Helm charts like Chekhov, Kix  or Trivi again, Trivi we see more and more often for different things.  For secrets we use Gitleaks a lot, although there are other tools as well.  Dynamic, OWASP ZEP, lots of you will know as an open source tool that can do a lot of  things there, partially manually, but partially they can do stuff automated in the pipeline  as well, some basic and some more elaborated checks.  There's a tool docker header, I think it's written by Santander initially, that checks,  it's in the name, HTTP headers, if they are set correctly, I think this is a science on  its own how to set these headers and there are some good guides for it, for example from  OWASP and they check, you can have your own rules, there's a standard set of rules and  it says yeah you should have this and this and maybe you should have different settings  and Cryptolizer checks the whole TLS encryption of your connections and says what it found  there and we use it in a way that we check it against the BSI norm or you could do the  NIST norm as well and see are there things that violate this current regulations there.  And then there are some other tools that are not really in the, that works a bit different,  like Prowler, Azure Defender, they really need access for your infrastructure for the  cloud, they need a cloud access and then check, have a lot of rules, so they can be used if  you don't use infrastructure as code, you could still use these kind of tools and there's  someone working currently, Trivy can work in the Kubernetes cluster as well to check  all the configuration there and then you get the data out via Prometheus and access  this via an API, this is a method as well, so there are a lot of tools and there are  more around as well that you could really use, that's great.  Problem, you have to integrate when you, this can be really tedious integrating these different  scanners because they're all installed differently, sometimes they have just the Docker image,  sometimes they have a script how to install it, sometimes it's a binary you download from  GitHub, it's all different and when you call them they have completely different parameters,  so you have to check every manual to see what parameters you can set, how you set it, it's  not very much fun, so what we did together with the SecObserve tool to have a repository  with GitLab CI templates and GitHub Actions, they provide uniform methods for launching  the tools, a unified set of parameters there and so then it gets really easy to try different  tools to see the outcome and so there's a huge Docker image with it where you can use  things and it gets regularly updated, typically once a month, so then you have the latest  features, bug fixes in these tools available as well.  How does it look like?  This is part of a GitLab pipeline, you include a template in GitLab for the vulnerability  scanner that runs in the stage test and you need only a few lines because here it says  yeah extend the vulnerability scanner template you have just included and use a configuration  that I'll show in a moment and that's done, that's all you need to do in a GitLab pipeline  and for GitHub with the GitHub Actions, the pre-made actions is basically the same idea,  the same complexity to do this.  You could have templates if you just have one or two of the tools, you could have single  templates as well.  And then the configuration file looks for example like this, this is a YAML file where  you have a section for each scan you want to do, you say which kind of scan I do, you  want to use like bandit or trivvy image, trivvy can scan container images or gradle  files or package log or stuff like this and be great trivvy file system.  You say where should it run, what's the target and the report name, so it writes the report  as a Sarif, it's a JSON format, a special kind of JSON format and so they write then  a file and this file then is part as an artifact, you could download it and have a look at it  but then there's an importer section that says yeah do an upload to a SecObserve instance  where you then can have a look at it as well.  And so this is really easy to be done, not much effort and then you get results very  soon and can start to work on it.  Yeah that's the thing, all the scanners they write JSON files and you could just run the  scanners and I've seen people just running the scanners and you can break your build,  the pipeline to say if it finds something that's high or critical then stop everything.  Always not sure if it's a really good idea if you have a critical bug fix to do and then  something gets detected and the pipeline says no I don't do it anymore and yeah looking  at this JSON files and have a look what's happening is not much fun and nobody does  it.  And so then they get, I've seen customers they set up this pipeline and then never look  at it again anymore and this is not what makes sense.  So the file should get imported in an open source vulnerability management tool like  SecObserve for example which I will show and that's really easy because they are API wrappers  so this is part of the GitLab templates, GitHub actions but there are other tools that  have other vulnerability management tools that have API wrappers as well.  So final step and it's the longest step then assessment and remediation.  So we have imported it into our system, now we can start working on what kind of vulnerability  management systems are out there.  I think basically really these three, there's Dependency Track which is an OWASP flagship  project, it's very mature, started seven years ago, a good single page application, really  easy to use but this is really specialized.  It reads S-bomb files in Cyclone DX and does SCA, software composition analysis and checks  for licenses and they could do license checks in there with your own rules.  It's a very good tool but it's very specialized, it does one thing, this one thing it does  really good.  There's Defect Dojo, very mature as well, an OWASP flagship project as well.  This has a really complex user interface where I would say this is more suited for  security specialists, you would not have programmers and other people working with it, there you  have a special security team that really cares for things, then it might be a good idea to  use it.  I was a maintainer of this for two years and know its strength and some weaknesses as well  but it's a good tool.  You can use it and it has hundreds of scanners that you can integrate there and there are  a lot of stuff that cares for on-prem and other stuff like burp and all the things,  lots of commercial scanners, there's a lot of support in there.  If you do more than software development and cloud environments you might want to have  a look at this but we will now talk about SecObserve because basically a tool that we  have written in my company that I am responsible for but it's open source so there are no limitations,  we don't use a better version than you can download, everything is open source there.  It's pretty new, started last year, a modern clean single page application we will see  but it has this focus on software development, maintenance of cloud-based software systems  so the list of scanners we saw are basically what is supported plus everything that can  write the ZARIF format because lots of SAS scanners write ZARIF, there's one JSON format  for SAS that is supported by GitHub as well and so lots of scanners use this, they can  import it and if you have your vulnerabilities in an SBOM you can import them as well from  lots of other tools.  What will you get?  So what we've seen is there are some vulnerability scanners that import their vulnerabilities  in there, there's an OpenID Connect authentication so you can have your own users but you can  have your OIDC server like EntraID, Google or others connect to it that have your users  in there and with a role-based authorization model behind it so everyone sees or can do  only what they're allowed to do and see.  We have some EPSS scores, this is a score about the probability that things got exploited  already, this can be added so that you get an idea of the risk, is this just a vulnerability  or does it get really exploited already and then some more integration, VEX files are  a way to express vulnerability status so you could say oh I know there's a vulnerability  but the vendor has said that's not relevant because it's a false positive or it cannot  be exploited.  These files can be written and imported, you can export stuff to Excel, CSV to get it in  other systems.  We don't have the code in the system so there are code links to several code repositories  and you can generate issues, not in DevOps at the moment but GitHub, GitLab, Jira, Cloud  you can export the issues surrounded and they get then tracked there as well so if  someone changes in the vulnerability the issue gets changed as well.  Notifications if something happens, you have a new vulnerability that's really critical  you can send a message to Teams, Slack or via email and then some information links  to NVD for GitHub advisories, open source insights is a helpful tool where you can see  more information.  So you get a very good infrastructure around it that really makes things easy.  So let's have a look at it, how it really looks.  This is how the system looks.  We start with the dashboard where we see some graphs about severities, about status, what's  going on, how the timeline is.  Then we have products, I have here a demo product RUPES 2024 which has, you can already  see some open observations, we call it observations here in the tool because they are detected,  some things have been detected and imported but if there are really vulnerabilities we  don't know yet, it's just something that has been observed for the moment.  And then you get a list of different things that are coming from different scanners and  so you have everything in one list and you don't care if it's SAS, Dust, Secret, Infrastructure,  open source code, it's all in one list and the idea is it looks, you have a similar look  and feel for every tool so you don't hear people who just use this don't care about  different scanners that have generated the observations, it's always a similar look,  sometimes with more or less information, that really depends then on the scanner, what the  scanner actually gets.  We always have a severity and a status, severity is critical, high, medium, low and informational  status we'll see in a moment.  We have a description that comes from the scanner, the scanner has a recommendation,  we have it here, sorry we have a vulnerability information so for dependency we see the CVE  and we could directly look at it at the CVE database of the NVD which currently is more  or less helpful, like CVS scores and stuff like that, the EPSS score, we see where it's  coming from, it's a cryptography 4105, we could get more information about this.  We see where it's coming from, so how is it integrated in the system, so there's a library  called Encrypted Model Fields that uses this library and this is often helpful to do your  research about it.  We see a log if things change, some meta information, references, so you get a lot of information  when you start to look at things, you can really look at a lot of information around  it and some extracts of the original reports, like here it is coming from a S-bomb, you  can see the relevant extracts of this S-bomb as well if you want to see a bit more of it.  When we look at others, for example, something coming from a Dockerfile, it says, ensure  that a user for the container has been created, if you have a code snippet in the report then  we show it here as well, unfortunately the check of parser then gives us the whole Dockerfile,  it says ensure that a user for the container has been created, again we see the origins  and when we have a source code repository specified then we have to link to the source  code as well, where it then goes with the line numbers, it's not properly configured  here at the moment.  Or for example, the hardcoded SQL injections, this is not a real application, I really put  it in the source code, something like a query that is hardcoded and this is roughly really  what happens with the moveit tool and so to see, yeah, they are found easily and you can  have a look at it and you can change it.  So really see a lot of things here and then can start to actually work on it and we see  we have a lot of information, it looks similar, as I said we could export it to different  formats, the metrics and we could do manual imports as well, the idea is you get your  scans in a pipeline but you can import stuff as well from files and partially from APIs  like if you use dependency track for SCA you could have an API call to that.  You can work with branches and versions, not configured here but you can say I have my  main branch, dev branch or different versions where I have the data separated so there are  lots of ways how to work with the system.  All right, now you have it all there, that's good for a start, but now you have to do an  assessment of it, so you want to eliminate undesired results, you can have false positives,  things that get detected where you say, nah, that's not true.  You could have things where you say, no, I'm not affected, it's there but I know the reason  there's a library but how I use it, this vulnerability cannot be exploited or sometimes they report  stuff that's just not security related and so you can eliminate, you can change the configuration  of the scanner, say the SAS scanner should not check test directories because we don't  care about them, you could do this in the configuration of the scanner, you can do an  assessment per observation or you can make a rule-based assessment which I will show.  So then you have the list that are really relevant, you could accept risk, you can say  okay I know the risks, I have my threat model with the risk and so I just say yes, I know  it's there and I know it might be exploitable but I accept the risks and it's documented  that I did it and tutored it and then you start to fix vulnerabilities, the remaining  things you might want to fix and then with the next scan, so you fix things, you check  it in and with the next scan the tool will say, oh yeah, I didn't find it in the next  report anymore, so it's set to resolved and it's not in the lists anymore, so you don't  have to resolve things there as well, the scanner sees it and with the next import it  automatically is resolved and out of the way when you have done your work, so we don't  have to care about that, let me show a few things how to do it.  So here a manual assessment, our hard-coded SQL expression, we could say, we could change  the severity or the status, I could say well no it's not affected or I say I put it in  review, we have to look at it, here it's pretty clear actually what happens and so I would  say I wouldn't change the status but I say, I say that's not medium, that's critical and  then it changes the status, if you see and it's done, you have a log entry who actually  set it, which user, when it was done, stuff like that, so it's all audible and so you  can do manual assessments, you can remove it as well when you see that was wrong, we  have stuff in there, so here is something where we have a false positive because Django  uses Python, Django has some functions that don't look like functions or where we have  a false positive, it says if super user function on attribute, it looks, because it doesn't  have the parentheses at the end but it actually is really a function and so there's a rule  about it saying every time you see something like this title with a regular expression  and coming from the same grep parser, say that's a false positive as a rule, so you  set this rule once and so you could do it with the title, with the origin, you have  several attributes where you can set rules on and say how does it change the status or  the severity when this happens, so you can automate things here as well and there would  be the vex files if you have, people would supply you a vex file saying yeah this is  not relevant and this could be imported as well, so you really get away how to deal with  things to make the list smaller, I would always say start with critical and high and if you  not haven't done it at the beginning you will have long lists but you can get them really  smaller quite fast and then have a reasonable amount that where you can really work and  you know what's happening if and you can accept the risk if you say that's too much  for me at the moment, you have visibility and if your management ask or something happens  you have an idea what's happening.  So does it work?  So they're all open source scanners, are they any good?  Do we know?  So we had a master thesis in our company two years ago I think, where we checked vulnerability  scanners against our security experts in the projects and seen, checked the false positives.  Do they get a lot of small false positive and it was less than I expected?  So the false positives that they give, so they don't give a lot of them, secured scanning  of the secrets, that's always a bit tricky that the scanners work with lots of different  rack access, they find a bit too much but it's always worth looking at it and the second  one is, are the quality, is the quality good, especially the severity and what we found  is all of the critical vulnerabilities, the tools found, our security experts said yeah  we really think that's critical and it should be done.  With the high it's not 100% but still 91% of the high vulnerabilities the tools found  our guys said yeah and it's a bit less for medium and low, so there we see our guys typically  said oh what's medium or low, they would see more as an informational thing, say well yes  maybe not that necessary to do but it was good to see that high and critical, the tools seem to  be really good to detect these things, so they seem to do a good job. Then you could say well  open source it's a good thing but there are a lot of commercial products there on the market,  what about this and lots of the scanners are either a respected community like OWASP ZEPP,  this is there for a long time, it's well supported, stuff like Trivi, GRIBE, other scanners are  actually from commercial companies, they use it in their commercial products where they have  fancy user interfaces as well but have their base scanners open source, so it is then the same  basically what you get what they use for their products and it can often be introduced faster,  we see it in our project when I come to a customer and say we should do this and then yeah I need a  vendor selection, these are quite expensive, you can say 50 to 100 dollar per user per month,  it's quite a lot of money and it's a lengthy process, you can start with this and do it quite  easily maybe for a small project or a larger project or in your department, you could just  start with it without procurement, vendor selection, all this stuff. On the other hand,  commercial products have their strengths, so the support for developers fix the vulnerabilities  are often better, they then have to list really how things develop in the source code,  how a variable is used and where it started and where it has been used, so they are quite good  in this what you don't get, they have strong user interfaces often because then they have  lots of developers behind them, I think SecObserve and DefectDojo and DependencyTrack have good user  interfaces, they can do often more, to say it, and I think AI might give commercial products  advantage in the future in identifying vulnerabilities, what is a vulnerability,  to get even better there and know they're working on it, companies like Snyk also to generate pull  requests that fixes them when you trust AI, I think you will always have to still have to look  at it, but this might change because I would not expect this in the open source world in the future.  So both, there are good products out there, but there are some you can use open source approach  as well and it has its own advantages, so I would say go for it, we have made very good experiences  in our company. That's the summary, I think I have 10 minutes or so. Yeah, these are the five  steps, avoid vulnerabilities, do things not to have vulnerabilities in the first place, use some  well-known good vulnerability scanners, use ready-made templates to make it easier for you,  import it into SecObserve or DefectDojo, and then start working on this. You have transparency,  you can do an assessment and then you can start eliminate, and when you do these five things,  you will have much more secure projects and make it a safer and a more secure world,  and that's what this conference is about. Thank you very much for your attention and  I think we might have time for a few questions then.  There's one here in the front.  Thank you, Stefan. So, how can AI help for this process, what do you believe?  Where it can help? How can AI help for vulnerability management? Yeah, I think maybe  really in detecting things to support, say, the reading of the source code to  maybe better detect things and have less false positives and maybe interpret the context of  vulnerability in the source code. And then really, and I know Snik and I think Semgrep  are working on this too, what a GitHub copilot can do for you already to say,  I have a problem, fix it or write me some source code and then just generate the pull request. I  think like this for the SQL injection should be quite easy for an AI system to generate  than the other versions of an SQL that does not have this injection.  If I may ask another question, if you have 10 teams, how do you distribute the topics to the  different teams? Well, you would separate, so you have this product, you would have several  products. So, we have a large team for a customer, there are several sub-teams and they have, you can  use product and product groups. So, you have a product where you say this observation belongs  to this product and you can have a product per team or they have even further down because they  say every one of their microservices as a product and then they have a team which is responsible for  certain products and then you use a product group and you can say, can have a group of users or  single users, you can say they are allowed to see this product or this allowed to see this product  and then you can separate it. Thank you.  Hello, my question is related to the same context. Can AI help to figure out the false negative  instead of false positive and how it can improve to figure out the zero-day vulnerabilities?  Not sure I got the second part. The zero-day vulnerabilities. Yeah. Is there zero-day? Yeah.  The AI, you mean? I mean zero-day vulnerabilities. Zero-day, yeah. This is always depends on the  scanner, how good they are, how fast they are to have things in their rule set or if there's a CVE  on GitHub. So typically the scanners don't detect like in your libraries, they don't detect  zero-day vulnerabilities, they don't do source code analysis, they just say there's a CVE or  on GitHub or in any other source there is something they detected. The open source scanners  will need that time until there's an official entry somewhere of the vulnerability.  Okay, but what about the false negative? If my scanner doesn't able to find out some kind of the  vulnerabilities which is hidden still, will the AI will help? Say for an example I just did a scan  with a tool and somehow it is not able to figure out some vulnerabilities. Might help the AI, I'm  not really sure. I'm not very deep into this area how AI could really help there. Okay,  no issue. Sorry. All right, any more questions? Oh, sorry.  Thank you very much for the presentation of your product. It's a really interesting product for  me. I have a few questions regarding which vulnerability scanners I'm able to connect  to your software. Because you listed Azure Defender for Cloud as a source that you found to  be reliable and good working. So what is reliable and good working? And am I right to assume that  you mean working good with, how is it called, SecObserve? Because I was wondering why you did  not list things like security command center from Google as well. And is SecObserve able to  parse information from the Google command center? I don't have, we don't do a lot with Google in  our company. Most customers use Amazon and Microsoft. So I don't have any experiences  with Google. That's the reason why it's not on the list. Lack of experience with it.  Writing a parser is very quick. If there's demand, you could write your own parser. We had,  so for example, just Trivi and Prometheus. This will be a contribution from someone. I won't write  it myself or our team. And so writing a parser is quite an easy thing to integrate stuff. Okay,  thank you. May I ask another question? It's a really short one. You talked about rules for  excluding false positive findings or changing the criticality of a finding. So am I right  to assume that these rules are only done by knowing the syntax of writing those rules and  you not yet have something like a nice interface or something like that? And I know if I buy a  software like that or use a software like that, I should know the syntax.  It's not a syntax actually. It's a form where you write things. So you can just, it's pretty basic  to be honest. You can say you have rec access for different fields of observation and there you can  with a rec x say if it fits the rec x, then the new status or new severity will be applied.  So it's in a form where you just have several fields. It's still basic but easy to use. Okay,  thank you very much. All right, thanks again. If you have additional questions, feel free to  approach him anytime. And thanks again, Stefan, for your contribution. Thank you.