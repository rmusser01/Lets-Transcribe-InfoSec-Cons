{
  "webpage_url": "https://www.youtube.com/watch?v=z-ug2dwcSz8",
  "title": "TROOPERS24: From ASCII to UTF-16: Leveraging Encodings to Break Software",
  "description": "Talk by Stefan Schiller - June 26th, 2024 at TROOPERS24 IT security conference in Heidelberg, Germany hosted by @ERNW_ITSec\n\n#TROOPERS24 #ITsecurity \nhttps://troopers.de/troopers24/talks/r3hxdq/\n\nMore impressions:\nhttps://twitter.com/WEareTROOPERS\nhttps://twitter.com/ERNW_ITSec\nhttps://infosec.exchange/@WEareTROOPERS\nhttps://infosec.exchange/@ERNW https://ernw.de",
  "channel_url": "https://www.youtube.com/channel/UCPY5aUREHmbDO4PtR6AYLfQ",
  "duration": 3565,
  "channel": "TROOPERS IT Security Conference",
  "uploader": "TROOPERS IT Security Conference",
  "upload_date": "20240902"
}

This text was transcribed using whisper model: large-v2

 Thank you. Hi, all, and thank you for joining. This talk will be about character encodings.  ASCII, UTF-8, UTF-16. Unfortunately, character encodings in general are pretty boring and  mainly annoying. So, for example, this. Probably a lot of you, at least non-native English  speakers have seen something similar. Some name contains a special character like an  umlaut character or a character with an accent, and they're totally messed up when being displayed.  Admittedly, this guy here, Jürgen Großmüller, is pretty unfortunate with his names in terms  of encoding, but this is not the only annoying thing about encoding. Are there any pen testers  or red teamers here? Yes? Okay. Maybe you also did something like this. A small script  in Python 2 back then that uses a good old RQTXT to perform some brute forcing. When  running the script with Python 2, everything was fine. However, Python 2 was replicated  and Python 3 was released. With Python 3, the incomprehensible decision was made to  handle strings as actual strings, which is super annoying. When running the script now,  Python 3 just throws an error message that says something weird about UTF-8. The script  doesn't work anymore. Pretty annoying, isn't it? But hold on. There is more annoying stuff.  Are there any people with forensic skills here? Yeah. My forensic skills are quite limited.  If I want to search for a specific string in some files, I usually do something like  this. A simple grab. Recursively look for files with a string matrix in it. Works like  a charm and outputs this Linux binary which contains the string. Unfortunately, though,  I missed the second occurrence of the string in this Windows binary. It also contains a  string, but it's encoded with Studio 16. Because of that, there are plenty of annoying null  bytes here in the string, and my simple grab command just didn't find it. I don't know  about you, but that's already enough for me, and we are on a tight schedule, so let's come  to a final conclusion. Character encodings are boring. Dealing with them is very annoying.  They keep us from doing our actual job in various situations, and, after all, if some  strange umlaut character is not displayed correctly, this does not have any security  impact at all. However, I want to quickly show you something, a small demo. So we have  some website here where you can enter a text. The text is reflected on the website itself.  So what we do, of course, try our usual XSS payload. It's reflected. It does not trigger.  Having a look at the HTTP response, we can see that the brackets are probably HTML encoded,  so the payload doesn't trigger. What we can notice is a script tag here with a lang variable  which is obviously populated from the query parameter we can also set in the URL. Let's  try to escape the string by inserting a double quote and adding some payload here, but, again,  we can see it doesn't trigger. The double quote escaped with a backslash. So the string  context cannot be escaped. Now we enter some magic sequence, boom, the payload triggers,  and we see some weird Japanese characters here. So let's have a look at the HTTP response.  Our double quote is still escaped with a backslash. So it's pretty strange. The payload  is here. But if you now have a look at the DOM tree, you can notice that there's no backslash  any more. Instead of a backslash, there's now a yen sign. So the double quote we inserted  actually escaped the string context, and our payload triggered. So what the heck is going  on here? Maybe this last point about security impact is not totally valid. With some weird  character encoding behavior in the browser, we were able to inject JavaScript code. Before  we explore what's happening here, a quick slide. My name is Stefan. You can find me  on Twitter on X. I'm a vulnerability researcher in the R&D team. Our vulnerability research  team discovers zero-day vulnerabilities in popular open source software that we responsibly  disclose to the vendors, and they use to strengthen our product with innovation ideas. So character  encodings. I think these emojis here describe pretty well how my attitude towards character  encodings changed over time, the more and more I learned about them. I hope that after  the demo, we are already in this mood. Obviously, there is more to characters encodings than  just all this annoying stuff. So let's start very simple. What even is an encoding? There  are different assets that we would like to store and process with a computer. For example,  image, sound, text. The problem is that a computer cannot process these assets out of  the box. The only thing a computer can process are ones and zeros. Thus, we need a way to  map these assets to ones and zeros. The process of this mapping is called encoding. After  the assets have been encoded to bits and bytes, a computer can process these. For an image,  for example, we start with the first pixel and use some colour module, usually RGB, which  defines how specific colours map to bytes. In case of RGB, three bytes are used to represent  the colour of a single pixel. We just need to look up the RGB value for our pixel, and  we get the corresponding three-byte sequence which represents the colour of this pixel.  This byte can then be processed and stored by a computer. The same process is also required  for a string. So, we take the first character, and this time, we don't want to map colours  to bytes but characters to bytes, so we use some encoding scheme, for example, ASCII,  which provides a mapping between characters and bytes. We now only need to look up the  character in this table, and we get the bytes that represent this character. This byte can  then again be processed and stored by a computer. So generally, very straightforward. So, time  for a small quiz. What do you think does this byte sequence represent here? I think I heard  it somewhere there, so, yes, it looks like the string hello! But in this case, it's not.  These are the RGB values of two pixels from this image. They just happen to have byte  values which are also valid ASCII characters. The obvious conclusion from this is that we  cannot decode a byte stream if you don't know what was encoded. However, we are really used  to ASCII. Take a look at this hex dump, for example. There's obviously an A character  here. The byte hex for one is always A. That's how it is. But, in fact, the byte hex for  one here has nothing to do with a character at all. This is part of an executable file,  and the byte hex for one introduces an x64 opcode. Of course, hex dump is just the wrong  tool here, and we should have used a disassembler, but there's an unofficial rule which states  when in doubt, it's ASCII. Instead of mapping single bytes to ASCII characters, hex dump  could also map three byte sequences to a colour, and that's how this would look like. It's  just as valuable as looking at the ASCII characters decoded from binary data which is not encoded  in ASCII. So, of course, we need to know what data we are handling with. But in most cases,  this should be very obvious, especially for image files with different file extensions  that indicate what kind of data to expect. We have PNGs, BMPs, JPEGs, all of these three  with different file extensions. This file extension indicates that this is an image,  and what specific encoding should be used. But what about strings? We don't really have  different file extensions, and strings are also used in memory and as part of network  protocols. Do we even have different kinds of strings? Let's have a look at an application  everyone here is using. Every day, probably for hours, a web browser. These are just  a few of the character encodings supported by all modern browsers. But what is the big  difference? Let's have a look at two of the more common ones, UTF-8 and UTF-16. Imagine  we receive this byte sequence, and we know that it's a string, but which specific character  encoding was used? Is it UTF-8, and the sender just wanted to warmly welcome us with this  two emojis here? Or could it be UTF-16, and the sender is heavily insulting us in, I guess,  Chinese? We don't know if we don't know the character encoding. And the fact that we usually  try to stay in our Esky comfort zone blinds us from the truth which is very well summarised  by this quote. There ain't no such thing as plain text. The quote is from Jules Bolski  who wrote a great article about Unicode and character sets in 2003 which is still  absolutely relevant today. If you have a string in memory or in file, you need to know  the encoding or it cannot be correctly mapped to the information it's holding, the characters.  The false assumption of the existence of plain text also hides where character encoding and  decoding actually happens. Let me give you a very simple example. Imagine this simple  website where you can submit a comment. What actually happens if you push the submit button?  Well, the text free your mind is submitted to the server which stores it. But wait, it  submits the text free your mind. How does free your mind look like on a network wire?  We only have ones and zeros. So there needs to be some encoding. And this encoding is  performed by the browser when we push the submit button. The browser encodes the text  free your mind, puts it in an HTTP request and sends it over the wire. Now the web server  receives the byte stream containing our encoded text and can decode it back to the text free  your mind. The logic and functionality of a web server is usually implemented in PHP,  Java, Python, JavaScript or a similar language. All of these languages differ in terms of  how they handle strings. But what they have all in common is that they need to store strings  in memory. Yeah, and guess what? We cannot store a text in memory. We can only store  bits and bytes. Thus the text needs to be encoded again. And the encoding used here  differs from language to language and is totally independent of the encoding used by the browser  when sending the request on the wire. So now the string is loaded into memory, the application  can perform some string manipulation on the comment, and once this is done, the comment  should be stored in a database. Thus it needs to be submitted over the wire again. And the  encoding used here is again totally independent of the encoding internally used by the programming  language. So the bytes in memory needs to be decoded and reencoded before they can be  sent over the wire. Now the database server receives this byte stream, can decode it back  to the text free your mind. Yet again, the database server needs to store the string  in memory and thus needs to encode the text independent of the on the wire encoding. At  last the text should be stored on the hard disk of a database server. Thus it's decoded  from memory and reencoded with a character encoding used for the file storage. Again  an independent encoding. So that's it. Finally the comment made its way from the browser  to the hard disk of the database server. And this is quite an adventurous journey for the  little comment. It passed through plenty of encoding and decoding steps. All these steps  pose the risk of potential character encoding issues. But what even are character encoding  issues from a security point of view? If you're a pen tester, you've probably seen  strange payloads like this. For example, for path traversal, there are these payloads which  use some weird non-ASCII characters. And also for SQL injection, there are similar strange-looking  payloads. You should always test these sequences. They just never work. But if we can figure  out what the fundamental problem here is, we might be able to discover similar issues.  So we already know the basics of encoding in general. So let's have a look at character  encoding specifically. In fact, character encodings are quite old and were even used  before the digital age. This is one of the first kind of chat application. A telegraph  and the Morse code for character encoding. The core principle is straightforward. Characters  and numbers are mapped to a sequence of dots and dashes. This way it's possible to transmit  these via telegraph. Now fast forward to the digital age and to the rise of computers.  Computers are awesome. And manufacturers are eager to provide great products. As with all  fast evolving technologies, though, there is a lot of competition and not much time  for coordination. So each manufacturer came up with their own character encoding solution.  These solutions work perfectly fine as long as everything stayed within an isolated environment.  But as soon as data needs to be exchanged, this became kind of a problem since these  solutions were incompatible. It also slowly appeared that these connecting of computers  might be a thing in the future. So we need some standard. The answer to this was ASCII.  All letters are covered. All numbers are covered. There are additional symbols. And we even  have control characters. Pretty solid solution. Unfortunately, somebody realized that there  are more languages in English. And these use kind of weird other characters which are not  covered by ASCII. So it became obvious that ASCII is not enough. But no worries. ASCII  is a seven bit encoding. This means that the most significant bit is always zero. Let's  just set this bit to one, which allows us to encode another 128 characters. This is  not enough for all languages, but we can map the bit sequences where the most significant  bit is one to different characters for each language. And that's how the result of this  looks like. The ISO 8859 standard. For each language, there are different substandards  that are compatible with ASCII and just use the additional 128 code points to encode the  language specific characters. But again, in terms of compatibility, this was not the perfect  solution. When exchanging data between systems, there was still a lot of potential for confusion.  Also in the 1980s, additional character encodings emerged which cover Asian characters for China,  Korea, and Japan. These encodings, too, were limited to these specific languages. So the  confusion was perfect and the situation seemed pretty hopeless. But then the savior emerged.  Unicode. Unicode assigns code points to practically every character from all major languages and  writing systems. The latest version roughly contains 150,000 characters. It includes control  characters, additional symbols, emojis, so pretty much everything covered when it comes  to a stream. This is the character set used today and probably also during the next decades.  Unicode itself only maps characters or symbols to code points. So whenever you see this U  plus followed by a hex value, that's a code point. You can map a code point to a character,  but it does not tell you how it looks like on the wire or memory or on a file. For this  purpose, we need a specific character encoding. And for Unicode, the major encodings are UTF-8,  UTF-16, UTF-32. Let's start with the most simple one, UTF-32. UTF-32 is the most straightforward  encoding because it uses 32 bits or 4 bytes for each character. Because of that, it can  directly use the code point values. So the A character, for example, is encoded with  a 4-byte sequence, hex for 1 followed by 3 zero bytes. This directly matches the code  point value. The other bytes in memory are just reversed because in most cases, the little  engine is used. The second character is also encoded with a byte sequence that matches  the code point value. The same thing can be applied to the third character. The byte sequence  just matches the code point value. So UTF-32 is very straightforward, but the downside  is that we always use 4 bytes for each character. If a string only contains ASCII characters  like the symbol A character, there are 3 zero bytes that are totally wasted. UTF-16 tries  to partially solve this with a more memory efficient approach. It uses a fixed amount  of 2 bytes per character. So for the A character, the code point value is taken again and put  into a 2-byte sequence. This also works fine for the second character. The code point value  fits into 2 bytes. However, for the third character, we have kind of a problem. The  code point value does not fit into 2 bytes. This is true for all code points that are  not part of the basic multilingual plane and have more than 4 hex digits. So, for example,  this emoji rocket here. These characters are encoded with something called a surrogate  pair. A surrogate pair consists of a high surrogate and a low surrogate. These surrogates  are just specific code points designated for the purpose of encoding characters that don't  fit into 2 bytes. Each of the surrogates itself can be stored in 2 bytes. That's the  total amount of bytes required to store this character in UTF-16 is 4. And that's everything  you need to know about UTF-16. Fixed amount of 2 bytes per character and characters with  a code point that does not fit into 2 bytes are encoded as a surrogate pair. For ASCII  characters, UTF-16 is still not the perfect solution. Not as bad as UTF-32, but there  is still one completely wasted null byte. And a lot of strings only contain ASCII characters.  UTF-8 tries to solve this with an even more flexible approach that uses 1, 2, 3, or 4  bytes per character. How does this work? We already mentioned that ASCII is a 7-bit encoding  and the most significant bit is always 0. For these bytes, UTF-8 does not change anything  and simply matches ASCII. So the A character is encoded with a single byte hex for 1.  The great advantage of this is that you can take an ASCII-encoded byte stream and decode  it with UTF-8, it will still result in the same characters. A 1 is the most significant  byte is used to introduce a longer byte sequence required for all other characters. Two leading  ones in the 0 designates a leading byte which introduces a 2-byte sequence. Three ones in  the 0 introduces a 3-byte sequence. Four ones in the 0 introduces a 4-byte sequence.  And all bytes falling in such a sequence are called continuation bytes and always begin  with a single 1 and a 0. So we can either have a 1-byte character which is equal to  ASCII, we can have a 2-byte sequence, a 3-byte sequence, or a 4-byte sequence. All these  remaining yellow bits here are not used for meta information and can be used to encode  the actual code point value. This allows UTF-8 to encode over 2 million code point  values, so far more than are actually assigned by the current Unicode standard. For example,  let's have a look at this non-ASCII character. This character is encoded with a 3-byte sequence.  This is a binary representation of these bytes. The first byte begins with three 1s and a  0, so it's a leading byte for a 3-byte sequence. Thus, the next byte begins with a single 1  and a 0, thus it's a continuation byte. And also the third byte is a continuation byte.  Each of these bytes contains three yellow bits which can be used to encode the actual  code point. So this byte sequence matches the bytes 0x7D, 0x20, which is the code point  of this character. And that's everything for UTF-8. Single ASCII characters are just taken  as they are. The most significant bit is used to introduce a multi-byte sequence of  2, 3, or 4, and all these byte sequences start with a leading byte followed by a continuation  byte. If that still feels a little bit confusing to you, I recommend to give a look at this  UTF-8 visualiser I built to help understand how UTF-8 works internally. The tool allows  you to enter text binary hex and decode your input with UTF-8. This way, you can play around  with UTF-8 directly on a bit level. I will share the link later. Okay, so that's basically  all we need to know to finally break something. I've picked four major security issues that  you may encounter when dealing with character encodings. These are definitely not the only  issues that exist, but this choice should give you a very good understanding of what  can go wrong. We will cover overlong UTF-8 sequences, invalid byte sequences, string  names issues, and encoding differentials. If you listen carefully, you might be able  to understand what was going on in the demo I showed at the beginning. Let's start with  overlong UTF-8 sequences and have a look at one of the weird paths, traversal payloads  we already mentioned. This one. In binary, the first byte looks like this. There are  two ones and a zero at the beginning. Thus, this is a leading byte for a two-byte sequence,  so we need another byte. This is the next byte in binary with a single one and a zero  at the beginning. Thus, this is a valid continuation byte. Both bytes complete the two-byte sequence  and we can now combine them to the resulting code point. In this case, the code point is  hex 2E, which is a dot character, but this character is a simple ASCII character which  can be encoded with a single byte. These leading zeros here are not necessary. Still, this  two-byte sequence resides in a dot character. The next two bytes in the payload are the  same. Again, a zero-padded leading byte and a continuation byte which both represent the  dot character. The next two bytes are also similar, only this time, they encode a slash  character which is padded with zeros to form a two-byte sequence. This payload is just  a common path traversal sequence. The problem is that the flexible approach of UTF-8 allows  ambiguous mappings. An A character, for example, can be encoded with a single byte like ASCII.  It's what we would expect, but it can also be when padded with zeros encoded as a two-byte  sequence, or we can pad it with even more zeros and make it a three-byte sequence, or  we can add even more zeros and make it a four-byte sequence. It always resides in the same A character.  Okay, but is this even a problem? Let's determine in which scenarios this could be exploited.  Imagine we have a web application which accepts a query parameter called path. Before the  query parameter is processed, it's verified that the value does not contain a path traversal  sequence. In this case, it's fine, and the check is passed. Now, after this check, the  value is UTF-8 decoded. In this case, the two-byte sequence, CS3, A9, is probably decoded  to the accent E character. Now the application processes the string to access some file.  If we try to use the classical path traversal sequence, the path traversal check fails,  and our input is rejected. If we instead use the overlong UTF-8 path traversal sequence,  the check succeeds. Now the input is UTF-8 decoded, and the resulting string actually  contains the dot dot slash sequence. This string is now processed by the application  and may allow us to use, to access arbitrary files. And this exact vulnerability affected  Microsoft's IIS web server. In that case, it was even worse because you could actually  traverse to system 32 and run cmd.exe with a single get request. So, I won't guarantee  that something like this couldn't happen today, but to be fair, it was in the year  2001, so 23 years ago. And the vulnerability was even added to the RC of UTF-8. The RC  explicitly mentions this overlong sequence and also defines sequences like these illegal.  It also mentions that this is a real threat and was used to attack web servers in 2001.  So generally, you would assume that this is not a thing at all nowadays. But the reality  is that there are plenty of custom UTF-8 paths out there, and a lot of them are not compliant  with the RC. Let's have a look at an example for Java itself, specifically Java's deserialisation.  Let's imagine we have a simple class called EvilClass. The class is serialisable and has  a private attribute called cmd. We can create an instance of this class and set the cmd  attribute to some malicious value. When this instance is serialised, the resulting byte  stream looks like this. We've got the class name here, here's the name of the attribute,  followed by the type of the attribute, string in this case, and also its value. So, obviously,  all these strings are encoded in ASCII. Or maybe not. This is the open JDK implementation  of the object input stream which is used for deserialisation. The message used to decode  a single character is called read UTF char. This is kind of UTF-8 but slightly different.  At the beginning, the first byte is read. Based on the upper bits of this byte, one  of the following switch case statements applies. We can either have a single byte character,  a two-byte sequence, or a three-byte sequence. Four-byte sequences are not supported. This  is different from actual UTF-8. In the single byte case, the byte value is just taken as  is since this is equal to ASCII. In the two-byte sequence case, there's a check that the second  byte is a valid continuation byte. However, when the code point is calculated, there is  no check for the long sequences. The same thing is also true for three-byte sequences.  There is no check for overlong sequences. So let's assume we have an application that  is prone to a deserialisation vulnerability but we cannot get our class through because  a firewall or IPS detects the payload and blocks it. In order to bypass this, we can  simply take our deserialised payload and change the first character of the class name to an  overlong UTF-8 sequence. We could also hide any other string like the malicious command.  The modified data still deserialises to the exactly same evil class instance as before.  When we send this payload to the application now, the firewall inspects it and doesn't  find any malicious input. Thus, the payload is passed through and is eventually deserialised.  In summary, if you notice that a custom UTF-8 parser is used, check for overlong sequences  and determine where in the data processing chain the parser is used. You can also use  the UTF-8 visualiser to craft specific overlong sequences. It recognises these sequences and  shows you what character such a sequence would represent. Okay, an overlong sequence is technically  possible but just forbidden by the standard. However, there are also byte sequences that  directly violate the way UTF-8 is working. For example, this byte sequence here. In binary,  the sequence looks like this. The first byte is a leading byte which introduces a four-byte  sequence. Thus, three continuation bytes should follow. The second byte is a valid continuation  byte, but the third byte, it's not. It's a single byte which represents a character itself.  How should we handle this? Discard the incomplete sequence, decode the single byte, or assume  that it's a corrupted continuation byte and fix it? A lot of questions when dealing with  an invalid sequence. What does the RC say about this? Implementations of decoding algorithms  must protect against decoding invalid sequences. Sounds like a plan. Let's see how this works  in real life and have a look at a popular web application, Joomla. It's not as famous  as WordPress but with roughly 2 per cent of all websites on the internet, there are  still millions of deployments out there. At the beginning of this year, we disclosed a  critical cross-site scripting vulnerability in Joomla's core module that was caused by  a bug in PHP itself. So, to prevent against XSS attacks, Joomla sanitises all input. Let's  assume we have this string with a malicious script tag. Before the string is displayed,  Joomla removes all HTML tags. The resulting string is safe to be displayed. So how is  this sanitisation performed? At first, Joomla is unaware of the structure of the input string.  Now the position of the first opening angle bracket is determined. Each character before  it is considered safe since it is outside of an HTML tag. All following characters up  until the closing angle brackets are inside the HTML tag. This portion of the input is  dangerous and is removed. The following characters up until the next opening angle brackets are  safe again, and, at last, the characters under the closing angle brackets are also removed.  The resulting string does not contain any HTML tags and is safe to be displayed. Now,  have a look at this part of the implementation. It uses PHP's string functions to process  the input string. These functions probably handle UTF-8 and allow the usage of all Unicode  characters. For example, it is possible to correctly determine the index of this evil  emoji here. In this case, index 2 is returned because there are two other emojis before  it. For this to work, the function needs to pass UTF-8 multi-byte sequences. In the Joomla  case, mbStringPos is used to determine the position of an opening angle bracket. If this  character is present, the index is passed as the length to mbSubstring to extract all  characters before it. So, exactly what we've just seen. First, determine the position of  an opening angle bracket with mbStringPos, and then extract all characters before it  with mbSubstring. The extracted characters are outside of an HTML tag and are thus safe.  Now let's see how mbStringPos and mbSubstring handle invalid UTF-8 sequences. We've got  this byte sequence, and there is an opening angle bracket here. When mbStringPos processes  input, it starts to pass the sequence from the beginning. The first byte is a leading  byte which introduces a four-byte sequence. Thus, three continuation bytes should follow.  Here's the first continuation byte, but the next byte is not a continuation byte. mbStringPos  still needs to somehow map the byte sequence to an index. So, it considers the invalid  sequence one character. This character is assumed to have the index zero. The next byte  is a valid single byte, so it gets the index one. The next byte is also a single byte,  so it receives the index two, and so forth. So, from the mbStringPos perspective, the  input string looks like this. The opening angle bracket is at index four. Now let's  have a look at mbSubstring. It operates on the same byte sequence and starts to pass  it from the beginning. The first byte is a leading byte for a four-byte sequence, so  three continuation bytes should follow. But, at this point, mbSubstring takes a little  small shortcut. After all, it only needs to cut off the string at a specific offset, so  it doesn't really care about the value of the continuation bytes. Thus, it simply skips  over the next three bytes. These should be continuation bytes, right? This means that  the four-byte sequence is considered one character and gets index zero. Now a single  byte follows which gets the next one, another one, and so forth. So, from the perspective  of mbSubstring, the input string looks like this. The opening angle bracket is at index  two. So there's a significant difference between both functions. The invalid byte sequence  can be used to shift the index used to extract the safe input beyond the opening angle bracket.  The result is that the safe data can actually contain an opening angle bracket and also  a whole HTML tag when adding more of these invalid byte sequences. Since admins can customise  journalist PHP templates, it only takes a single click from an admin to achieve remote  code execution with the Spark. So, let's see this in action. So, here, the attacker  prepares a Python web server to host the JavaScript payload and also starts the Netcat listener,  and now only an admin needs to be tricked into basically just clicking on a malicious  link, so here in this case via mail. Very obscure. So, of course, the admin clicks,  and now it goes pretty far. A PHP template is changed, and arbitrary PHP code is executed,  and the attacker basically immediately receives the reverse shell.  Okay, so far, we've looked at overlong and  invalid byte sequences, but things can go even badly wrong for perfectly valid byte  sequences when it comes to string length calculation. This works very differently from what you  might have expected. We just had some PHP code, so let's have a look at some Python.  In Python, the function len can be used to determine the length of a string. Unsurprisingly,  a single A character has a length of one. The same thing is true for an umlaut character,  and also this single cup of coffee, and also this bug character. We can do the same experiment  with Java, a single A character has a length of one, same here, same here, same here, wait,  what? The single bug character suddenly has a length of two. And when doing the same experiment  with JavaScript, we can see that the bug character also has a length of two here. How  can a single character have a length of two? To understand this, let's go back to our initial  example. This time, we are specifically interested in this part, the internal encoding used by  the programming language. As already mentioned, a programming language needs to store strings  in memory, and the memory can only hold 1s and 0s. Thus, the string needs to be encoded.  And the internal encoding also affects how a programming language usually defines the  length of a string. Python 3, which uses an encoding, returned one for the single bug  character. Java and JavaScript returned two. The reason for this is that they define the  string length as the amount of code units, not characters. A simple A character is encoded  with the two bytes. These two bytes are one code unit, so the length is one. If you now  take the bug character, for example, two bytes are not sufficient any more. This character  is encoded with a surrogate pair. Each of these surrogates is encoded in one code unit.  Thus, we have two code units and also a length of two. This behaviour can also be observed  in calling split on a string in JavaScript. So, for an A character, split returns an array  with one element, the A character, so no big surprise here. But, for the bug character,  split returns an array with two elements, the high surrogate and the low surrogate,  so it makes sense that the string length is two. When an application does not account  for the specific behaviour of string length calculations, it might introduce severe security  vulnerabilities. One of these is an interesting partial differential in Apache Guacamole which  I already presented at Hexacon last year. The gist of it is that Guacamole has a client  component and a server component. The client is written in Java and the server is written  in C. Both components communicate via the custom Guacamole protocol. The protocol is  pretty simple. A single method consists of different elements. The first element is the  op code which specifies which instruction should be executed, and the following elements  are arguments to this instruction. Each element on its own consists of a length followed by  a value which contains five characters in this case. A user is restricted and cannot  send arbitrary instructions. However, there is one instruction called image where user-controlled  data is put into the arguments of this instruction. So, these strings can be controlled. If an  attacker now places four bug characters here, the length field will be populated with eight  because four bug characters are encoded with eight code units. The attacker can now place  this usual string in the following argument. From the perspective of the Guacamole client,  the message looks like this. One op code with two arguments. This message is now submitted  to the Guacamole server. The server is unaware of the structure and begins to pass a sequence.  The first digit here denotes that five characters should be read. So, one, two, three, four,  five. Op code is complete. Read next digit. This time, eight. So, one character is read,  two characters are read, three characters are read, four characters are read, and all  bug characters have already been read, but only four characters were consumed so far.  So, more characters are read. Six, seven, eight. Now, the argument is complete, and  the semicolon designates the end of the whole instruction. However, there is still data  in the buffer, so the next instruction is read. The length field is eight, so eight  characters are read, one, two, three, four, five, six, seven, eight, and the instruction  is complete. This is a whole new instruction which can be set to an arbitrary value. This  vulnerability could be used to leverage specific Guacamole instructions to read arbitrary files  and even combine it with another vulnerability to get remote code execution. Okay, last topic,  but really my favourite one. Encoding differentials. Let's assume we have the string real which  is encoded using UTF-8. The resulting bytes can be seen here at the bottom. When these  bytes are sent over the wire, loaded into memory, or stored in a file, the string should  be decoded at some point again which should result in the same string. This way, the very  same string is restored. It contains exactly the same characters as the string before.  This works fine because for encoding and decoding the same character coding, UTF-8 in this case  was used. If instead of UTF-8, UTF-6 would have been used to decode the byte stream,  the resulting characters were totally different from the original string. This mismatch between  the character encoding used for encoding and decoding is what I'm referring to as encoding  differential here. Such an encoding differential can basically happen everywhere. But let's  have a look at a popular example. The browser and the web server. Our initial example outlines  how a comment was submitted from the browser to the web server to the database server.  This is the direction we've been considering. Of course, all of this encoding and decoding  steps are also required when viewing the comment. Just the other way around. The comment needs  to be read from the hard disk of the database server, it's sent to the web server, and the  web server serves the comment in form of an HTTP response to the browser. Now we are specifically  interested in the part where the web server encodes the HTTP body and the browser decodes  it. We've already seen that there are a lot of character encodings supported by a modern  browser. When the browser receives an HTTP response from a web server, it needs to know  which of those encodings it should use to decode the HTTP body. How does the browser  decide which encoding to use? The most common approach probably all of you are familiar  with is the content type header in the HTTP response. This header does not only contain  the MIME type but also an optional attribute which indicates which character encoding should  be used to decode the body. Okay, but what if the attribute is not present? In that case,  the browser can check the HTML document in the body itself. There might be a meta text  that indicates which charset to use. This is already kind of a weird way to indicate  a charset, because, in order to read the HTML document, the browser needs to decode the  body. It needs to assume some encoding, try to decode the body, check what the HTML document  says is the actual encoding, and then potentially re-decode the body. But there are even more  ways to tell the browser which character encoding should be used. There's a special Unicode  character which can be put at the beginning of a byte stream to indicate the character  encoding. This character is called zero white snow break space but is better known as byte  order mark. Let's assume we want to encode the string new. These are the related Unicode  points, and, at the beginning, we put the additional Unicode byte order mark. Now we  use some encoding, for example, to turn the code points into a byte sequence. When the  byte sequence is decoded again, the corresponding parser only needs to check the first byte  to notice that we used UTF-16. Changing both of these bytes would result in an undefined  character. The byte order marker can be used to determine the byte and also the encoding.  For UTF-8, for example, the first byte would look like this, which can be distinguished  from UTF-16. This also works for UTF-32. The byte order marker is not specific to HTTP  or HTML and is commonly used in files, but it also works for HTTP response bodies and  modern browsers support it. So, there are three different approaches we've seen so far  that tell a browser which encoding should be used. However, the char set attribute and  the content type header is not always present, especially for partial HTML responses, there's  usually no meta tag, and the byte order mark is generally quite uncommon. So, what should  the browser do when there is no information on what encoding should be used? Well, it  just tries to make an educated guess based on the content which is called auto-detection.  If it looks like UTF-8, then it probably is UTF-8. However, an attacker can easily  influence the judgement of auto-detection by intentionally adding bytes that would result  in a reasonable character in another encoding. By the way, if you want to figure out which  encoding the browser assumes, you can just enter document.char set in the web dev concept  that will output the character encoding assumed by the browser. So, let's quickly summarise  the techniques that can be used to force an encoding between the browser and the web  server. If you can control the first bytes of the body, you can insert a byte order mark.  If you've got a header injection or can control the char set in the content type, you can  directly set the encoding this way. If you've got an HTML injection, you can insert an HTML  meta tag, and if no char set information is provided at all, which is really bad, you  can leverage the auto-detection of the browser. Okay, so now we can make the browser use an  arbitrary character encoding. And there are quite a lot to choose from. However, most  of these are ASCII-compatible and not really useful from an attacker's point of view. There  is one specific encoding, though, which is very interesting. ISO 2022 JP. This encoding  is a Japanese character encoding that supports four different alphabets. It's an official  part of the HTML standard, and it's supported by all modern browsers. What's really special  about this encoding is that it supports certain escape sequences to switch between different  character sets. So, if a byte stream contains these three bytes here, they are not decoded  to a character, but instead indicate that all following bytes should be decoded using  ASCII. If a byte stream contains these three bytes here, the following bytes should be  decoded using JISX0201, and there are two more escape sequences which can be used to  change the character set to two different versions of the JISX0208 character set. One  thing worth noting here is that common browsers like Chrome and Firefox will auto-detect this  encoding if the char set information is missing and the body contains such a sequence. Another  interesting aspect is that the later two character sets use two bytes per character, so these  are not ASCII-compatible. Let's have a look at an example. The default mode is ASCII.  If we decode this ASCII byte stream, we don't recognise any difference. In this case, the  byte stream encodes the string do not try and bend the spoon, that's impossible. Now,  let's insert a few additional bytes in between this byte sequence. The byte sequence we inserted  is the escape sequence to switch to the JISX0208 character set. This means that all bytes following  which when decoded with ASCII represents the string bend the spoon, now are decoded  with JISX and become these Japanese characters at the bottom. There are also some question  marks here because a few of these two-byte sequences are invalid. So that's how our string  looks like now. The text beginning from bend the spoon changed to some Japanese characters.  Let's insert another escape sequence here. This switches the character set back to ASCII.  Thus, the bytes representing the text that's impossible are correctly decoded again. This  interesting feature of ISO2022JP seems very handy but it can break some fundamental assumptions.  I want to show you two different attacks that you can apply to any target for which you  are unable to force an encoding differential via the outline techniques. The first one  is breaking HTML context. Let's assume we have a website where the user can enter text  in markdown. The markdown is converted to HTML but only a basic set of HTML elements  is allowed. Thus, no JavaScript code can be injected. A user can, for example, enter this  markdown text. It contains a bold text and also an image. The resulting HTML code looks  like this. For the bold text, the strong text is used, and the image was added via  an IMG tag with the corresponding source and alt attribute. If a malicious user tries to  inject the JavaScript handler, this does not work because special characters like a double  quote are HTML encoded and cannot be used to escape the attribute context. Now let's  consider this markdown text. Just two images with a text in between. The resulting HTML  code looks like this. Again, the images are added via an IMG tag with the corresponding  source and alt attributes. When the byte stream is decoded with ISO2022JP, the default mode  is ASCII and everything is working as intended. We don't even notice any difference. Now let's  insert an escape sequence in the description of the first image. This escape sequence  will be placed in the value of the alt attribute. Thus, all bytes following until the end of  the document will be decoded using JSX. This breaks the HTML document. Although this is  already a denial-of-service attack, it does not yield the ability yet to execute JavaScript  code. But we can switch the encoding back to ASCII by inserting another escape sequence  between both images. So, now, only two strange characters appear in the resulting stream.  If you look at the HTML syntax, though, we can notice something different. The beginning  of the second IMG tag is now part of the attribute value. So, what happened here? We inserted  the first escape sequence within the attribute value, and the second escape sequence outside  of the HTML text and the plain text. The four bytes in between the escape sequences are  decoded with JSX. This also includes a closing double quote of the attribute value which  is now simply not present any more. This means that all following characters are also considered  to be part of the attribute value. The opening double quote of the second IMG tag now becomes  a closing double quote of this attribute value. This also means that the second IMG source  attribute is not an attribute value any more. Thus, an attacker can replace the file name  of the second image within JavaScript error handler. This error handler is now part of  the first IMG tag. This allows an attacker to inject arbitrary JavaScript code. The second  technique can be leveraged to negate backslash escaping. The scenario for this attack is  that some user input is placed in a JavaScript stream. Double quotes and backslashes in the  input are probably escaped, so it's not possible to break out of the stream context. So let's  imagine we have a website that accepts two query parameters, the search parameter and  the lang parameter. You might be roughly familiar with this example. The search parameter is  reflected here in the response, and the lang parameter is inserted into this string in  a JavaScript context. Both parameters are probably sanitised, so, for the search parameter,  all HTML tags are HTML encoded, and for the lang parameter, double quotes are escaped  with a backslash. Thus, it's not possible to inject JavaScript code. One of the possible  char sets which we have not covered so far is the JSX-02-01 character set. In contrast  to both other JSX character sets, it uses only one byte instead of two. And that's how  the corresponding code table looks like. The upper bytes here are used for some Japanese  characters, and the lower bytes are as usual equivalent to ASCII. But, if we look closely,  there are two exceptions here. Both of these highlighted characters don't match with ASCII.  For ASCII, the byte hex 5C represents the backslash character, and the byte hex 7E represents  the tilt character. For JSX-02-01, both of these characters are mapped differently. Decoding  the byte hex 5C results in a yen character, and decoding the byte hex 7E results in an  overline character. So, back to our web application. By default, all bytes are decoded using ASCII.  If you try to inject JavaScript code by escaping the string context with a double quote, this  does not work because our double quote is escaped with a backslash. But now, let's insert  this escape sequence in the search query parameter. This switches the character set  for all bytes following to JSX-02-01, and this is how the browser sees the HTML document  now. There is no backslash character here any more, just the yen sign within the string.  This means that the double quote we inserted actually designates the end of the string,  and we can add arbitrary JavaScript code outside of the string context. Another ugly  but yet so beautiful pop-up. So, if you can make the browser assume an ISO 2022 JP character  encoding, you can use these two techniques to get an XSS. What makes this so powerful  is the auto-detection feature of the browser. An application with missing char set information  is very likely to be vulnerable because you only need to insert an escape sequence to  make the browser assume an ISO 2022 JP char set. From a security point of view, I hope  that browsers will disable auto-detection for this encoding, and that's also what I  suggested. Still, these techniques could be applied if you can control the character set  means. Okay, I hope by now your mood changed to this. At least, for me, it did. So, let's  give it another try and summarise character encodings. The first major observation is  character encodings are basically everywhere. For all these places, there is potential for  something to break. UGF-8 is the most common encoding, especially in the world wide web.  However, there are plenty of legacy encodings which are still supported by all modern browsers.  Not so much present in the world wide web but still very popular is UGF-16. It's commonly  used on Windows and also internally by a lot of programming languages to store strings  in memory. At last, we barely stretched the surface here. There are so much more vulnerabilities  to be discovered, and I hope that this talk inspires you to don't just look away if you  see some weird encoding stuff, but instead dive into it. It's really worth it. So, thank  you very much for attending and listening to my talk. If you're interested in more of  this kind of content, feel free to follow us on X or Mastodon, and also check out our  blog post where we regularly release new articles on our vulnerability research. Furthermore,  you can find the UGF-8 visualiser via the QR code here on the right. Thank you very  much.  All right. Thank you very much for the great talk. I think we still have time for one question.  So, is there anyone wanting to ask something?  Maybe I can start with a question, and you've got some ideas. So, after your research, I  get you're probably pretty sensitive to finding some partial differential bugs and stuff in  the applications you look at. How do you usually start looking into this? Do you usually just  try some certain sequences, or do you directly look into code? What decoder do they use?  What is your typical approach?  Mainly two approaches. One is a more dynamic approach to just get a feeling that something  is decoded with some familiar stuff. This is also probably based on experience. And  the second approach is to, if you are looking at white box targets where you have source  code access, then specifically search for things like UGF-8, UGF-16. It's also like  you can also, if you want to look for UGF-8 custom parsers, there are some constants which  are used when calculating the code pound value using UGF-8, and you can just search for those  specific constants, and then you can quickly find places where custom UGF-8 parsers are  used.  Any other questions?  All right. So then, thank you. We'll just change a little bit, and then we'll be back in a minute with the next talk. Thank you.