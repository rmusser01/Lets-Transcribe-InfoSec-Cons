{
  "webpage_url": "https://www.youtube.com/watch?v=tZmollb8NXk",
  "title": "TROOPERS24: Fuzzing at Mach Speed: Uncovering IPC Vulnerabilities on MacOS",
  "description": "Talk by Dillon Franke - June 26th, 2024 at TROOPERS24 IT security conference in Heidelberg, Germany hosted by @ERNW_ITSec\n\n#TROOPERS24 #ITsecurity \nhttps://troopers.de/troopers24/talks/lb9pjt\n\nMore impressions:\nhttps://twitter.com/WEareTROOPERS\nhttps://twitter.com/ERNW_ITSec\nhttps://infosec.exchange/@WEareTROOPERS\nhttps://infosec.exchange/@ERNW https://ernw.de",
  "channel_url": "https://www.youtube.com/channel/UCPY5aUREHmbDO4PtR6AYLfQ",
  "duration": 3486,
  "channel": "TROOPERS IT Security Conference",
  "uploader": "TROOPERS IT Security Conference",
  "upload_date": "20240902"
}

This text was transcribed using whisper model: large-v2

 All right, we'll just go for it like this.  Sorry about that, everyone.  Even though there was a little hiccup there, that actually went a lot better than my troopers  talk in 2022, because I actually got food poisoning and COVID on the day of my talk.  So I'd say things are going a lot better this time around already.  Thanks for bearing with me, though.  I'm really excited to speak about some research that I've done the past year or so, fuzzing  mock IPC mechanisms, but more importantly, talk about fuzzing in general, and how hopefully  you can apply it to research projects you want to do, find security vulnerabilities,  make software safer, and hopefully bring something back to your respective teams.  So a quick bit about who I am.  My name is Dylan Franke.  I'm from San Francisco, California.  I am a Senior Proactive Security Consultant at Mandiant, now part of Google.  I mostly do pen testing consulting right now, so application security, source code reviews,  and embedded device assessments primarily.  I also do a lot of reverse engineering.  So I also did a job rotation on Mandiant's Flare team, which is a malware reverse engineering  team, specialized also in searching for exploits that are used in the real world, doing zero-day  vulnerability research and exploit development.  And I'm currently doing a project with Project Zero, extending some of the research that  I'm talking about today, trying to find and make finding zero days harder for attackers  out there.  So let's go ahead and jump right into things.  Who is this talk for?  I really want this to be applicable to everyone here.  I hope, first and foremost, it'll be applicable to offensive security researchers like myself.  You'll hopefully learn how to use fuzzing to identify memory corruption issues.  And craft exploits.  However, I really hope it's useful as well for defensive security engineers, the blue  teamers out there, software developers, and mobile application researchers.  So just a little bit of an overview about what we're going to be talking about today.  I'm going to start with a crash course on what fuzzing is and macOS IPC mechanisms.  We're going to start from a very basic level and build up so that we can all be on the  same playing field here.  Then I'm going to talk about the fuzzing cycle, which is kind of a way that I like to think  about fuzzing and identifying memory corruption issues.  And then we're going to talk about kind of next steps in my research.  And then we'll leave some time for Q&A.  So let's start very basic.  What is fuzzing?  Fuzzing, the way I like to define it, is just sending unexpected inputs to a system in the  hopes of making something unexpected happen.  So very basic definition.  Now an attack vector, I like to define as a channel to send an input to a system.  An attack vector can really be anything.  And I urge you to get creative in thinking about what might be an attack vector out there.  So on a laptop, for example, Bluetooth connections, incoming packets that are being parsed by  the Bluetooth protocol might be an attack vector.  Inter-process communications, notifications being pushed to your phone, peripherals connected  to your device, or a wireless connection.  All of these and more can be attack vectors.  Couple examples here.  On Adobe Acrobat, simply opening a PDF.  The PDF there might be an attack vector that an attacker could exploit for a victim.  A query parameter in a Google search could be an attack vector.  A smartwatch that's handling Bluetooth data coming from your phone could also be an attack  vector.  So you can really get creative here.  So why do we want to fuzz?  Why is that useful?  And it's because in memory-unsafe languages, C and C++, which build virtually all the software  that we rely on, we want to send input that causes a crash.  And this is possible because they deal with memory unsafely.  Depending on the type of crash that we're able to generate, our inputs might be able  to trigger a buffer overflow, a heap overflow, a use-after-free, a double-free, or a memory  leak, which can help us bypass ASLR.  And here's just an example of a vulnerability that was disclosed in Adobe Acrobat.  This is, first of all, calling out, number one, the attack vector.  Let me get this laser pointer.  The attack vector here is opening a malicious file, and it says, the existence of an object  prior to performing operations on the object.  That's textbook terms for a use-after-free vulnerability.  So given this attack vector and a fuzzer that was able to find a crafted input to trigger  this vulnerability, an attacker was able to abuse this.  So there are two main types of fuzzing that we're going to talk about today.  The first is mutation-based fuzzing.  This is where we take existing inputs that we know are valid through an attack vector,  and we modify them to create new and more interesting inputs, then we send them back  to the program.  Another type is grammar-based fuzzing.  This is where we generate inputs based on a specified rule or syntax that defines a  structure of valid inputs.  So for example, if you're trying to fuzz a web browser, you might write a grammar to  produce valid JavaScript, and then create a bunch of interesting mutations on valid  JavaScript syntax.  Today though, we're going to be focused on the former here, mutation-based fuzzing, and  the research I'm going to talk to you about.  Okay, let's do a quick crash course into the XNU kernel.  This is the kernel that powers macOS.  There's three main layers of the XNU kernel that I want to talk about.  First off is the mock layer.  This is responsible for most of your low-level tasks like thread management, IPC, inter-process  communication, and memory management.  You also have the BSD layer, which handles higher-level tasks, things like file system  accesses, read and writes, and network protocols.  And then there's IOKit, which is a framework for developing device drivers.  Once again, we're going to be honing in on the mock layer here, and specifically inter-process  communications.  So what is inter-process communication or IPC?  At a very basic level, processes need to talk to each other for different reasons.  One process might need to tell another that it's done reading a file.  It can continue the task that it was doing.  One might be blocking, waiting for a network response.  And so processes are talking to each other all the time on modern operating systems.  On macOS, there's a lot of different ways they can do this.  A few are listed here.  Mock messages are what we're going to be focusing on, because they're the lowest-level IPC mechanism  and the direct basis for which many other mechanisms are built on top of.  So my thought was, if we target this base IPC mechanism and try to generate crashes,  we might be able to find some interesting things going on that we can take advantage  of as an attacker.  What are mock ports?  So a mock port is essentially an IPC message queue.  So you can think of it as a mailbox that's managed by the kernel.  So messages are going to come into these mock ports.  The kernel is then going to distribute them to the processes that need to receive them.  There's this notion of a port write, which is a handle to a mock port that allows either  sending or receiving messages to the port.  And you can actually, if you have a Mac system, see these mock ports and list some pretty  detailed information about them.  If you use the LSMP tool here, it'll show you all of the mock ports that are on your  system, and you'll be able to see different details about them.  For example, whether they have receive writes, send writes, which processes are sending and  receiving to them.  And a lot of other pieces of information.  But they're being used constantly by the operating system.  So they are ripe for taking advantage of.  Let's go through a basic example of how one would establish a mock connection using mock  ports and mock IPC mechanisms.  So first, a couple terms.  The bootstrap server is a special mock port that helps establish and facilitate connections  with other mock ports.  By default, every process on the system has a send write to this bootstrap server.  Additionally, mock services.  A mock service is just a special mock port with a name that is registered with the bootstrap  server.  So, for example, let's say we have a service called com.apple.troopers.  That's going to be registered with the bootstrap server, and it is going to know about it.  So now let's say that Alice would like to communicate with a service.  Or create a service, rather.  And let others communicate with it.  So Alice is going to first allocate a new mock port, which will give her a receive write  to that port.  So she can check that mailbox and grab any messages that are sent to it.  Alice is then going to register her service using that specific service name, com.apple.troopers,  with the bootstrap server.  And by doing this, she is giving the bootstrap server a send write to the port that Alice  has a receive write to.  Now Bob is going to come along and ask the bootstrap service for the service named com.apple.troopers.  And the server will give Bob a copy of that send write for Alice's mock port.  And Bob can now send messages to Alice's mock port, which Alice can then receive with  her receive write.  So all mock messages are sent via this function that you see here called mock underscore message.  This is a low-level API that's used.  And really, the two important things to notice about this, first, it's used for both sending  and receiving mock messages.  And the struct that we really care about here is this first one here, this mock message  header T.  I won't dive too much into the syntax here, but this contains a bunch of important information  like which mock port is being sent to, where the reply should be sent back to, this specific  message ID.  And then there's this body field, which is just a binary blob.  And there's actually no sort of syntax or length that is necessary here.  So it's really a freeform building block IPC mechanism that other IPC mechanisms can build  off of.  So that's a little bit of a background on macOS IPC mechanisms.  Now let's jump into the fuzzing cycle here.  This is kind of a process that I've coined for myself and how I like to think about research.  The first step here is really going to be identifying an attack vector.  So finding something you think is interesting, something that is exposed and available for  attackers to interact with.  If it's exposed via network protocol, that's even better.  It's a lot easier to send your inputs to.  Then we're going to talk about generating a corpus of inputs.  So how do we generate those valid messages to send through that attack vector?  Then we're going to talk about creating a fuzzing harness.  That's the actual code that will send your input through the attack vector and to your  target.  Then we'll fuzz and hopefully produce crashes.  And then we'll identify relevant crashes that might be useful for crafting an exploit.  So let's jump into this fuzzing cycle here.  We learned a little bit about mock messages.  I'm going to talk briefly about how they've been abused in the past and why we should  care about them.  The first reason is for sandbox escapes.  Let's say that we have a web browser process.  An attacker has an exploit to get code execution within the browser renderer process.  This is usually not going to be very useful to an attacker on their own because they're  running in a sandboxed, restricted context, they're not going to be able to access files  on the system or do really anything that they care about.  However, even these restricted sandbox processes can often send mock IPC messages to other  processes on the system.  Many of which are not sandboxed.  So by sending a crafted mock IPC message to a message handler that we can take advantage  of, we could go ahead and jump out of that sandbox escape and get into an unrestricted  context.  Additionally, there's the potential for privilege escalation here.  We could be in an unprivileged process where we have code execution, but we take advantage  of once again a mock message handler and a privileged process and then execute under  that context.  Here's a couple really interesting links to previous exploits that have dealt with mock  messages and exploiting those mock message handlers.  The first one I got to see at Black Hat was a very cool race condition in the Mojo framework  in Chrome.  And the second one here, which was a huge inspiration for me, this is like an eight  part blog series about crafting a Safari RCE exploit and a sandbox escape using IPC messages.  So definitely recommend that as a read if any of this interests you.  So the next thing I wanted to do here is figure out what I should target.  I knew I wanted to target IPC mechanisms and mock IPC messages, but I wanted to narrow  that down a little bit.  And so I wanted to determine which processes could actually allow for that sandbox escape  and would be meaningful.  So there's a tool called SBtool, which was written by Jonathan Levin.  He does a lot of macOS research.  He's written multiple books.  Very smart guy.  And this SBtool uses macOS's built-in sandbox check function to determine which mock services  a given process can send messages to.  So those message handlers that we can send to from a sandbox context are ones that we  might be able to exploit for a sandbox escape.  So you can see us using this here.  These are all of the mock services we can interact with from Safari is the process I  ran this against, I believe.  So we went from first of all having all macOS processes as ones we could target to then  just ones with a mock service exposed finally to a smaller subset of processes with a sandbox  allowed mock service.  Okay.  So now we have a small grouping of processes we want to look at.  Now we need to find an entry point.  So an entry point is gonna be how we actually send our input and, you know, send it through  an attack vector to the target.  So as we talked about, we know that mock message is used to send mock messages from one process  to another.  And we see that going on here.  We have a sending process.  It calls mock message.  It sends this message by land.  The kernel processes that, passes it to the receiving process.  The message handler accepts it and says, okay, we got a message by land.  So you might say, why don't we just modify the real mock messages being sent?  And that's a great idea.  We can absolutely do that.  That's how I started.  You can use Frida or LLDB, a debugger, and set a break point on the mock message function.  You can then wait for it to be called.  You can mutate the mock message that you see being sent.  And instead of saying by land, we're saying by sea now.  And then the kernel will process that and send it to the message handler.  So this is a very valid way to fuzz.  And it's a really great way to get your hands dirty and really dive in pretty easily.  It's simple.  And it's also really similar to the end exploit.  So you're gonna need to send a mock message through the kernel, through all these different  layers to your message handler, ultimately, when you're writing the exploit.  So those are a couple pros.  The cons of doing this, first of all, it's really slow, because you have to wait for  the application to send messages in order to mutate and fuzz them.  There's also a lot of points of potential failure along the way.  We're having to go through the kernel here.  It's slower and can cause bad things to happen if we do something wrong.  And we're also dealing with two different process spaces here.  This is important for a reason we'll talk about in a minute called code coverage.  That's actually understanding where our fuzzed input is getting to in the code that we're  fuzzing.  And it can be difficult to determine which message actually caused a crash in our receiving  process.  So what if instead of waiting for that mock message function to be called by the system,  we just write a program to call it ourselves?  So we cut out sort of the middleman here, and we say...  Let's just call mock message in a C program that we write.  So this is a lot better, because we don't have to wait for mock message to be called.  But we still have to deal with the kernel here.  So what if we went ahead and directly called the message handler?  So instead of calling mock message, we say...  We're just gonna call this message handler function, whatever that might be, directly,  and pass our input into it.  This does require some reverse engineering, but we're not afraid of that.  So we'll jump into that.  And this I like to refer to as getting close to the system of interest.  So we care about this message handler.  Instead of going all the way from here and sending messages through the kernel, we should  get as close as we possibly can.  And write a harness that will directly call that message handler of interest.  The pros of this, it's very fast.  We're calling it directly.  There's no waiting.  There's no kernel processing.  It's also in the same process space.  So it's easy to instrument and get code coverage about where we're getting to in the process.  And it's easy to know which input caused a crash and replicate it when we need to.  It is different from the end exploit that we'll craft, because we are gonna have to  use the kernel when we develop that exploit.  And we'll talk about this later.  But it might be naive about initialization routines.  This receiving process might need specific initialization routines called before it can  process messages.  Okay.  But this, in general, is the approach that I've spent most of my time using.  Calling that message handler directly with our inputs.  Okay.  So we identified an attack vector.  But now we have to figure out what should we actually send through the attack vector.  Sending totally random data is not likely to produce anything meaningful.  I'm sure anyone who's pentested has seen that as well.  If you're fuzzing a web application with just random bits, you're likely not gonna  get very far.  There are exception handlers that will catch malformed input, especially when you're fuzzing.  And there's often input validation schemes that will not allow your fuzzed input to get  very far.  So we need to identify some examples of valid mock messages.  And this is known as building a corpus.  So a corpus of legit valid inputs that we can fuzz with.  Just as a quick aside here.  When you are fuzzing on macOS, there's a few things to take into consideration.  This took me a while to figure all these out and get kind of my fuzzing setup all situated.  I recommend setting up a macOS virtual machine.  Because that enables you to pretty easily disable system integrity protection, SIP.  You're gonna need to do that whenever you start debugging certain processes on macOS  and really getting kind of down and dirty with the system.  I also recommend disabling report crash so that you can have your fuzzer detect and log  crashes instead of the native report crash handler.  Disabling sleep.  You don't want your device going to bed while you're in the middle of a fuzzing session.  And there's some really good information provided in this DEF CON talk, which I've linked here.  And I'll show the slides after.  Okay.  So we want to generate this corpus of valid inputs.  So that we know what a valid message looks like and we can mutate that when we're fuzzing.  The first thing that we want to do is find a mock service of interest.  In our case, this is gonna be those sandbox processes that we know we can communicate  with.  The service I've focused most on is Core Audio D.  Com Apple Audio Core Audio D here.  And I focus on it because it handles all interactions with audio hardware.  It's a very privileged process.  And it is actually...  It processes mock messages from many processes.  Many of which are sandboxed.  So it's a good candidate for what we're looking for.  Sandbox escapes.  Once we find...  Next, we need to find the binary that actually implements that mock service.  That Com Apple Audio Core Audio D. So this service is registered with LaunchD.  LaunchD takes care of all of the different services on the Mac operating system.  And spawns the necessary daemons.  So LaunchD spawns this Core Audio D binary.  And then the mock server itself, the code for that, is within the Core Audio framework.  Which is a framework built into the Mac operating system.  You can see this.  We attach to the Core Audio daemon process here.  And we list the modules that are loaded.  And we see Core Audio D right here...  Core Audio framework right here.  So as a reverse engineer, like myself, you might say...  Okay, let's pop that open in IDA and start taking a look at the Core Audio framework.  But we try to stat it, and we see that there's no such file or directory.  That's interesting, right?  That is because starting with Big Sur on Mac, most framework binaries are actually not sitting  on disk.  They are stored in what's called the dilled shared cache.  This is done mostly for optimization purposes, but it's really annoying when you're wanting  to do security research as well.  Luckily, we can actually extract them.  So there's a few different GitHub projects that you can take a look at.  The one I used here is this dilled shared cache extractor.  And those will allow you to actually recover the framework binaries that you care about.  So that's pretty great.  Now we have the binary in our hand.  We can analyze it.  But now we need to find the function that implements the mock receive functionality.  You might be asking yourself...  Isn't this just the mock message function?  Because I said you use that to send and receive mock messages.  And you are right.  But when receiving, that's actually a non-blocking function.  You don't want your process just busy waiting for a mock message to come in.  So there's actually a trap to the kernel that will occur when a mock message comes in.  So you can't just sit there and wait for a mock message to be called.  You could do kernel debugging.  Someone's actually done that.  I'm linking the link right here.  And they were able to dump the incoming mock messages.  But the cons of that are that you see every single mock message sent on the system.  It's really annoying.  And it's difficult to isolate the target process that you care about.  And you also need two machines to do that.  Which you can do with a host and a virtual machine.  But I thought...  Is there an easier way than doing kernel debugging?  And it turns out there is.  Because pretty much every mock service on the macOS operating system uses what's called  the mock interface generator, or MIG.  Apple provides this as an interface definition language compiler.  It just abstracts much of the mock IPC layer away from developers.  So they can not really have to deal with all the bits and bytes of the mock spec.  So I thought...  What if we just search for MIG-generated routines and dump their incoming mock message?  So looking for these MIG subsystems is actually not too hard.  There's a Hopper script out there, if you use Hopper.  I went ahead and modified that for use with IDA Pro.  You can also just look at the symbols and kind of grep for subsystem.  And they're pretty easy to find.  Just hanging out in the binaries.  So here we're looking at the MIG server subsystem, which is in the core audio framework that  we care about.  Then once we find that subsystem, we actually have to see how it's used.  So if you look at this IDA function here real quick...  This is called MIG server server.  It takes in a mock message header T here as its first argument, the RDI register.  If we take a look at that, and if you can see it, it is getting an index to the message  ID field, which is passed in the mock message header.  And then it's using that as a lookup in this subsystem.  So it's essentially a function lookup table.  It's parsing that ID, getting an offset, and then calling a virtual function right here.  If we look at what those functions actually map to, we've renamed them here.  These are all sort of RPC-like functions.  And some of them have some pretty interesting names.  Getting property data, setting property data.  A lot of interesting functionality that is exposed once we figure out how this mock message  handler works.  So once I figured all this out, I wrote a simple script to hook on to the message handler  that I had found using LLDB, and go ahead and just dump any messages that get sent there.  So I'll show you just a little example of running this.  The first thing, we're looking for that core audio D process, because we know that that  does the bulk of the core audio framework functionality.  Get the PID for it, paste that in here, and then we paste the module that we care about,  and the function that we just saw that was doing that function lookup.  And then we should start getting some messages that are sent and saved to our corpus.  So there's a message that came in, a really big one.  One thing that you might want to do is open some other applications.  So here I'm opening Apple Music, and you can see when I do that, there's more messages  that come in, because different programs are sending different mock messages.  So we want to get a good sample of different inputs for our corpus.  Okay.  So now that we've got that corpus, or that grouping of valid inputs that we can fuzz with,  we now need to create a fuzzing harness.  So this is going to be the nuts and bolts of how we actually call the function that  we care about.  And a fuzzing harness is really just code that allows you to send input through an attack  vector, or call a desired function.  So in our case, this is our target function, the MIG server.  To do this on Windows, this would be pretty easy.  We would just use load library, load in the core audio framework, and then call get proc  address to the function that we care about here.  On macOS, there's a couple similar APIs, the DLopen and the DLSim APIs that we can use  to get a function pointer.  However, the problem I had is that my symbol wasn't actually exported by the core audio  framework.  So I ended up writing a custom Mako symbol parser to get the function offset that I cared  about, and then call that with the help of a couple colleagues.  So that's a talk for another time.  But it's not always super easy to call these functions that shouldn't be exported.  But it is possible.  You can get around it.  So here's us running our fuzzing harness.  We are passing in the core audio framework to our harness here, and the function that  we care about.  It's going to perform the symbol lookup and then get the function pointer and call that  function.  And then here is the specific input that we are fuzzing with from our corpus.  Here's the function, our attack vector.  Here's what our mock message looks like being sent.  And then we actually get a returned mock message from the service.  So we can see that our harness did, in fact, call our function that we care about with  our corpus input.  So that's pretty cool.  We wrote a harness that can call the function we care about.  But that's really only one execution, and we just sent a valid mock message.  That's not going to get a crash to happen.  So we need a fuzzer for that part.  A fuzzer is going to be a program that generates inputs to be sent to a system and monitors  for crashes that might occur.  So the idea here with a fuzzer, we have our corpus, which we've generated, of valid inputs.  We're going to pass this to the fuzzer.  The fuzzer is then going to mutate one of the corpus inputs, and it's going to use our  fuzzing harness that we wrote to call the message handler.  Now then, the fuzzer is going to say, did that input produce a crash?  If it did, that's awesome.  Let's save that input.  That's a really good input.  If it's not a crash, let's continue to the next one.  So pretty simple state diagram here.  But let's talk about a toy example here of why that might not work super well.  So we have this process string function.  We're taking an input string.  There's a couple conditional checks here.  The first is, is the input string greater than 3 in length?  Is it greater than 6 in length?  Does it start with an S?  And then, does it contain the string secret?  If it does, no pointer dereference and crash.  So this would actually be really difficult for the fuzzing setup that we just saw to  identify.  It's pretty uncommon to be able to brute force, especially longer strings in this way.  And so we need feedback that we're actually getting somewhere in the code.  We want to know that, hey, we sent a string of 8, and we got to the second block.  But when we send a string of 2, we don't even get past the first one.  And so this is kind of illustrating the concept of code coverage.  And code coverage is really just a trace of a program's execution flow to identify new  code paths.  And we really care about reaching a new code path, because it means our input did something  better than the last input.  So let's take a look at our fuzzing setup and see how it works with code coverage included.  We still have our corpus of inputs.  We feed it to our fuzzer.  It mutates, passes through the fuzzing harness.  Then, however, there's three kind of states that can occur here after our message handler  is called.  We can have a crash, which we definitely care about that input.  Let's save it.  But then we can say, okay, we went along a code path that we've seen before.  Or we went along a new code path.  And if we went along a new code path, that's a great input.  Let's add it to our corpus and continue to mutate on that again.  So this is kind of the state-of-the-art approach for fuzzing these days, is instrumentation  to get code coverage to figure out if your inputs are actually good.  And then if it's not, sort of trimming it and keeping the ones that are good and further  mutating them.  So how do we actually determine code coverage is a question that might come up.  This is really simple if you have source code and you're fuzzing with AFL++, for example.  You can use AFL++ instrumentation, compile that into the binary that you're trying to  fuzz, and it will keep track of exactly where the fuzzer is getting to with your inputs.  However, when you're dealing with black box binaries where you don't have source code,  like we're dealing with here, this gets a lot more tricky.  But it's still very possible due to programs like Frida, which is a dynamic instrumentation  framework.  It's awesome.  Another one written by one of my colleagues is TinyInst, which I've used quite a bit for  this research as well.  It does a really good job on macOS.  And then for actually interpreting that code coverage, you can use Lighthouse.  Lighthouse is for IDA Pro or Binary Ninja.  And this diagram on the right here shows you a little bit about what that Lighthouse output  looks like.  The green blocks are basic blocks that we are getting to.  And everything in white here are basic blocks that haven't been explored yet.  So this can tell us a ton about how our fuzzer is doing and how our inputs are doing.  If we are never getting to a specific area of the code, we might want to investigate  that and look at why.  Okay.  So let's talk about actually fuzzing now.  The fuzzer that I have mostly been using for this research is called Jackalope Fuzzer.  There's lots of really good ones out there.  I'm sure most of you have heard of AFL++.  It's a great one.  LibFuzzer.  A lot of good fuzzers out there.  This one works really well on the macOS operating system.  I recommend enabling memory sanitization if you can.  So if you're on Linux, you can use ASAN.  Basically what this will do is place restricted pages around all memory allocations.  And so you will know very quickly if there's a buffer overread or maybe a one-byte overflow  that would be difficult to identify otherwise.  And it will tell you exactly where that happened in the crash backtrace.  Apple has a similar library that you can use called libgmalloc, which will allocate these  restricted pages.  I used a tinyinst for dynamic instrumentation to dump coverage from the process that I was  fuzzing.  And then I used Lighthouse to interpret that code coverage.  And here's just a little output of running my fuzzer.  I know this is a lot of text.  But just to show you a little bit about some of the components here.  This first piece is guardmalloc.  So that is the memory sanitization library that we injected.  Allocations are being placed on 16-byte boundaries.  So we'll be able to tell more easily if there's a memory corruption issue that occurs.  There's an exception being thrown here.  This one is actually just a C++ exception.  We don't care too much about that.  But it's gonna tell us whenever we have a crash.  And there's also instrumentation that's occurring from tinyinst.  So it's saying, okay, before that crash, we were at this specific instruction set.  This was the code right after that.  Okay.  So once we actually start fuzzing, we usually very quickly want to improve how our fuzzing  is doing.  Because we realize we're not getting to portions of the code that we want to.  We're not producing crashes or other reasons.  So one thing that we can do to help improve our fuzzing is to regularly check that code  coverage.  We can learn a lot about how our fuzzer is doing by looking at the code paths it does  and doesn't take.  Our goal should be to cover as much of the binary as possible with code coverage.  So here's a little bit of an example from my fuzzing.  This is a mock message handler.  And you can see it's always going left here.  And it's always throwing this error that there's no system initialized.  And it's not getting to a bunch of juicy code on the right here.  So therefore, the fuzzing is not super meaningful.  Because we're not getting to the bulk of the code.  And we're just hitting an exception handler almost every time.  So let me illustrate this a little bit more and talk to you why we need to think about  initialization of the process that we're fuzzing a little bit more.  So in our case, we have our fuzzer and fuzzing harness.  We are calling a function within the core audio library.  We already talked about that.  And we're just right off the bat calling this process mock message function with our input.  But the core audio daemon process, the native process, it actually does a bunch of things  before it ever processes a mock message.  It might initialize global variables.  It might set up hardware, do some system checks.  I don't know everything it does.  But it certainly just doesn't start receiving mock messages.  And it's actually not expecting to receive any mock messages right off the bat like we're  sending them.  So I noticed that my fuzzer wasn't getting very deep into the code.  And that was because, like we mentioned, the core audio daemon process was performing initialization  and setup for the core audio framework.  It normally calls all these functions, which you can see here.  You can determine this by using dynamic instrumentation, like Frida or Tinyinst, and running it on  a process you know performs proper initialization.  So in my case, I ran it on core audio daemon.  I said, wow, look at all these functions getting lit up before it ever receives a mock message.  So we want our fuzzer to be in the same process space as this initialized state.  The core audio daemon did a bunch of work to set up the core audio framework to a state  where it can receive messages.  And we want to be able to fuzz in that same state.  Otherwise we're not going to get very far.  So there's a couple approaches we can take here.  The first one would be to figure out the exact initialization functions that need to be called,  and then call them within our fuzzing harness that we wrote before we start fuzzing.  This can be great when there's very simple initialization needed.  For example, if you're parsing like an image parser, there's probably not a whole lot of  setup that needs to be done.  You just feed it an image.  You might need to set a couple of global variables.  But in our case with core audio daemon, it was performing very complex initialization,  and I didn't have any source code.  So in this case, what you can do is modify a process you know performs proper initialization,  and then have it spawn your fuzzer once it gets everything set up properly.  You can do this by hooking a function.  You can binary patch if you want to.  You can use something like LD preload, or DILD insert libraries, or detours on Windows.  Or you can instrument the binary using Frida or TinyInst once again.  You're seeing that I really enjoy dynamic instrumentation here.  Okay, so...  And we're gonna go ahead and use that second approach here, using Frida to inject our fuzzer  into the space that we care about.  But we do need to figure out the proper place to instrument our code.  So that core audio daemon process loops continuously after it performs that initialization routine.  And I noticed by looking at it in IDA that once it kind of set everything up here, it  called this CF run loop in mode function, and then it just continuously looped around  this basic block.  So I thought, okay, what if once we get here, I just have Frida attached to it, change that  function so that it calls our fuzzing harness instead of just looping and waiting?  So that's exactly what I did.  I wrote a really simple Frida script.  What this is doing, this is my fuzzing harness that I put together, exported as a dilib.  We're just loading that module in Frida, and then we're getting the fuzz function from  that dilib.  And then we're using the Frida interceptor to find that function that we just talked  about, CF run loop run in mode.  And then instead of calling that function, we are just calling our fuzz function instead.  So we'll get an alert that says, hey, that function was called, starting our fuzzer,  and then we just call the fuzz function.  And you can see a screenshot here.  This PID is the core audio daemon process, and we have that function called after initialization.  And then the fuzzer was started.  I hooked on to the core audio daemon process with LLDB just to show that our fuzz function  did actually get called here.  So that's pretty cool.  We can call a function in the already initialized process that we care about.  But we're going to need to do more than that.  As we just talked about, we need to manage our corpus.  We need to mutate our inputs and generate new inputs.  We need to get code coverage, and we need to identify crashes.  So we've got to inject more than just a simple harness function call.  So this is not a trivial task.  As I'm finding out, this is something I'm still working on and is in progress.  But one thing I want to recommend is LibAFL.  It's a really great project.  It's maintained by the maintainers of AFL++, the kind of state-of-the-art fuzzer out there  in the open source community.  And it's a library that allows you to slot custom components of fuzzers together using  those same state-of-the-art approaches as AFL++.  So what this really does is it helps us inject fuzzers in interesting places.  Things you couldn't normally fuzz are a lot easier to do when you can create a custom  library and inject them with something like Frida.  So now, instead of our core AudioD process doing its setup and then just waiting for  mock messages, we instead have it spawn LibAFL and our fuzzer and just start looping forever,  getting code coverage, and hopefully causing lots of crashes.  Okay.  So we talked about fuzzing and how we can improve the fuzzing process.  The last thing I want to talk about here is how do we actually identify crashes that are  relevant and ones that we should care about?  There's some heuristics that we can think about that are really helpful.  If there's a crash on write or on execution, an illegal instruction exception that gets  thrown, a heap corruption, or if there's a stack trace where a back trace contains free  or malloc, those are things we really care about.  And as I'm triaging crashes, those are things I'm going to go back to because they might  be exploitable.  Things that are likely not exploitable are a crash on a read, although you could leak  memory addresses through this, a handled exception, a null pointer dereference, or a stack recursion.  So those are some basic heuristics you can use.  There are a couple of tools that can help you actually sift through crash dumps and  figure out which ones you might want to look at from an exploitation point of view.  One is Apple's Crash Wrangler, and the other is, I believe this is written in Node.  It's a bit easier to use, and a really great tool.  It's called Crashmon.  Provided the links for both of those here.  One important thing also I want to mention is crash reproducibility.  So as you're sifting through crashes and you find something that looks interesting, you  should always make sure that you can run that input back through your harness and reproduce  the same crash.  Because an exploit that is not reproducible is not really relevant.  Okay.  So let's take a step back and look at what we've covered.  I know that was a lot of material.  Gave a quick crash course on what fuzzing is and mock IPC mechanisms.  We did a walkthrough of the fuzzing process as I like to go through it.  Writing attack vectors, generating a corpus of fuzzing inputs, writing a custom fuzzing  harness, fuzzing and producing crashes, and triaging those crashes.  I talked about a couple of common pitfalls and things to consider, and what I really  hope I did is inspired you to do your own vulnerability research, leveraging fuzzing.  Three personal fuzzing tips that I've learned through this research project.  The first one I mentioned already.  If you can, try to get your fuzzer as close to the target function that you want to call  as possible.  This just makes it run way faster.  It makes things easier to triage, and it'll just make your life a lot easier.  Another interesting take I have is I think that the harder something is to fuzz, the  more likely there's actually going to be vulnerabilities there.  If you're trying to set up a fuzzer, you're having a bunch of difficulties, which you  will, because it's never easy to fuzz something for some reason.  Don't give up from that, because it likely means there's something there, because someone  likely hasn't put in that effort to fuzz it yet.  And then finally, definitely leverage code coverage to inform what fuzzing modifications  you can do.  Like I mentioned, you can see exactly where you're getting in the code.  You can see which branch conditions aren't being taken, and where you should dedicate  time to try to get past.  Thank you so much.  I've listed my Twitter here and my personal blog.  Feel free to reach out, and I'm also happy to take questions now.  Thank you very much for this entertaining and interesting talk and presentation, and  yeah, are there any questions in the audience?  Thank you very much for your presentation.  The question I had is that, well, from what I understood, you start with the IPCs as a  nice way to find potential pre-registration or sandbox escapes, but after a moment you  just hook onto starting an initialization function.  And so I don't understand how then from an IPC, well, from a MAC handler, you can, well,  do the previous, because you are hooking into an initialization function.  So I don't see the bridge between the two.  Yeah.  Yeah, that's a great question.  So what I was explaining is, yes, when you create your end exploit, you will need to  call that mock message function.  You will need to send a mock message across the wire.  It's going to need to be processed by the kernel.  It'll then end up in your target function.  So for fuzzing purposes, we cut out the middleman.  We cut out the kernel, which is delivering the message, and we go directly to where the  message is being processed.  That way we can produce a crash, and you're absolutely right that sometimes it is more  difficult once you have a crash to get back to, okay, now I need to go back to Safari  or whatever process and send a valid mock message to the process we're targeting.  Sometimes it's difficult, but yes, when you're fuzzing, you want to get as close as you can  to that target function, and then you need to backtrack often when you're creating that  exploit.  Thank you.  Of course.  Good question.  I have a follow-up question for that one.  How difficult would it actually be to, if you have the full chain, including the kernel,  to filter in the kernel these mock messages to prevent such a kind of attack?  Sorry, so you're saying have some sort of filter in the kernel?  I'm not very fluent in macOS, but if that's possible, yeah.  Yeah, that's a good question.  I mean, you know the signatures.  It's defined, kind of.  Yeah.  That's a great question.  I'm not sure.  That's not something that I've personally looked at.  I definitely think there's detection opportunities for inspecting mock messages coming through.  I definitely agree with that.  It does get difficult, though, because there are a lot of legitimate reasons to send mock  messages, as we looked at, and you can just pop open a system and see thousands of them  being sent per second.  So it is difficult to detect, because there's a lot of legitimate traffic.  But I think certain heuristics, really long messages being sent, maybe could be used to  identify buffer overflows if one exists.  And follow-up question, did you, on purpose, decide against snapshot fuzzing?  It's a great question as well.  Yeah, I didn't mention snapshot fuzzing.  Didn't quite have time, but snapshot fuzzing is a really cool, kind of pretty new, state-of-the-art  approach where you take snapshots and you revert your state, sort of each fuzzing cycle,  to a known state.  This is really great for network protocol fuzzing, because you can take a look at incoming  messages, and then if you get additional code coverage, you can go back to that state  and continue fuzzing from there.  Snapshot fuzzing on macOS is a bit of a pain in the ass, which is the reason I didn't go  through with it.  I do know there is a project, I forget right now where it's coming from, but it was with  using the VMware Snapshot API on VMware to actually perform snapshot fuzzing for macOS.  That's been a project I've been meaning to look more into, and it looks very cool.  You also can use Nix, which is the main snapshot fuzzer.  You do need to set up Hackintosh on KVM, as far as I'm aware, which I have not gone through  the trouble in doing, but you certainly could use that approach as well.  Yeah, good questions.  Do you know of approaches to automate the increase in coverage, maybe symbolic execution  or something?  Yeah, another great question.  Symbolic execution is definitely something that I would like to use further along in  the research.  That's taking a look at the source code, if you have it, or the binary and looking at  the branch conditions and seeing what inputs might get us to a certain spot in the function.  Absolutely, to answer your question, that's something that I want to continue looking  into.  Additionally, I don't know.  I think there's going to be interesting applications of artificial intelligence in the future.  A little more difficult with binary analysis, but honestly, the AIs do a pretty decent job  with assembly.  So I think, yeah, symbolic execution and AI in the future is going to really help with  honing in fuzzers.  Actually, I believe Google has a project they've incorporated into OSS Fuzz, where they're  using AI to generate automatic fuzzing harnesses for members of OSS Fuzz, which is pretty cool.  So I definitely think we're starting to see a little bit of that already, but I think  we have a long ways to go still.  Thank you.  Of course.  Any other question?  Maybe I have one, because I didn't know libafl existed.  I also built a fuzzer with Frida in the past doing in-process fuzzing.  Does libafl do coverage for you, or do you still do that in Frida when you build your  fuzzer?  It does do coverage, yeah, which is really cool.  Cool.  Yeah.  I will look into that.  It's a cool project.  I'm still learning it as well, but I think it's got a lot of applications.  They also have a lot of example fuzzers, so you can look and find one that might be useful  for your unique circumstance.  Awesome.  Thank you.  Yeah, of course.  Okay.  If there are no questions left, give Dylan another round of applause.  Thank you again.  Thank you all so much.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.