{
  "webpage_url": "https://www.youtube.com/watch?v=WC-tY-gEIPc",
  "title": "DEF CON 32 - Hacker vs AI perspectives from an ex spy - Harriet Farlow",
  "description": "The convergence of Artificial Intelligence (AI) and national security not only fuels international discourse but also inspires narratives within popular culture. Harriet is no stranger to these myths, as an ex-intelligence professional who specialized in applying machine learning to cyber security. In fact, she likes to lean into them. This makes her previous bosses nervous, so she uses pop culture as the lens through which to communicate her insights - and in this talk she utilizes the worlds of Ghost in the Shell, Neuromancer and Mission Impossible.\n\nThrough these stories, as well as her own decade of experience working at the intersection of artificial intelligence and cyber security, Harriet discusses the extent to which fears surrounding AI systems are applicable to real life national security settings. From cyber warfare to AI-driven surveillance, she unravels the interplay between hackers, AI, and government agencies. This session is interactive, with demos of how these AI systems actually work under the hood, as well as discussion time. Blur the lines between human and machine, and understand how you can contribute your skills to prevent our own modern day Puppet Master.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2439,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 5.24s | This text was transcribed using whisper model: large-v2

 Okay. We're starting. Um, it was really wild walking past a queue to get into this room
5.24s - 10.80s |  to see me talk. I mean, I, I know I'm, I'm obviously ex intelligence, but I'm not going
10.80s - 16.20s |  to be divulging any state secrets here. In fact, definitely the opposite because obviously
16.20s - 22.16s |  like someone is watching this, so I don't want to get in big trouble or arrested. Um,
22.16s - 28.36s |  but it's, it's amazing to see you all here. So thank you. Um, so this is a talk about
28.36s - 35.16s |  the intersection of AI and security specifically from the national security lens. Um, this
35.16s - 40.40s |  is a policy village talk that is up here on the creator stage. So there is a policy slant
40.40s - 46.60s |  to it. So if you're expecting to see cool like hacking techniques or how to evade an
46.60s - 52.68s |  APT, this is not the talk. Um, although maybe I will talk about it. Um, but I hope you're
52.68s - 56.68s |  all here because you're interested in policy in some way. Could I get a show of hands if
56.72s - 62.48s |  you are like a policy person? Okay. That's good. That's good. And what if you're more
62.48s - 68.64s |  of a technical person, like a cyber or an AI person? Okay. So probably a bit more, but
68.64s - 73.68s |  a good mix. Okay. That's okay. Um, hopefully this is interesting for you. I don't know.
73.68s - 80.08s |  Um, we'll see. So who am I? Um, my name's Harriet Farlow. I've worked at the intersection
80.08s - 86.72s |  of artificial intelligence and security for about 10 years now. My undergraduate degree
86.72s - 92.00s |  was in physics and like many physicists, I didn't really know what to do with it. So
92.00s - 98.68s |  I ended up working in data science. Why is there a laugh there? Um, so I ended up working
98.68s - 104.80s |  in data science, um, mostly specializing in defense projects, um, in Australia. So I spent
104.80s - 109.48s |  a year working on the patrol boats in Darwin. If anyone's familiar with Darwin, it's, it's
109.48s - 113.96s |  a small town far North in Australia where there's obviously a lot of military activity
113.96s - 118.76s |  and crocodiles and that's sort of what's going on, but an insane number of British backpackers
118.76s - 123.88s |  and it's, it's, it's loose all the time. Um, and I also worked on sort of air force projects
123.88s - 130.40s |  where we were augmenting people's roles with, um, like robotic process automation. Um, ideally,
130.40s - 134.80s |  hopefully not to, you know, take people's jobs away. Right. But just to make them not
134.80s - 141.08s |  have to do the, the uncool bits. Um, I worked there for a while, um, had my, you know, quarter
141.08s - 145.56s |  life crisis ended up working at a startup in New York city, which is really cool. And
145.56s - 152.56s |  I was also working on sort of advising large companies in the United States on AI and cybersecurity.
152.56s - 158.28s |  Uh, unfortunately during COVID I had to move back home. Um, and the Australian government
158.28s - 163.44s |  was the only people hiring. So that's how I ended up working in intelligence. Um, not
163.44s - 167.08s |  to make it sound like I didn't want to, I was really grateful to, and it's something
167.08s - 172.08s |  that I'd always been really interested in. Um, and I guess COVID just forced me into
172.08s - 177.12s |  that opportunity. Um, but it was really great. I worked at the Australian signals directorate
177.12s - 182.76s |  in Australia, which is our equivalent of the NSA. I always get scared about saying the
182.76s - 187.64s |  wrong thing. So yeah, like this is not going to be the spiciest talk obviously. Um, because
187.64s - 191.64s |  I'm scared of saying the wrong thing. So this is, this is ASD and this is what they do.
191.96s - 195.52s |  They're the Australian government intelligence agency responsible for foreign signals, intelligence
195.52s - 200.68s |  and cybersecurity. And they do all those things. And so over the time I was working there,
200.68s - 206.12s |  I was mostly a data scientist in the cybersecurity teams. Um, and by the time I left, I was acting
206.12s - 211.72s |  as a technical director on one of their sort of AI streams. And through this job, I got
211.72s - 217.88s |  to travel to Canada and the USA and lead some of the technical work around AI in that space
217.92s - 223.88s |  there as well, which is really great. Um, fantastic organization, fantastic people believed
223.88s - 228.16s |  in the mission wholeheartedly, but I guess the entrepreneurial spirit never really left
228.16s - 233.20s |  me. Um, and while I was there, I started doing my PhD in adversarial machine learning and
233.20s - 240.20s |  investigating ways to hack models and basically found that in the, like the, the, the private
240.20s - 244.24s |  sphere, there was just not that much activity happening in that space, even though there
244.28s - 249.96s |  was, um, you know, that it represented a real threat. So I left my government job and for
249.96s - 255.56s |  the last year I've been running an AI security company called Maleba Security Labs. Um, we've
255.56s - 259.72s |  just received funding to start building a tech product, which is exciting, especially
259.72s - 264.60s |  because there's, there's so much grind. It's, it's a long time, but that's who I am. Um,
264.60s - 270.44s |  I also have a social media presence at Harriet Hacks, which is not amazing, but I'm trying.
270.48s - 277.20s |  Um, I'm there on YouTube. Um, you can reach out to me on, uh, like X and other platforms
277.20s - 282.08s |  and we have a podcast now, which is really cringy, but, um, anyway, you know, as an
282.08s - 286.96s |  entrepreneur, you just got to do what you got to do. Um, so I've been doing quite a lot of
286.96s - 294.76s |  research at the intersection of AI and security for a while. This is my fourth talk this
294.76s - 300.96s |  week. So if I am a bit like foggy or tired, I apologize. It's been a massive week and I
300.96s - 306.36s |  don't think I've ever needed a drink so much as I will right after this talk. Um, but
306.36s - 310.68s |  yesterday, um, well, I spoke at B-Sides and then the main stage yesterday and the AI
310.68s - 317.64s |  village yesterday, um, on some of the methods that are used and new methods to, uh, hack
317.68s - 322.40s |  facial recognition AI models. Um, and that's in the field of adversarial machine learning,
322.40s - 327.80s |  which is my PhD topic. And in particular, um, the, the content I was talking about was in
327.80s - 333.04s |  the context of urban camouflage. That's sort of how I envisioned it, um, at the time that
333.04s - 338.00s |  I, they created that method. Um, but basically the idea is that machine learning models are
338.00s - 342.60s |  pretty brittle and easily exploitable and that most of the organizations that are
342.60s - 348.44s |  leveraging machine learning, um, are relying on pretty insecure models, even though lots
348.44s - 353.52s |  of organizations are doing the right thing. Like it's, it's really cool to see, um, AIXCC
353.52s - 358.00s |  if you've been there and all the companies like, um, Google, Microsoft, Anthropic, Open
358.00s - 362.56s |  AI that are working on secure AI. But for all the other organizations that are creating
362.56s - 367.68s |  AI products, there's no real mandate for them to actually be secure. And, and that's part
367.68s - 372.12s |  of the problem. Um, some other talks were showing that, but I also do research, um,
372.12s - 378.36s |  on policy as well and applying that in the policy sort of lens. And maybe it sounds really
378.36s - 384.40s |  bad, but I'm far more excited to deliver this talk than the other ones. Um, I, I never
384.40s - 389.52s |  really thought that I would consider myself a policy nerd. It always seemed pretty dry.
389.52s - 394.52s |  Um, especially when you work in the bureaucracy and you have to try and create policy, like
394.52s - 399.64s |  that just sucks. But it's actually really interesting to think about the, the impacts
399.64s - 404.00s |  of policy because it is real. Um, so now if someone called me a policy nerd, I would probably
404.00s - 409.08s |  actually be quite happy about that, uh, which is an identity crisis slash shift I guess
409.08s - 414.72s |  I've gone through. But that sort of leads us into the objectives of this talk, which
414.72s - 419.20s |  I hope it's not too theoretical for you. It kind of has to be, but I find it really interesting
419.20s - 422.84s |  actually. So maybe if there's something you take away from this, it's that policy can
422.84s - 428.68s |  actually be quite interesting and important. Um, so I'm going to cover some of the different
428.68s - 435.68s |  ways that AI, um, is used in a national security context, cyber defense and offense. I'm sure
435.68s - 439.84s |  this will not be surprising to you at all. Um, so I'll just go over that lightly, but
439.84s - 444.08s |  then I'll talk about some of the national security implications from that, uh, from
444.08s - 448.88s |  my perspective. And then you might be disappointed by this as well, but I'm going to talk about
448.88s - 452.80s |  security theory because I actually find it really interesting. And I think it's really
452.80s - 459.44s |  interesting to see how traditional security theory at stemming from sort of military sciences
459.44s - 465.44s |  has moved into cyber security, but hasn't really moved into AI security yet. And, and
465.44s - 472.44s |  how we can sort of encourage that change to make sure that AI also is secure, um, as,
472.44s - 479.16s |  as our other, other, as our other disciplines. But first, so this is an AI, uh, pop the,
479.20s - 483.92s |  the policy village talk and they told me in submitting this talk that it had to be interactive
483.92s - 489.08s |  with the audience and then it had to encourage discussion. So I, I feel like maybe that's
489.08s - 494.08s |  a bit different as a creative stage like this. I think like maybe in the policy area, that's
494.08s - 498.64s |  a bit easier, but I do still think it's important actually. So maybe take 30 seconds and say
498.64s - 504.00s |  hello to the person next to you. Um, you can explain why you're interested in policy and
504.00s - 507.36s |  then note your favorite sci-fi movie. If you read the description, obviously I'm talking
507.36s - 511.28s |  about sort of leaning into sci-fi here. So I'm going to break for 30 seconds and you,
511.28s - 581.83s |  you can meet the people around you. Okay. Well, that's a minute. I hope everyone found
581.83s - 587.91s |  that valuable. Okay. We can pause the talking now. We're going to, we're going to kill the,
587.91s - 592.47s |  the audio for the, for the recording. Okay, great. Well, I've ticked that box for, for
592.47s - 596.39s |  the policy people. So I hope you found that useful as well. Um, that is the benefit of
596.39s - 603.65s |  being here, right? Um, I, I don't, I don't think I'll try audience engagement. I was
603.65s - 608.85s |  going to say, can someone yell out their favorite sci-fi movie that might work? Terminator.
608.85s - 621.16s |  Yes. Classic use of AI. Anything else? Star Wars. Yes. Blade Runner. Fantastic. Well,
621.16s - 626.52s |  all of them involve AI, right? Which is, uh, uh, I guess funny, but sort of the point of
626.52s - 631.68s |  why I wanted to involve sci-fi in the talk. Um, also because then it's easy to talk about
631.68s - 637.56s |  stuff without necessarily saying that, um, you know, it comes from experience. I should make
637.56s - 642.20s |  the disclaimer, actually, all of this is my own opinion and nothing from ASD. Um, I forgot to
642.20s - 647.24s |  mention that earlier. Um, but that is the benefit of leaning into the science fiction narrative as
647.24s - 652.24s |  well, because all of those movies that you've referred to do involve artificial intelligence
652.36s - 659.24s |  and see the way that it's depicted in movies really does. I'm sure that will come back. It
659.24s - 663.40s |  really does impact how people think about it and the kinds of narratives that people have about it.
663.40s - 668.88s |  Um, as well as sort of in the world of intelligence, right? All of the things we think about when it
668.88s - 672.60s |  comes to national security and intelligence, all of, a lot of the time, unless you've worked in
672.60s - 676.52s |  those kinds of fields or with people in those fields come from movies, it's, it's really
676.64s - 682.64s |  impactful. Um, and there have been quite a lot of studies that show that the, the way that
682.64s - 689.04s |  technology is constructed in movies is how it progresses. Not necessarily, but the, the
689.04s - 695.12s |  creativity that's shown in, um, in narratives like this ends up, you know, informing people
695.12s - 699.72s |  who then go and design them themselves. We've seen that in sort of biotech. So this is
699.72s - 703.32s |  important when it comes to AI, cause we need to think about it. But these are some of my
703.36s - 708.24s |  favorite science fiction. Oh, those were some of my favorite science fiction movies. I think
708.24s - 714.68s |  it'll come back. Um, there we go. Um, they all also involve AI. Um, if you've read the
714.68s - 717.68s |  description, which I can't really remember what I put in it actually, but I think I
717.68s - 723.72s |  referenced, um, Ghost in the Shell, Neuromancer, um, Mission Impossible, not really a sci-fi
723.72s - 728.80s |  movie, but it's a, an action movie that I do quite enjoy. Um, especially the last one
728.80s - 732.84s |  because it involved some examples of hacking AI, which I really like and is the kind of
732.84s - 737.08s |  thing that I do and had not seen in movies very much before. I also put Doctor Who up
737.08s - 741.32s |  there because I'm not talking about it today, but it is my favorite science fiction show
741.32s - 745.68s |  slash content. My, um, I don't know if you can tell that I was a nerd because I studied
745.68s - 754.64s |  physics, but my sweet 16th birthday party was a Doctor Who themed party. And it's even
754.64s - 758.08s |  amazing that I had friends to come to this at the time, but they all came dressed up as
758.08s - 763.56s |  different Doctor Who characters. Um, and I was Rose of course, and I made my own custom
763.56s - 768.60s |  Doctor Who Cluedo game. Um, I created all the different cards and characters myself
768.60s - 773.60s |  and then stuck them on top of my original Cluedo. So I guess we're all impacted by science
773.60s - 778.76s |  fiction in some way. And now you are too because of that really terrible story. But I'm going
778.76s - 784.12s |  to be sort of discussing some of the implications of the technologies that we see in these movies.
784.16s - 788.64s |  Of course there is an important question we have to ask, and what is AI at all? Have any
788.64s - 794.60s |  of you in here been to the other talks I gave this week? Okay, because I've, there's a really
794.60s - 797.92s |  bad joke that I have about what AI is and I don't know whether to give it again because
797.92s - 806.16s |  I've given it three times already this week and it's really bad. Okay, so when I first
806.16s - 814.38s |  told my mum I work in AI, she said, oh darling, why do you work in artificial insemination?
814.42s - 819.22s |  Which is really like, it's not a good joke. Um, but it's because she works in medicine
819.22s - 824.50s |  and that's what AI meant to her. And she wasn't even saying that to be funny. That, that's
824.50s - 829.78s |  literally the first thing that came to her mind. Um, and so even when I say AI to all
829.78s - 834.26s |  of you, you think artificial intelligence, what that means to different people is really
834.26s - 838.94s |  different. So some people would think something like the Terminator or Skynet. Um, other people
838.94s - 844.06s |  would think like just a decision making algorithm. Um, I have friends who aren't really tech
844.06s - 848.30s |  people and I was talking to them about AI and they said, oh yeah, I saw this really
848.30s - 853.02s |  fancy like 3D billboard in the street. That's an example of AI, right? So I think we also
853.02s - 856.62s |  take it for granted that, you know, maybe we're a bit more technically literate than
856.62s - 862.10s |  most people out there. Um, including people who work in sort of, um, you know, policy
862.10s - 868.62s |  and a lot of, a lot of government jobs, uh, which is not, you know, not a, not to discredit
868.62s - 873.02s |  their work or anything. It's just that it's such a new and emerging technology and we
873.02s - 877.14s |  need to have a bit of empathy for the people who really aren't there. But I guess when
877.14s - 884.06s |  I talk about AI, when we think about it, artificial intelligence really refers to a goal and the
884.06s - 888.14s |  term artificial intelligence has been around since the 1950s when John McCarthy coined
888.14s - 894.02s |  the term for the Dartmouth conference in 1956 and he was a cybernetics researcher. And so
894.02s - 897.86s |  like he was trying to get grant funding for his cybernetics research, which is basically
897.90s - 903.10s |  the ability to just like create models. Um, and at the time in the 1950s that was pretty
903.10s - 909.62s |  simplistic, but over the years, whatever we've referred to as AI has just referred to the
909.62s - 914.18s |  cutting edge, right? So I guess these days you, you would consider AI something like
914.18s - 919.66s |  LLMs and chat GPT, but in 10 years time, what we consider AI will, will change as well.
919.66s - 926.38s |  So the goalposts are always moving. The other thing about definitions in AI is also that
926.38s - 931.86s |  when I say AI security, so like when I say I run an AI security company, what that means
931.86s - 938.14s |  is also very different. Most people assume AI security means AI for cybersecurity. Um,
938.14s - 941.90s |  whereas for me and other people who sort of work in the field, what I really mean is the
941.90s - 947.26s |  security of AI systems themselves. And they're very different, like they're inherently different.
947.26s - 953.26s |  It's either using AI to do something, um, like cybersecurity applications versus like
953.34s - 958.46s |  implementing or adopting security methodologies into the world of AI, which are like very
958.46s - 962.38s |  different to how you would implement it in a cybersecurity setting. And then another
962.38s - 967.82s |  term is AI safety, which some people would consider to be kind of the same as AI security.
967.82s - 973.34s |  And I would consider quite different. Um, and why that is, I will sort of get to in a moment
973.34s - 979.82s |  as well. But when we think of all the different AI uses in like security, we can sort of group
979.82s - 984.94s |  them into offensive and defensive security. So in terms of the different job roles,
984.94s - 990.06s |  uh, we have things like ethical hackers, red teamers, exploit developers, malware analysts,
990.06s - 995.18s |  vulnerability researchers, social engineers, um, just to name a few. These are all the kinds of
995.18s - 1001.90s |  roles that are now being, um, you know, supplanted or augmented by AI technologies.
1003.26s - 1006.94s |  When we think of defensive security, we have things like security analysts,
1006.94s - 1010.94s |  network security engineers, security architects, incident responses,
1010.94s - 1016.94s |  sock analysts, and threat hunters. Um, again, just to name a subset. And there is so much work
1016.94s - 1024.38s |  and investment going on in this space at the moment. Um, again, looking at AI CC in the,
1024.38s - 1029.50s |  in the hall below, which is insane and massive and so much funding. And I just can't imagine
1029.50s - 1034.94s |  that happening in Australia. Um, again, not to bash Australia, but just like, this is such an
1034.94s - 1041.02s |  incredible investment from the government in that. So that's really cool. And the challenge
1041.02s - 1048.14s |  about applying AI in these kinds of settings in any, in any organization, national security or not
1048.70s - 1054.78s |  is that like actually building the model itself is pretty, pretty simple, but making it work,
1054.78s - 1060.94s |  making it work as part of a system is really hard. Um, I do sort of have a demo. Um, I guess
1060.94s - 1068.22s |  it's worth showing, but I think that the point of the demo is really just to show how simple it is
1068.22s - 1076.86s |  to build like a model that, that does this kind of thing. So as part of our work, we're often
1076.86s - 1083.50s |  thinking about like the intersection of AI and security, we do demos, labs, all that kind of
1083.50s - 1089.10s |  stuff. Um, we, we were originally like trying to apply for AXCC, but like we're a small company,
1089.10s - 1095.42s |  we couldn't quite get it together enough to do it in time. Um, but as a, as a demo, we decided to
1095.42s - 1101.66s |  build a model that could classify vulnerabilities in code. Um, I know a lot of you might not be
1101.66s - 1107.10s |  code people. The point of this is just to show that in the, like this is a two time speed, but
1107.10s - 1115.02s |  in the three minutes that this notebook takes to run, we were able to, um, down, we were able to
1115.02s - 1123.50s |  download a bunch of code, secure and insecure code, um, build a model that was able to classify
1123.50s - 1130.38s |  which of this code was secure or insecure. Um, run that a few times, you can sort of see this
1130.38s - 1136.94s |  training process here. And then when it was presented new code, it could correctly classify
1136.94s - 1143.42s |  whether it was vulnerable or not. Um, and that's the whole point of the AI XCC challenge, but
1143.42s - 1149.10s |  actually applying this at scale is really, really hard. And so some of the troubles that we see in
1149.10s - 1154.70s |  applying AI in all of these settings is this whole idea of like moving beyond the pilot stage,
1154.70s - 1158.38s |  like actually getting these sorts of models to speak together is really difficult and it's not
1158.38s - 1164.78s |  trivial. And it's something that we're all grappling with. Um, and the reason that it's
1164.78s - 1170.38s |  important from a sort of policy perspective, I guess, is because like the kinds of organizations
1170.38s - 1177.82s |  who are working on this stuff, um, you know, the big tech giants, um, as in like who have that sort
1177.82s - 1183.02s |  of that AI capability and have the compute to be able to do this. Um, this is a really novel
1183.02s - 1187.50s |  challenge, right? Like in the past we've seen governments have quite a lot of power when it
1187.50s - 1192.86s |  comes to the technologies that are challenging. So you've probably heard of like the crypto wars
1192.86s - 1199.10s |  in the nineties and the noughties and like the, the challenge that different governments and
1199.10s - 1204.78s |  national security organizations were having around people using end-to-end encryption and
1204.78s - 1210.78s |  their inability to sort of get through that and how they can now, um, you know, mandate different
1210.78s - 1216.30s |  bodies to be able to either like bypass that, or they were trying to do things like, um, mandate
1216.30s - 1221.42s |  implants into different technologies. In Australia in particular, there was some recent controversial
1221.42s - 1227.18s |  legislation that would also sort of mandate companies to, or at least provide the ability
1227.26s - 1235.02s |  to mandate companies to open those, those back doors as well. But being able to like do that in
1235.02s - 1239.50s |  an AI setting, when we're talking about like applying these models to really difficult use
1239.50s - 1245.42s |  cases, um, is, is a real challenge that we're seeing in the policy space at the moment. Like
1245.42s - 1251.66s |  how do we actually do that? How do governments cooperate with, like with these different
1251.66s - 1257.34s |  companies? And that's something that we're trying to figure out now. Um, so if you consider that a
1257.34s - 1265.42s |  bit of a summary of how AI for security in an offensive and defensive, um, like fashion is used,
1265.42s - 1269.66s |  um, of course I feel like I have to touch on hacking with AI. There have been some really
1269.66s - 1276.86s |  high profile cases of, um, APT groups, so Advanced Persistent Threats, um, trying to use,
1277.58s - 1283.90s |  um, you know, AI created by big companies like OpenAI and Anthropic to do hacking for them and to
1283.90s - 1290.62s |  augment this. Um, is this a real threat? I mean, yes and no. I think the way to think about
1290.62s - 1295.26s |  artificial intelligence is that it augments existing efforts, right? So we've seen quite
1295.26s - 1302.30s |  a lot of hype around how this can be, um, I guess a revolutionary challenge, um, and how it's a,
1303.26s - 1309.26s |  a really big deal and maybe that's not really the right way of thinking about it but, but thinking
1309.26s - 1315.26s |  about it as more of a tool because we're still sort of waiting to see, like, the kind of
1315.26s - 1322.86s |  impacts that people are promising we might see. Um, there are two examples of, like, APT groups
1322.86s - 1328.06s |  that have been using AI that I can sort of think of off the top of my head. Um, Russia and Iran,
1328.06s - 1333.98s |  for example, have been doing it to create deepfakes, um, to sort of augment existing,
1334.78s - 1342.22s |  um, you know, ways to make money for them, um, I guess as a way of sort of putting it bluntly.
1342.22s - 1347.18s |  Um, but I think the right way to think about it, again, is that it's part of the kill chain. It's
1347.18s - 1355.10s |  not necessarily a new, a new technology, um, or a new threat but it's really how it augments the
1355.10s - 1364.65s |  existing kill chain and amplifies those sorts of efforts. I hope the, I can't tell how often
1364.65s - 1369.61s |  the screen is doing that so I hope it's not too annoying. Um, we've seen examples of this from
1369.61s - 1375.37s |  all of the different APT groups, um, to different extents, um, and this is just a selection of them
1375.37s - 1383.53s |  as well. But I, I feel like quite a lot of talks do focus on things like AI for hacking and I,
1383.53s - 1388.17s |  I don't want to go down that rabbit hole because that's not really the thing that I'm interested
1388.17s - 1394.65s |  in. The thing I'm interested in is more of the, the strategic so what, um, which again might not
1394.65s - 1399.37s |  be all that interesting for some people but the thing I care about is like how do we actually
1399.37s - 1404.33s |  approach all those different issues of, you know, AI for security and security of AI because
1404.33s - 1409.61s |  especially working in an AI security field, I see there is so much talk about AI for security,
1409.61s - 1412.81s |  there's a lot of investment, there's a lot of smart people thinking about it, I don't think
1412.81s - 1417.45s |  that's really the problem, problem at the moment. I think the problem right now is that not many
1417.45s - 1421.61s |  people or not enough people are talking about the security of the AI systems themselves.
1422.89s - 1429.77s |  So let's think about how we hack an AI system at all. So I guess you could say that the idea
1429.77s - 1435.29s |  of hacking an AI has been around as long as AI systems have themselves, you know, especially
1435.29s - 1440.89s |  whatever we define as AI, right? So the first example that comes to mind is a simple algorithm
1440.89s - 1445.69s |  like an email spam filter. As long as those have been around, people have been trying to evade
1445.69s - 1452.49s |  spam filters and you could think of that as an example of hacking an AI system. When we think
1452.49s - 1457.93s |  about the modern or the the most recent implementation of AI hacking, it's the field
1457.93s - 1462.81s |  of adversarial machine learning which is sort of my field and the topic of my PhD and my other research
1463.53s - 1467.93s |  but it's basically ways that you can exploit the inherent characteristics of machine learning
1467.93s - 1472.57s |  models and machine learning models are the underpinning of most or all AI systems at the
1472.57s - 1479.29s |  moment and so in the real world we're starting to see these kinds of attacks possibly come to
1479.29s - 1485.05s |  fruition. It's always been considered a very academic discipline. This is sort of the classic
1485.05s - 1489.37s |  adversarial machine learning example. Can I get a show of hands if anyone's like seen this before
1489.37s - 1496.25s |  or familiar with adversarial machine learning? Okay, so it's definitely entering more of the
1496.25s - 1501.21s |  discourse at the moment but the idea at its heart, and this is from a paper about 10 years
1501.21s - 1506.81s |  ago now, so it's it's new but it's not that new. The idea was that you could take a clean image,
1506.81s - 1513.45s |  so of this panda, and you could create special noise that when superimposed on top of that image
1513.45s - 1518.73s |  prevents a model from recognizing it and the idea is that this this noise or these adversarial
1518.73s - 1524.65s |  perturbations, this adversarial example, is crafted specifically with the target model in mind and
1524.65s - 1531.37s |  that you're able to constrain it within a small epsilon value so that a human can't recognize it.
1531.37s - 1535.45s |  It's just enough that it passes a sort of classification boundary according to the model
1535.45s - 1540.81s |  but not so much that it's recognized and this is sort of the, I guess, the proverbial example that
1540.81s - 1545.53s |  most people show when they talk about adversarial machine learning and it's always been considered
1545.53s - 1550.33s |  a bit of an academic threat. Like it wasn't necessarily taken seriously. When I told my
1550.33s - 1554.89s |  PhD supervisors this is what I wanted to work on, they were like, I don't know if this will ever
1554.89s - 1558.89s |  become a thing, you know, it might not, you know, you might be shooting yourself in the foot, this AI
1558.89s - 1566.73s |  security thing. Um times have changed. It was also Australia. Again, love Australia but it's I'm not
1566.73s - 1574.25s |  trying to, yeah, it's it's it's always um yeah it's a smaller market. Um so now we're starting to see
1574.89s - 1580.81s |  more examples in the wild as you'd call it. So things like being able to paint special paint
1580.81s - 1586.01s |  onto a stop sign so that like an autonomous vehicle doesn't recognize it and would drive
1586.01s - 1591.77s |  straight through. And this is just in computer vision, right? So computer vision is a kind of
1591.77s - 1596.01s |  machine learning where you're looking at the world around you and identifying what it is.
1596.65s - 1600.49s |  Um but when we think about all of the different use cases of AI, there's things like natural
1600.49s - 1606.57s |  language processing and LLMs, um signal classification, RF, radio frequency, audio
1606.57s - 1610.65s |  waves, those are all use cases for machine learning where all of these kinds of attacks
1610.65s - 1616.73s |  apply as well but they're harder to see, like they're even harder to see. And if we think about
1616.73s - 1622.01s |  machine learning systems as well you know you'd call them an attack surface, right? As you'd use
1622.01s - 1626.89s |  the the cyber sort of lexicon. You can think about all of these different attack surfaces that have
1626.89s - 1631.45s |  their own unique characteristics. All the different kinds of models that do different things.
1631.45s - 1636.01s |  So at the top we have a convolutional neural network which is a kind of machine learning
1636.01s - 1640.73s |  model that's really good for computer vision because it decomposes information down really
1640.73s - 1645.77s |  easily. Um and then on the bottom we have a transformer which is the backbone of of GPT
1645.77s - 1649.85s |  models and is very good at natural language processing. Those are very different kinds
1649.85s - 1654.65s |  of models. Like they have some commonalities in the sense that it's all about optimization and
1654.65s - 1659.45s |  being able to you know predict things whether it's next words or images um but they're really
1659.45s - 1664.01s |  quite different. They're different attack surfaces and especially if we're adding these two cyber
1664.01s - 1669.53s |  systems they present entirely new threats. Like this is an extension of the existing cyber attack
1669.53s - 1675.85s |  surface um that is not covered by existing cyber security methodology. You know for the organizations
1675.85s - 1680.97s |  that are adopting AI without thinking about the security of them um they're they're opening
1680.97s - 1685.93s |  themselves up to new vulnerabilities. So this is sort of the example that I gave in my other talks.
1685.93s - 1693.29s |  It's being able to add different adversarial um like regions distributed regions to images so that
1693.29s - 1698.49s |  you can sort of hack facial recognition. You can do this with any kind of model really. Um the reason
1698.49s - 1702.97s |  I was so excited to see the recent Mission Impossible movie is because this is what they did.
1702.97s - 1708.33s |  I was it was really great. Um there was this scene where Ethan Hunt was running through an airport
1708.33s - 1712.97s |  and people were looking at the surveillance footage and someone was able to digitally
1712.97s - 1718.33s |  manipulate the surveillance footage so that he looked like somebody else. And the thing was that
1718.33s - 1721.69s |  they didn't need to do it in the real world because the people were only looking at the
1721.69s - 1726.89s |  surveillance footage. And it's the kind of thing where you know a lot of AML techniques or adversarial
1726.89s - 1731.45s |  machine learning techniques they were considered too academic because people couldn't really see
1731.45s - 1737.05s |  how it could relate to the real world. Like how do you put in a bunch of adversarial pixels in a
1737.05s - 1741.29s |  real world and move it around. But you know the point is that you don't always need to especially
1741.29s - 1746.01s |  now that machine learning models are doing a lot of this kind of stuff autonomously. You know
1746.01s - 1751.13s |  in the national security setting especially you know you have a lot of models that are being built
1751.13s - 1756.41s |  to do things like platform detection looking for uh you know tanks and ships and boats and things.
1756.41s - 1762.01s |  Um person identification um you don't always need to rely on having something in the real
1762.01s - 1765.77s |  world anymore. A lot of it's digital and a lot of this kind of stuff can be injected
1765.77s - 1772.97s |  just like in Mission Impossible. So this is where we get to something that I find really interesting.
1774.57s - 1781.37s |  So bear with me um there's some security studies uh I guess theory that is really interesting to
1781.37s - 1788.49s |  apply to AI and AI security. Um a few particular things are thinking about the referent object,
1788.49s - 1795.05s |  the direction of protection and securitization. Um so what do I mean by these things?
1796.25s - 1802.57s |  The referent object basically refers to the thing that you're trying to protect. Um so security
1802.57s - 1809.93s |  studies comes from sort of military security um and then is now being applied to like cyber
1809.93s - 1818.41s |  security um more rigorously. But it it basically means uh if we're creating um policy or legislation
1818.41s - 1825.93s |  or you know rules um what are we actually trying to protect? Um and usually in a in a national
1825.93s - 1830.65s |  security sense that would be the state so like a nation state um and if we're thinking about
1830.65s - 1837.61s |  cyber security it'd be like the cyber system, the cyber system. Um direction of protection
1837.61s - 1845.37s |  is all about what is if we think about what is being protected from whom. So if we think about
1845.37s - 1853.29s |  comparing AI for security versus the security of AI um this is one way of thinking about the
1853.29s - 1860.09s |  direction of protection um and it's a fundamental difference. The other one is AI safety versus AI
1860.09s - 1865.93s |  security. In the AI world there's actually quite a lot of debate about whether AI safety is different
1865.93s - 1873.05s |  to AI security and what safety versus security even means and it's fascinating to me maybe not
1873.05s - 1878.57s |  to everyone else but people in AI get really heated about this because people have very strong
1878.57s - 1885.21s |  opinions and it's often seen as sort of a philosophical question almost whereas to me
1885.21s - 1890.81s |  I I think it's pretty simple and it comes down to the direction of protection. Like to me AI safety
1890.81s - 1897.45s |  is protecting the environment from the AI system whereas AI security is about protecting the system
1897.45s - 1902.09s |  from the environment and that's how you'd characterize it in a military studies or a
1902.09s - 1908.25s |  cyber security approach as well. Like if we think about the cyber analogy, cyber safety is protecting
1908.25s - 1915.13s |  like people or users from um like cyber systems that might do them harm. So protecting them from
1915.13s - 1919.21s |  online bullying, making sure there's equitable access to the internet, things like that whereas
1919.77s - 1923.85s |  cyber security is about protecting the cyber system from external threat actors.
1923.93s - 1933.21s |  We then arrive at this concept of securitization uh and securitization basically takes the premise
1933.21s - 1938.33s |  that if something is considered an existential threat then a state is able to use extraordinary
1938.33s - 1947.53s |  measures to protect that thing. So a good example of this is the rise of um yeah cyber security.
1947.53s - 1956.17s |  So in uh say 2008 kind of times we were starting to see um the cyber attacks on Estonia uh and
1956.17s - 1962.73s |  Georgia being considered the first instances of cyber war and cyber warfare and this you know
1962.73s - 1970.17s |  enabled um the government to enact lots of cyber security laws and and measures because the threat
1970.17s - 1976.33s |  was suddenly considered existential when it's applied to say a nation state versus when it's
1976.33s - 1982.25s |  just applied to sort of the users. And it's interesting because we've seen this evolution
1982.25s - 1987.93s |  of that take place through terms like computer security versus cyber security. I mean the
1987.93s - 1994.17s |  discipline of protecting cyber systems used to be called computer security um but as it's gone
1994.17s - 1999.45s |  through this transition to becoming an existential threat and one which warrants additional laws and
1999.45s - 2005.85s |  regulations to protect it, it's become um cyber security. And there's other you know stories that
2005.85s - 2010.81s |  people have about the evolution of that as well. Um the point I'm making is that it's a transition
2010.81s - 2019.35s |  that has happened alongside the securitization of of computers and cyber systems. So when we
2019.35s - 2024.63s |  think about this in terms of AI policy there's a lot we can learn about how to make sure we're
2024.63s - 2032.23s |  creating good AI policy based on those principles and the evolutions that we've seen from cyber.
2033.19s - 2041.91s |  This is a a research paper that um my team has done recently which was um a lot of work um but
2041.91s - 2047.27s |  basically it's uh it it's looking at this, it's it's understanding if AI is going through a
2047.27s - 2053.43s |  securitization process. Um and so we did a review of public AI policy which is why I said it was a
2054.07s - 2064.31s |  lot of work. Um oecd.ai currently has a live repository of like all or at least um hopefully
2064.31s - 2073.19s |  all AI policies in the world. There's 1,937 AI policies in the world um at least time of doing
2073.19s - 2077.03s |  this research and based on what they have access to, based on certain requirements of things.
2077.99s - 2084.07s |  Um that's a lot of policy. So we wanted to understand which of these policies um
2085.03s - 2089.35s |  although those three different axes of information to what extent these policies
2089.35s - 2095.35s |  focus on those. So what did the policy consider as the referent object? Um how did it consider
2095.35s - 2105.59s |  the direction of security? Um and then um what was it did how did it define AI security? Like did
2105.59s - 2112.15s |  it define it as AI for cyber security? Did it define it as the security of AI? Um or maybe it
2112.15s - 2118.79s |  was something else. Um how did they they think about all of those things? And so we found that
2118.79s - 2125.19s |  out of all of those policies an overwhelming number of them only considered AI in the context
2125.19s - 2131.51s |  of using it for security and it wasn't even for cyber security right it was for national security.
2131.51s - 2135.11s |  Um I mean obviously these are public policies so that does sort of make sense.
2135.83s - 2141.75s |  Um but it was so overwhelming that for us this is really interesting because
2143.59s - 2149.83s |  the the the lack of consideration for AI safety in particular was really surprising to me because
2149.83s - 2156.63s |  there is quite a lot of narrative about AI safety at the moment. Um the lack of consideration for AI
2156.63s - 2161.75s |  security for me was less surprising um based on all the conversations I have which I also think
2161.75s - 2168.31s |  is a really big problem if um you know we need to be considering AI security far more. We also
2168.95s - 2173.59s |  looked at who the referent object was in the policy and especially given that these policies
2173.59s - 2179.03s |  mostly focused on AI for security of the state it makes sense that the referent object was the
2179.03s - 2184.31s |  nation state. Um and this was followed by sort of a mixture of the state and the individual
2184.31s - 2189.83s |  and then the individual and then the AI. Now what does like it does this even matter these
2189.83s - 2195.27s |  are public policies you know is this really what they need to be addressing? This number represents
2196.31s - 2202.23s |  the proportion of cyber policy that focuses on cyber security. 60 percent of cyber policy focuses
2202.23s - 2211.59s |  on cyber security. Um why can't we learn these lessons and apply it to AI? Um Neuromancer is a
2211.59s - 2216.31s |  really good like science fiction book. This is one of my favorite sci-fi books I think actually
2216.31s - 2223.91s |  and it's often credited as the first um time that the term cyberspace was used. This is written in
2223.91s - 2228.95s |  the I think seventies I believe by William William Gibson and I highly recommend it. Um and the
2228.95s - 2233.91s |  premise of the book is basically that you can sort of jack into cyberspace physically and then
2233.91s - 2240.39s |  interact with that environment. Um it's it's very cool but the connection here is that most of the
2240.39s - 2246.31s |  time AI security is considered a subset of cyber security. Uh rightly or wrongly I I just want to
2246.31s - 2251.83s |  ask everyone the question to consider. You know if if AI represents new risks and a new attack
2251.83s - 2257.27s |  surface should it be considered a subset of cyber security or shouldn't it? So outside of policy
2257.27s - 2263.67s |  we're starting to see uh more practitioner-based frameworks that address AI risk. So this example
2263.67s - 2268.63s |  by NIST is pretty good. There's lots of other sorts of taxonomies and references out there.
2268.63s - 2274.23s |  MITRE Atlas is a repository of all the TTPs of adversarial machine learning attacks for example.
2275.03s - 2279.03s |  But we're still sort of going through this maturation process especially when everything
2279.03s - 2285.51s |  is extremely AI safety focused. Um this is the model that I typically use to talk about AI
2285.51s - 2292.23s |  security threats. I talk about the three Ds. Those are disrupt, disclose and deceive. So if we're
2292.23s - 2297.75s |  thinking about that panda versus gibbon example um if you're able to disrupt the model it means
2297.75s - 2303.43s |  that that model basically just doesn't recognize that image as a panda. To deceive it means that
2303.43s - 2308.31s |  it's able to recognize that image as something specific like something else like we want it to
2308.31s - 2313.59s |  be specifically targeted as a gibbon and disclose means that you're able to leak information about
2313.59s - 2319.43s |  the training data or something like that. A lot of the conversations I have with other AI people
2319.43s - 2324.15s |  are very much that you know you can solve this problem through technology. Like if you create
2324.15s - 2329.99s |  solutions that basically pen test AI systems um you don't need to think about people or process
2329.99s - 2335.67s |  because the AI will be secure. And I can see from some of you in the room that you also don't
2335.67s - 2340.63s |  agree with that premise um because I definitely don't agree with that premise and most of the
2340.63s - 2347.51s |  time it is actually a people and process issue. Um you know you can have as much secure technology
2347.51s - 2351.67s |  as you want but if you're not able to actually create an environment where it supports that
2351.67s - 2360.23s |  security um that's that's really challenging. So I'm sort of at time but I want to end on again
2360.23s - 2366.63s |  because it's the policy village some discussion points um or maybe questions. I I we might have
2366.63s - 2371.99s |  time for Q&A I'm not really sure but I'm really interested to understand from all of you in the
2371.99s - 2378.71s |  room who obviously care about sort of the intersection of AI and security um to what
2378.71s - 2384.23s |  extent you think that you know policy can actually address these kinds of challenges
2384.23s - 2392.39s |  and how you know why is it that we're not seeing more um more action around AI security when
2392.39s - 2399.35s |  cyber security is clearly such a big problem um for all levels of society. You know the national
2399.35s - 2405.35s |  security implications of AI um are extreme and we're just not seeing these lessons learnt being
2405.35s - 2410.95s |  applied from cyber security and national security into AI security and so I hope that's something
2410.95s - 2415.35s |  that maybe you take away with you and some questions that you can ask and I also really
2415.35s - 2421.43s |  want to hear from you as well because I I want to see AI security more on the agenda. So um please
2421.43s - 2432.23s |  do keep in touch. Um yeah I I do we have time for questions? No okay well stay in touch.