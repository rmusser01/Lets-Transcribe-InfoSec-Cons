{
  "webpage_url": "local:1731819283:a0e5021e:DEF CON 32 - Crypto Privacy Village - Attacks on GenAI data and using vector encryption to stop them - Patrick Walsh  Bob Wall.mp4",
  "title": "DEF CON 32 - Crypto Privacy Village - Attacks on GenAI data and using vector encryption to stop them - Patrick Walsh  Bob Wall.mp4",
  "description": "Local file",
  "channel_url": null,
  "duration": null,
  "channel": null,
  "uploader": null,
  "upload_date": null
}

0.00s - 0.50s | This text was transcribed using whisper model: large-v2

 Welcome.
0.50s - 3.50s |  This is the Attacks on GenAI Data
3.50s - 7.38s |  and Using Vector Encryption to Stop Dumb Talk.
7.38s - 8.18s |  I'm Patrick Walsh.
8.18s - 9.22s |  This is Bob Wall.
9.22s - 11.52s |  We're the co-founders of IronCore Labs,
11.52s - 13.72s |  where we help people protect data in their cloud
13.72s - 17.74s |  applications by making application layer encryption
17.74s - 21.62s |  more easy and practical and usable.
21.62s - 23.86s |  So this talk is going to be about the ecosystems
23.86s - 26.94s |  around LLMs, but we'll start by talking about LLMs
26.94s - 29.06s |  specifically.
29.06s - 32.38s |  And in particular, some limitations of LLMs.
32.38s - 34.02s |  When an LLM is trained, it's trained
34.02s - 38.70s |  to predict a plausible completion to a prompt.
38.70s - 40.78s |  And that means it can sometimes make stuff up
40.78s - 43.14s |  that sounds good but isn't necessarily right.
43.14s - 45.70s |  It's also trained on what, by the time it's released,
45.70s - 48.46s |  is typically stale data, because it takes a long time
48.46s - 51.46s |  to train it, to test it, to fine tune it, et cetera.
51.46s - 53.74s |  And then finally, it hopefully hasn't
53.74s - 55.46s |  been trained on your data.
55.46s - 57.14s |  And therefore, if you want it to do
57.14s - 59.98s |  things that are cool across your corporate data,
59.98s - 62.02s |  your personal data, or something like that,
62.02s - 63.26s |  it's not going to work.
63.26s - 65.34s |  And to fill those gaps, there's a thing
65.34s - 68.66s |  called RAG, or Retrieval Augmented Generation.
68.66s - 71.32s |  Terrible name basically means stuff extra stuff
71.32s - 74.42s |  into the prompt so the LLM can answer questions better.
74.42s - 75.98s |  And it works a lot like this.
75.98s - 77.06s |  You have a chat app.
77.06s - 78.42s |  It doesn't have to be a chat app.
78.42s - 80.90s |  It's just the most common thing to use, right?
80.90s - 82.98s |  A user asks a question, and now instead
82.98s - 86.22s |  of passing that question to the LLM, the first thing it does
86.26s - 87.30s |  is a search.
87.30s - 90.90s |  And that search is to find relevant documents related
90.90s - 92.22s |  to the query.
92.22s - 94.62s |  That all gets jammed together into a prompt, which
94.62s - 95.84s |  then goes to the LLM.
95.84s - 96.34s |  That's it.
96.34s - 97.34s |  That's RAG in a nutshell.
97.34s - 98.18s |  You get it.
98.18s - 100.74s |  Here's a kind of more practical example
100.74s - 102.14s |  of what that looks like.
102.14s - 103.62s |  There's a system prompt.
103.62s - 106.54s |  There's a history of the previous questions and answers
106.54s - 107.58s |  from the chat.
107.58s - 110.52s |  And then there's the most recent question.
110.52s - 115.34s |  And then stuffed in with that is any documents, or web pages,
115.34s - 117.58s |  or whatever it is, that might be related
117.58s - 118.62s |  to what the question was.
118.62s - 121.70s |  In this case, it's about, hey, what's that draft 10Q?
121.70s - 124.22s |  It's a private document that's not public.
124.22s - 126.74s |  And that's how it all gets in to the LLM
126.74s - 129.42s |  so it can answer questions on that private data.
129.42s - 131.90s |  It's fundamental to basically how everyone
131.90s - 136.70s |  is using this stuff today if you're using chat inside an app.
136.70s - 138.48s |  And to make all this work, you need
138.48s - 141.26s |  to have some type of more intelligent search
141.26s - 143.22s |  than what we typically do.
143.22s - 145.18s |  Call it AI search, call it semantic search,
145.18s - 146.54s |  or whatever you want to.
146.54s - 148.86s |  And there's a bunch of ways to do this.
148.86s - 151.90s |  The most popular right now is an approach called vector search.
151.90s - 154.02s |  It's kind of what everyone is doing,
154.02s - 156.18s |  although it's not the only way to do it.
156.18s - 159.46s |  And vector search and actually all of those AI search type
159.46s - 163.86s |  techniques are important because they differentiate
163.86s - 165.72s |  between different meanings of words.
165.72s - 168.00s |  They can understand a sentence in context.
168.00s - 169.90s |  They can understand when arm means
169.90s - 171.86s |  a division of an organization versus when
171.86s - 176.14s |  arm is a body part or even a verb.
176.14s - 179.86s |  And in practice, how these types of vector search
179.86s - 182.98s |  works is you start with some input document.
182.98s - 184.94s |  And that input, did we lose it?
184.94s - 187.86s |  And that input document then gets chunked up into pieces.
187.86s - 189.06s |  Sometimes it's by sentence.
189.06s - 191.30s |  Sometimes it's sets of, call it, 30 words.
191.30s - 194.62s |  Or it can be any strategy and often overlapping.
194.62s - 197.26s |  But each of those pieces gets run
197.26s - 199.98s |  through something that's very similar to an LLM.
199.98s - 201.58s |  That sucks.
201.58s - 203.90s |  That's very similar to an LLM, but is, in fact,
203.90s - 205.78s |  called a text embedding model.
205.78s - 208.50s |  And a text embedding model, unlike an LLM,
208.50s - 211.98s |  produces something called a vector embedding.
211.98s - 212.58s |  That's not great.
216.26s - 218.62s |  Produces something called a vector embedding.
218.62s - 221.94s |  And the vector embedding is basically just a long list
221.94s - 223.70s |  of very small numbers.
223.70s - 226.30s |  And those small numbers can be hundreds or thousands
226.30s - 227.46s |  in length.
227.46s - 230.02s |  And we'll get into what they mean in a minute here.
230.06s - 232.06s |  But to give you some intuition on these,
232.06s - 234.94s |  we'll look at a list, like a vector that's only three long.
234.94s - 236.70s |  Because you can graph this in 3D space
236.70s - 238.14s |  and you can understand it better.
238.14s - 240.06s |  And if we have a bunch of different vectors,
240.06s - 242.58s |  let's say we have A and B and we have C,
242.58s - 244.50s |  we can kind of look at this visually and say,
244.50s - 247.50s |  oh, look, A and B kind of look like they're close together.
247.50s - 249.90s |  And A and C look like they're far apart.
249.90s - 253.46s |  And in fact, if we run a distance comparison operation
253.46s - 254.98s |  like this one, Euclidean distance,
254.98s - 256.74s |  although there's a bunch of different ones,
256.74s - 258.54s |  they all have the same kind of effect,
258.54s - 261.14s |  you can see that A and B are 2.2 apart,
261.14s - 264.34s |  whereas A and C are 5.6 apart.
264.34s - 265.98s |  And the way this works with search
265.98s - 268.86s |  is we take a query, right, it's a sentence,
268.86s - 271.22s |  and we reduce it to, let's say, A.
271.22s - 273.86s |  A is a vector representation of what we queried.
273.86s - 276.06s |  And then we search across all our vectors
276.06s - 278.82s |  and we say, give me anything that's similar to A.
278.82s - 280.38s |  And then hopefully, if it does it right,
280.38s - 282.54s |  it returns B but not C.
282.54s - 284.70s |  That's it, that's what vector search is doing.
284.70s - 286.14s |  It's the whole shebang.
286.14s - 289.22s |  The vector represents the meaning of the content.
289.22s - 292.70s |  And to make that work, we have this kind of new concept,
292.70s - 295.30s |  or at least new to a lot of us, at least to me,
295.30s - 297.06s |  although I guess it's been around for quite a while,
297.06s - 299.26s |  and that's called a vector database.
299.26s - 301.02s |  And a vector database's purpose
301.02s - 302.90s |  is to be able to do these nearest neighbor
302.90s - 305.14s |  or approximate nearest neighbor searches
305.14s - 307.50s |  across the large amounts of data very quickly.
308.78s - 310.18s |  And there are a lot of these companies now
310.18s - 311.02s |  that are doing this.
311.02s - 314.66s |  In fact, last year we saw hundreds of millions of VC money
314.66s - 317.14s |  go into the startups that are building vector databases,
317.14s - 319.82s |  startups like Pinecone and Weaviate and Quadrant.
319.82s - 323.66s |  And despite that, the idea of a vector database
323.66s - 325.38s |  is already eroding away to nothing
325.38s - 328.98s |  because classic databases like Mongo and Postgres and Oracle
328.98s - 330.86s |  are all adding vector capabilities
330.86s - 332.10s |  to their existing databases.
332.10s - 334.46s |  So you're getting these type of vector comparison
334.46s - 335.70s |  nearest neighbor search capability
335.70s - 338.34s |  pretty much everywhere that you have a database now.
339.22s - 341.74s |  Okay, changing tracks for a second.
341.74s - 344.82s |  We started thinking to ourself, who's using this stuff?
344.82s - 349.54s |  Well, we looked at the top 10 SAS companies
349.54s - 351.18s |  and we thought, how many of them are doing
351.18s - 353.98s |  adding LLM capabilities over private data?
353.98s - 356.26s |  Which means that they're almost certainly using RAG.
356.26s - 358.10s |  The answer is 100%.
358.10s - 359.78s |  Then we said, well, what about security companies?
359.78s - 362.82s |  They wouldn't jump on to just using all this new technology
362.82s - 364.58s |  that we don't know how to secure yet, would they?
364.58s - 366.66s |  No, 100% of them too.
366.66s - 369.46s |  Also adding LLM capabilities on top of private data.
370.46s - 372.86s |  Think about that for a moment, right?
372.86s - 374.26s |  It means that these AI systems,
374.26s - 376.70s |  whatever cloud services and things you're using,
376.70s - 378.82s |  they are hoovering up this data.
378.82s - 381.90s |  Probably copies of it are going into the vector databases
381.90s - 384.34s |  as vectors and among other places,
384.34s - 386.78s |  which I'll talk about ever so briefly later.
387.78s - 390.78s |  So do these vectors matter?
390.78s - 392.78s |  I was talking to the CEO
392.78s - 394.18s |  of one of the vector database companies
394.18s - 396.10s |  who's raised tens of million dollars last year.
396.10s - 398.46s |  And he told me that a vector was like a hash
398.50s - 400.30s |  and it was basically meaningless
400.30s - 402.54s |  except for comparison purposes.
402.54s - 403.54s |  So which my first thought is,
403.54s - 406.42s |  well, you clearly don't know how people abuse hashes.
406.42s - 410.38s |  And my second thought is you're wrong.
410.38s - 412.54s |  So that isn't how it works at all.
412.54s - 415.06s |  If you look at taking a phrase
415.06s - 417.30s |  and you reduce it to an embedding vector,
417.30s - 420.02s |  an embedding model into a vector, okay,
420.02s - 422.58s |  it looks pretty meaningless to us.
422.58s - 426.18s |  But then again, so does an animated GIF like this cat
426.18s - 428.26s |  if you look at it in a text editor, right?
429.06s - 430.50s |  Just because it's meaningless to us to look at
430.50s - 432.66s |  doesn't mean it's meaningless to a computer
432.66s - 435.30s |  or in this case, to an AI model.
435.30s - 438.22s |  And so the way you go back from a vector
438.22s - 442.14s |  to the original text or something approximating it
442.14s - 443.94s |  is using something called an inversion model.
443.94s - 445.26s |  An inversion model is just a model
445.26s - 447.70s |  that's trained on vectors to sentences
447.70s - 450.38s |  so that it can make that same prediction.
450.38s - 452.22s |  In this case, it comes close.
452.22s - 454.34s |  It's our income is down instead of our earnings is down,
454.34s - 455.90s |  right, in this example.
455.90s - 460.62s |  100% match on meaning, not 100% match on words.
460.62s - 462.62s |  There are some tens of thousands of papers
462.62s - 463.86s |  on this at this point.
463.86s - 466.58s |  It's heavily studied in academia,
466.58s - 469.18s |  not widely known, I don't think, broadly.
470.22s - 472.58s |  Best paper we've seen for text inversions,
472.58s - 474.46s |  and it's not just text that vectors represent,
474.46s - 476.18s |  I'll talk about that in a second,
476.18s - 479.06s |  recovers 92% of exact words.
479.06s - 483.30s |  That means full names, everything coming right back out
483.30s - 485.98s |  exactly as it went in on the text.
485.98s - 488.82s |  The other 8% is basically effectively the same anyway.
490.14s - 491.86s |  So are these attacks hard?
492.90s - 495.54s |  What's the skill level for doing this kind of attack?
496.46s - 498.54s |  The answer is most of these papers
498.54s - 500.94s |  have accompanying open source software on GitHub,
500.94s - 502.70s |  and so really you just need someone
502.70s - 503.86s |  who can run the software,
503.86s - 506.26s |  not someone who knows how to build the software,
506.26s - 508.66s |  which means it's script kitty level.
508.66s - 509.50s |  And on that note,
509.50s - 511.62s |  we actually ran some of these attacks ourselves.
511.62s - 513.54s |  We posted some blogs you can check out.
513.54s - 514.58s |  We did one on,
514.58s - 516.82s |  so I mentioned vectors can represent different things.
516.82s - 518.86s |  They can represent images for image search.
518.86s - 521.46s |  They can represent faces or voice prints
521.46s - 523.06s |  or other things for that.
523.06s - 524.86s |  We did one against facial recognition.
524.86s - 526.26s |  That doesn't use an inversion model.
526.26s - 527.66s |  That's a different technique, by the way,
527.66s - 529.10s |  so it's kind of interesting for that purpose,
529.10s - 531.34s |  and we did one on text inversions.
532.26s - 534.06s |  Really good results.
534.06s - 536.30s |  You can really get all the stuff back out
536.30s - 541.88s |  or very near approximations of the inputs.
541.88s - 542.76s |  All right, so let's talk about
542.76s - 545.28s |  these new vector database companies.
545.28s - 548.00s |  They probably have good security, right?
548.00s - 550.24s |  Actually, no, super immature.
550.24s - 551.52s |  They'll fix it over time.
551.52s - 552.36s |  It'll be fine,
552.36s - 554.32s |  but if you look closely at these as we have,
555.76s - 560.04s |  two of them have no authentication required by default,
560.04s - 562.48s |  making the mistakes of Elasticsearch and MongoDB
562.48s - 564.04s |  of years past but not learning.
565.36s - 567.60s |  One of them has services
567.60s - 569.76s |  that are meant to be internal services
569.76s - 572.88s |  and that there's no protection or authentication on them
572.88s - 575.36s |  so that if someone goes to access that
575.36s - 578.12s |  because the firewall neglected to block a port,
578.12s - 581.32s |  direct access back to the data without any authentication.
582.16s - 583.88s |  Two of them claim to have end-to-end encryption
583.88s - 587.04s |  when they only have TLS in transport,
587.04s - 587.88s |  and more.
587.88s - 588.72s |  I could go on.
588.72s - 589.88s |  It doesn't matter.
589.88s - 591.96s |  The point is that these vector database systems,
591.96s - 594.52s |  and for that matter, the chat apps, either way,
594.52s - 597.84s |  are like total treasure troves of information,
597.84s - 599.80s |  and even if you don't use it and invert it,
599.80s - 602.84s |  they're at least treasure maps of information
602.84s - 605.44s |  because they point back to the data and you can query it,
605.44s - 607.80s |  and if you think about that for a second,
607.80s - 610.12s |  imagine that you're in a CTF competition
610.12s - 611.60s |  in that corner of this thing,
611.60s - 613.56s |  and you get into a machine, cool,
613.56s - 614.80s |  but now you have to get the flag,
614.80s - 615.64s |  and it's in a database.
615.64s - 616.68s |  You have to get into the database,
616.68s - 618.64s |  and now you have to reverse the schema
618.64s - 622.24s |  and figure out a query and find the flag.
622.24s - 623.52s |  It ain't as easy as it sounds, right?
623.52s - 626.44s |  It's a lot of expertise and time to do that
626.44s - 629.60s |  versus, you know, give me that data.
629.60s - 634.60s |  It is like incredibly far, far easier
634.60s - 635.68s |  to attack these systems
635.68s - 637.84s |  because you can do it in natural language.
639.08s - 642.60s |  So what do we think about bringing AI into our infrastructure?
642.60s - 645.40s |  Maybe we should pause for a moment
645.40s - 646.44s |  and think a little bit about it,
646.44s - 647.28s |  or if it's in there,
647.28s - 648.56s |  maybe if you're a hacker in the audience,
648.56s - 650.60s |  you should think about, holy shit, there's a lot here.
650.60s - 652.00s |  Let me give you some ideas.
653.44s - 655.72s |  Obviously, everyone knows about prompt injections.
655.72s - 657.92s |  Maybe you haven't thought so much about the fact
657.92s - 659.52s |  that you can put a prompt injection
659.52s - 661.24s |  in the documents being searched,
661.24s - 662.52s |  and they get pulled into the prompt,
662.52s - 664.44s |  and you can poison the prompt that way.
665.40s - 666.88s |  Logs are everywhere.
666.88s - 668.20s |  If you're putting private data
668.20s - 669.56s |  into the things that go to the LLMs,
669.56s - 671.16s |  and the LLMs log it, that's a problem,
671.16s - 674.20s |  but also the prompt firewalls,
674.20s - 676.08s |  if you're using those, are logging that stuff too,
676.08s - 678.04s |  so the data's kind of exploding.
678.04s - 680.20s |  And don't even get me started on agents.
680.20s - 681.48s |  Talk to me after if you want to hear
681.48s - 682.44s |  about my thoughts on that,
682.44s - 685.48s |  but if you're generating code to run to query a database,
686.36s - 687.92s |  you're beyond hope.
689.32s - 690.88s |  Today, though, we're not gonna talk about any of that.
690.88s - 693.44s |  We're gonna talk specifically about this vector stuff,
693.44s - 694.80s |  and how to protect it.
694.80s - 696.40s |  So basically today, the way to protect that
696.40s - 699.04s |  is to encrypt it, and there's two approaches to this,
699.04s - 701.00s |  at least in academia, that you could do.
701.00s - 702.48s |  One is fully homomorphic encryption,
702.48s - 704.32s |  and the other is partially homomorphic.
704.32s - 706.00s |  So fully homomorphic encryption,
707.56s - 709.04s |  there's actually quite a few papers on this,
709.04s - 710.04s |  and they're fairly interesting.
710.04s - 711.40s |  There's a lot of different approaches.
711.40s - 714.68s |  Some of them require sending all the vectors to the server,
714.80s - 716.52s |  some require some other stuff.
716.52s - 718.72s |  They typically suffer from being slow,
718.72s - 721.40s |  and you would have to build your own vector database,
721.40s - 722.72s |  and be another logo on that wall,
722.72s - 724.80s |  and compete with all the other vector databases
724.80s - 726.56s |  on every other thing.
726.56s - 729.44s |  The other option is partially homomorphic encryption,
729.44s - 731.24s |  and the idea here is that
731.24s - 732.84s |  something that's partially homomorphic
732.84s - 734.88s |  is able to do some operations,
734.88s - 736.68s |  but not arbitrary operations.
736.68s - 739.60s |  And in this case, we're talking about optimizing
739.60s - 743.96s |  for that nearest neighbor search type of operation.
744.08s - 747.16s |  So in this example, partially homomorphic,
747.16s - 750.12s |  specific to nearest neighbor searches.
750.12s - 752.28s |  And to talk more about this paper,
752.28s - 754.52s |  and the work that we've done around it,
754.52s - 760.20s |  I'm gonna turn it over to Bob.
760.20s - 763.96s |  All right, we found this paper
763.96s - 766.88s |  about a way to encrypt vectors,
766.88s - 768.92s |  and you can see the title,
768.92s - 770.76s |  Approximate Distance Comparison
770.76s - 772.40s |  Preserving Symmetric Encryption.
772.40s - 773.56s |  So that's a mouthful,
774.32s - 779.32s |  but this algorithm is a type of
779.68s - 781.52s |  property-preserving encryption,
781.52s - 783.84s |  which means it's kind of like order-preserving encryption,
783.84s - 786.32s |  or maybe deterministic encryption,
786.32s - 787.84s |  if you've heard of those.
787.84s - 791.04s |  The property that this algorithm is preserving, though,
791.04s - 795.28s |  is distance comparison instead of equality or order.
795.28s - 798.00s |  And so if you can preserve the ability
798.00s - 800.60s |  to compare the distance between two vectors
800.60s - 802.60s |  after they've been encrypted,
802.64s - 804.00s |  that means your vector database
804.00s - 806.60s |  continues to work on the encrypted vectors
806.60s - 808.80s |  just as well as it did on the original vectors.
808.80s - 813.80s |  So like deterministic and order-preserving encryption,
814.68s - 816.20s |  there's a trade-off.
816.20s - 818.28s |  The more secure you make it,
818.28s - 822.24s |  the less well the property is preserved,
822.24s - 825.56s |  and so the less well queries might work on the data.
825.56s - 828.08s |  So you have a decision to make
828.08s - 830.12s |  about whether you want a lot of security,
830.12s - 832.60s |  and you can tolerate maybe a little less accuracy
832.60s - 835.68s |  on queries, or you want a lot of accuracy on the queries,
835.68s - 837.20s |  and maybe you'll have a little bit less
837.20s - 839.00s |  protection of the data.
839.00s - 842.28s |  But the system works by,
842.28s - 845.16s |  if you've got a system that's ingesting data
845.16s - 846.48s |  and it's making these vectors,
846.48s - 848.32s |  putting them into a vector database,
848.32s - 850.60s |  it's easy to add this encryption.
850.60s - 853.44s |  You just, in the system that's already there,
853.44s - 855.44s |  after you've taken the data
855.44s - 856.80s |  and run it through the embedding model
856.80s - 858.56s |  and generated the vector embedding,
858.56s - 859.84s |  you run it through this algorithm
860.52s - 861.68s |  and encrypt it, it's still a vector.
861.68s - 865.04s |  It looks similar enough to the original vector
865.04s - 867.12s |  that the vector database is gonna take it
867.12s - 868.16s |  and be happy with it.
869.32s - 873.04s |  So you use a secret key, you encrypt the vector,
873.04s - 874.28s |  you throw it in the vector database.
874.28s - 878.72s |  So very simple change to the system to be able to do that.
878.72s - 881.24s |  Now, most of these vector database systems
881.24s - 884.92s |  support storing metadata along with the vectors,
884.92s - 888.92s |  and that metadata might have some sensitive data in it too,
888.92s - 891.44s |  so you might wanna consider encrypting that.
891.44s - 892.48s |  That's a whole different thing.
892.48s - 895.64s |  We're just gonna focus on the vectors for right now.
895.64s - 898.04s |  So once you've got all of these vectors encrypted,
898.04s - 901.24s |  you've loaded them into your vector database,
901.24s - 904.68s |  when it's time to do a query, it's also pretty simple.
904.68s - 907.52s |  Again, the normal process would be to take the query,
907.52s - 910.16s |  run it through the embedding model, get a vector.
910.16s - 913.68s |  So then you just encrypt that vector with the same key
913.68s - 916.28s |  and the same algorithm, and then you do your normal search.
916.28s - 918.64s |  And because the algorithm, the encryption
919.32s - 921.00s |  preserves distance comparison,
921.00s - 923.48s |  that means that when you give that encrypted query
923.48s - 926.28s |  to the vector database, it's gonna return
926.28s - 928.08s |  the encrypted vectors that are closest
928.08s - 931.60s |  to the encrypted query, and those are gonna correspond
931.60s - 934.76s |  to the decrypted vectors, the original vectors
934.76s - 936.80s |  that are closest to the original query.
936.80s - 940.68s |  So you get the same results out pretty much.
941.80s - 944.92s |  And once you get them out, you can maybe decrypt the vectors
944.92s - 947.44s |  if you need to, or maybe you've got some metadata
947.44s - 949.88s |  that lets you look up the associated data
949.88s - 951.52s |  in a database or something else.
952.60s - 957.28s |  So without the key, an attacker can't really understand
957.28s - 960.76s |  the queries, they are enough different
960.76s - 962.36s |  that they can't look at that and kind of know
962.36s - 963.92s |  what the query was.
963.92s - 966.24s |  They can't invert the data that's in the database,
966.24s - 969.24s |  even if they hacked it, exfiltrated all the data,
969.24s - 971.08s |  they can't easily invert it.
972.36s - 974.76s |  The results are still fairly accurate.
974.76s - 977.28s |  Again, that's a trade-off between the security
978.12s - 980.00s |  and the accuracy of those searches.
980.00s - 984.28s |  The whole thing is it introduces a negligible penalty
984.28s - 986.68s |  in terms of performance.
986.68s - 990.48s |  The encryption process is fast, so you can add this in
990.48s - 993.28s |  and not add a lot of time to your process.
994.56s - 995.72s |  So I'm gonna talk a little bit more
995.72s - 998.16s |  about this encryption function.
998.16s - 1000.92s |  So it's a function, it maps inputs to outputs,
1000.92s - 1005.56s |  and it does it such that for any distance function,
1005.56s - 1007.24s |  the margin of error that you get
1007.24s - 1009.72s |  when you compare two encrypted vectors
1009.72s - 1012.16s |  will be within some factor, we'll call that beta,
1012.16s - 1015.72s |  the approximation factor, of the original distance.
1015.72s - 1019.40s |  And so if you can build a function that preserves that
1019.40s - 1022.24s |  for any pair of vectors that you give as an input,
1022.24s - 1024.08s |  then you have a distance comparison
1024.08s - 1027.00s |  preserving encryption function.
1027.00s - 1031.20s |  So in real systems, we kind of talked about this,
1031.20s - 1034.36s |  the comparison function is used for nearest neighbor
1034.36s - 1036.04s |  and approximate nearest neighbor searches,
1036.04s - 1038.12s |  which is what all these vector databases are doing.
1038.12s - 1042.44s |  So if we can build, and we can,
1042.44s - 1045.64s |  we've built this function that preserves distance comparison
1045.64s - 1048.92s |  so we can use it to encrypt these vectors.
1048.92s - 1052.60s |  So the paper introduced this scale and perturb algorithm,
1052.60s - 1055.92s |  and I won't go into a lot of details about how it works,
1055.92s - 1058.50s |  but I will give you a few key points about it.
1059.72s - 1062.36s |  So there's a scale factor.
1062.84s - 1064.92s |  When you set up the algorithm,
1064.92s - 1067.04s |  you randomly choose this scale factor
1067.04s - 1069.08s |  and you choose a random key.
1069.08s - 1076.02s |  And so there's also, when you go to encrypt
1076.02s - 1078.98s |  an individual vector,
1078.98s - 1080.90s |  you generate an initialization vector,
1080.90s - 1082.78s |  an IV value, just the same as you might
1082.78s - 1085.78s |  if you're doing symmetric encryption on data
1085.78s - 1088.82s |  with a normal symmetric algorithm.
1088.82s - 1093.66s |  And you perturb each element of the vector
1093.66s - 1097.78s |  using that key in that initialization vector randomly.
1097.78s - 1099.98s |  So every element in the vector
1099.98s - 1101.70s |  is gonna get wiggled around a little bit,
1101.70s - 1104.62s |  and that amount that it gets wiggled around
1104.62s - 1107.34s |  depends on that value beta, that approximation value,
1107.34s - 1111.88s |  and that's how it limits the change in the distance
1111.88s - 1114.75s |  that you can get.
1114.75s - 1119.75s |  So the paper had this idea of perturbing the vectors as well
1120.03s - 1122.39s |  and they basically had this idea of shuffling.
1122.39s - 1123.83s |  If you had all of the vectors,
1123.83s - 1125.99s |  you would shuffle the order of them
1125.99s - 1127.67s |  before you encrypted them.
1127.67s - 1130.19s |  And we wanted an online algorithm
1130.19s - 1133.35s |  so that you could, you just encrypted the vectors
1133.35s - 1136.39s |  as you saw them, so we didn't find that shuffling
1136.39s - 1140.15s |  to be beneficial, but we did add a shuffle
1140.15s - 1141.95s |  where we took the elements of the vector
1141.95s - 1143.35s |  and shuffled them up.
1143.35s - 1145.31s |  So we randomly changed the order
1145.31s - 1148.35s |  and it's a deterministic shuffling based on the key.
1148.35s - 1151.23s |  So when you get done, all of the elements in the vector
1151.23s - 1154.39s |  have been scaled and then they get shifted around,
1154.39s - 1156.99s |  which makes it even harder, even if you knew
1156.99s - 1160.15s |  what the algorithm, what the embedding model was,
1160.15s - 1162.39s |  all of the elements of the vector are in different places,
1162.39s - 1164.79s |  which makes it a lot harder to look at it
1164.79s - 1167.97s |  and try to even guess how it corresponds
1167.97s - 1171.77s |  to the original vectors.
1171.77s - 1175.37s |  So to give a little bit of a more intuition about this,
1175.37s - 1176.69s |  you can think about these vectors,
1176.69s - 1180.53s |  I mean a vector is a line from the origin to some point
1180.53s - 1184.73s |  and the point is defined by the elements in the vector.
1184.73s - 1186.57s |  Those are the coordinates of that point.
1186.57s - 1191.57s |  And so the beta basically defines a sphere
1192.09s - 1195.61s |  in N dimensional space around that point.
1195.61s - 1198.61s |  And so as we perturb the elements of the vector,
1198.61s - 1202.21s |  the resulting point is somewhere within that sphere
1202.21s - 1204.45s |  and the size of that sphere is governed
1204.45s - 1206.77s |  by that beta value, the approximation factor.
1206.77s - 1211.73s |  So you can see that if you bound the amount
1211.73s - 1213.73s |  that you can move the point around
1213.73s - 1214.81s |  and you've bound the amount
1214.81s - 1216.53s |  that you've moved another point around,
1216.53s - 1218.81s |  the distance between those is bounded too.
1218.81s - 1222.53s |  So that's how it manages to preserve the distance comparison.
1223.67s - 1225.33s |  So in addition, we talked about,
1225.33s - 1228.97s |  we also scaled the vectors up so that they're,
1228.97s - 1231.09s |  they get moved around a little bit, they get scaled.
1231.09s - 1232.65s |  So if you just look at them
1232.65s - 1234.01s |  and then the elements get shuffled,
1234.01s - 1239.01s |  so it's very difficult to correlate an encrypted vector
1239.09s - 1241.41s |  in the original vector that was encrypted.
1242.85s - 1246.77s |  Again, because we've preserved that distance comparison,
1246.77s - 1250.23s |  they're still relatively close together with each other.
1251.57s - 1252.41s |  Here's an example.
1252.41s - 1255.97s |  We kind of turned off the shuffle part of it
1255.97s - 1258.29s |  so it wouldn't be quite as difficult to see,
1258.29s - 1260.63s |  but you can see, you know, we scaled it up
1260.63s - 1262.03s |  and then we perturb everything.
1262.03s - 1265.43s |  So sometimes, depending on how much we perturb stuff,
1265.43s - 1267.51s |  signs on elements might change,
1267.51s - 1270.37s |  you know, they bounce around a little bit,
1270.37s - 1272.71s |  but they're still in this vector.
1272.71s - 1276.73s |  So we've implemented this algorithm.
1276.73s - 1278.59s |  We've got an open source library
1278.59s - 1281.67s |  we call IronCoreAlloy that's out there on GitHub.
1281.67s - 1283.97s |  It's under the AGPL license.
1283.97s - 1287.07s |  So if you can use GPL software,
1287.07s - 1288.83s |  you can go grab it and use it.
1288.83s - 1290.69s |  If you can't use GPL software,
1290.69s - 1293.65s |  you can talk to us about a different arrangement.
1295.77s - 1298.25s |  Along with the library, we've got some sample code
1298.25s - 1299.37s |  that shows how you can use it
1299.37s - 1301.69s |  with a number of different vector databases.
1301.69s - 1304.37s |  And as I said earlier, it's easy.
1304.37s - 1306.61s |  So the examples aren't super complicated.
1306.61s - 1308.61s |  It's pretty easy to drop this into some code
1308.61s - 1312.21s |  that's gonna put data in one of these vector databases.
1312.21s - 1313.73s |  The library is written in Rust,
1313.73s - 1316.97s |  but we've exposed it in Java, Kotlin, and Python.
1316.97s - 1319.47s |  We've got some more language support on the way.
1321.09s - 1323.29s |  Along with the examples,
1323.29s - 1326.53s |  we've got some results of using this
1326.53s - 1331.53s |  on some output of different LLMs, open source LLMs.
1333.97s - 1336.81s |  And you can see for an approximation factor
1336.81s - 1339.21s |  of one and a half, which is a pretty modest one,
1340.25s - 1343.15s |  we don't impact the search performance.
1343.15s - 1344.97s |  We measured precision and recall loss,
1344.97s - 1347.89s |  which are pretty common measures of search accuracy.
1347.89s - 1350.57s |  It didn't really impact the search accuracy much at all.
1351.45s - 1355.85s |  So, you know, it works.
1355.85s - 1358.33s |  So now once we've encrypted these vectors,
1358.33s - 1361.69s |  suppose we want to run an attack on them.
1361.69s - 1363.53s |  We're gonna run one of the inversion models
1363.53s - 1365.03s |  that Patrick was talking about.
1366.25s - 1369.25s |  So we know exactly, somehow we know exactly
1369.25s - 1371.81s |  what the embedding model was that was used.
1371.81s - 1374.05s |  It was an open source one, so we don't have to,
1374.05s - 1375.93s |  we train this up once, and it's gonna work
1375.93s - 1377.97s |  on every embedding that was generated
1377.97s - 1380.05s |  by that embedding model.
1380.05s - 1382.17s |  But we're gonna try to run it on the encrypted vectors.
1382.17s - 1386.61s |  Well, it turns out because we've done the perturbation
1386.61s - 1389.69s |  and we shifted around all of the elements.
1391.45s - 1395.55s |  Okay, it's mostly gibberish that we get out.
1395.55s - 1400.35s |  So you really can't tell much about them.
1402.01s - 1404.17s |  Even if somebody knows the plain text
1404.17s - 1407.41s |  and they have a black box access to the model,
1407.41s - 1411.85s |  and they're gonna submit vectors, get encrypted vectors out
1412.53s - 1413.37s |  for a particular key,
1413.37s - 1415.49s |  and they're gonna train their own inversion model.
1415.49s - 1419.09s |  The randomization makes that so that it's very difficult
1419.09s - 1422.01s |  and you get a lot less valuable results.
1422.01s - 1423.85s |  You might get a little bit of the meaning.
1423.85s - 1425.61s |  You're not gonna get exact words.
1427.69s - 1432.65s |  And if you, and again, it depends on what value you use
1432.65s - 1435.23s |  for that approximation model.
1436.45s - 1439.05s |  If you knew the plain text and you knew the cipher text,
1439.05s - 1441.21s |  can you extract the key?
1441.21s - 1444.97s |  No, it's based on a probabilistic random function
1444.97s - 1449.53s |  that's essentially like a 256 bit keyed hash.
1449.53s - 1450.49s |  And I don't know about you,
1450.49s - 1452.99s |  but I don't wanna spend my next 2 billion weekends
1452.99s - 1457.45s |  trying to brute force attack a 256 bit keyed hash.
1457.45s - 1460.21s |  So I'm gonna turn it over to Patrick
1460.21s - 1465.50s |  and he'll summarize for you.
1465.50s - 1470.50s |  Okay, so the takeaways that we hope you have here
1472.10s - 1474.58s |  as we just did a real brief fast run through
1475.42s - 1476.70s |  on what this is and how it works,
1476.70s - 1479.78s |  is first and foremost, you should walk away realizing
1479.78s - 1482.94s |  that RAG produces a lot of copies of data.
1482.94s - 1487.06s |  So if people are using AI and LLMs in their applications
1487.06s - 1489.94s |  and somehow that they're operating over data
1489.94s - 1492.54s |  that that model wasn't trained on,
1492.54s - 1493.98s |  probably they're doing this
1493.98s - 1496.90s |  and probably lots of copies of that data are being made.
1496.90s - 1498.30s |  It's sprinkled everywhere.
1498.30s - 1501.86s |  And vector databases are like a key part of that.
1502.70s - 1505.62s |  Second thing is that vectors and vector databases
1505.62s - 1508.46s |  can be inverted back to their original input
1508.46s - 1511.90s |  or something very closely approximating it, right?
1513.26s - 1515.58s |  High fidelity and low effort attack.
1515.58s - 1517.94s |  And oftentimes an attacker won't even need to do that.
1517.94s - 1519.18s |  If they have access to the chat app,
1519.18s - 1522.14s |  they can bypass that even, but nevermind.
1522.14s - 1523.78s |  If they have access to the vector database,
1523.78s - 1527.26s |  that's as good as having access to the chat app, right?
1527.26s - 1528.52s |  And then the third takeaway
1528.52s - 1531.16s |  that we want you to walk away with is this.
1531.16s - 1533.84s |  It isn't that hard in this case
1533.84s - 1536.20s |  because there's a built open source library
1536.20s - 1539.68s |  to implement a solution to this problem in your software.
1539.68s - 1543.76s |  So if you want to, feel free, check out the results,
1543.76s - 1544.92s |  go try it online.
1546.16s - 1548.92s |  The results you get in a vector search
1548.92s - 1550.72s |  are really close to the original.
1550.72s - 1553.82s |  Like it looks the same to me pretty much across the board.
1553.82s - 1556.04s |  The benchmarks show a few percent differences.
1556.04s - 1558.16s |  There's almost no performance penalty.
1558.16s - 1559.50s |  From a latency perspective,
1559.50s - 1562.78s |  there's almost no reason not to encrypt your vectors
1562.78s - 1564.74s |  if they contain sensitive data in them.
1565.66s - 1566.98s |  And that's it.
1566.98s - 1567.88s |  Thanks for your time today.
1567.88s - 1569.02s |  We're going to be out there,
1569.02s - 1571.02s |  probably just outside of the Creator Stage area
1571.02s - 1573.96s |  if people want to chat with us or have any questions.
1573.96s - 1575.06s |  Thanks for your time.