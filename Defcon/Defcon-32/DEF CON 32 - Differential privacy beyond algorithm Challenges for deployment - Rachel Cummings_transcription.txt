{
  "webpage_url": "https://www.youtube.com/watch?v=3kA6oLhPbbs",
  "title": "DEF CON 32 - Differential privacy beyond algorithm:  Challenges for deployment - Rachel Cummings",
  "description": "Differential privacy (DP) has been hailed as the gold standard of privacy-preserving data analysis, by providing strong privacy guarantees while still enabling use of potentially sensitive data. Formally, DP gives a mathematically rigorous worst-case bound on the maximum amount of information that can be learned about an individual's data from the output of a computation. In the past two decades, the privacy community has developed DP algorithms that satisfy this privacy guarantee and allow for accurate data analysis for a wide variety of computational problems and application domains. We have also begun to see a number of high-profile deployments of DP systems in practice, both at large technology companies and government entities. Despite the promise and success of DP thus far, there are a number of critical challenges left to be addressed before DP can be easily deployed in practice, including: mapping the mathematical privacy guarantees onto protection against real-world threats, developing explanations of its guarantees and tradeoffs for non-technical users, integration with other privacy & security tools, preventing misuse, and more.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 1550,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 6.72s | This text was transcribed using whisper model: large-v2

 All right, very fast intro. Welcome to, well, Creator Stage is one, but CPB is over that way
6.72s - 13.76s |  right afterwards. But go to our second talk first, over here, again, afterwards. But welcome. And
13.76s - 21.28s |  this is Professor, or Rachel. We're thrilled to have her. She's a professor, once again,
21.28s - 26.24s |  at Columbia University. And we are so excited for this talk. I got a chance to see these slides
26.24s - 30.88s |  last night. You're going to walk away thinking I know everything about differential privacy.
32.08s - 38.96s |  So, once again, here you go. Thank you. Thank you. We made it.
41.36s - 46.96s |  This is my first DEF CON. Super excited to be here, all of that. Thank you for your extreme
46.96s - 52.88s |  patience as we got things going. As a result, I'm going to have to skip some things in my talk so
52.88s - 58.08s |  that we can end on time. But I will be hanging out afterwards at the Crypto Privacy Village.
58.08s - 65.20s |  And so, come see me and say hi. I'm giving a surprise talk there at 1.30, which I think
65.20s - 73.68s |  the surprise will be the slides that I skipped. So, come and see that. Great. So, my work is
73.68s - 78.40s |  really about differential privacy. If you haven't heard of it, you might be an expert by the end of
78.40s - 90.03s |  this talk. It's okay. It would be over here. Oh, no. Okay. So, the like a mental model you should
90.03s - 95.07s |  think about as we talk about differential privacy should be something about there's a database,
95.07s - 100.27s |  it's important, and we want to analyze it and do like a scientific business analytics, whatever,
100.27s - 106.03s |  on it. And there's an analyst who's going to like send some queries, receive back a few answers.
106.03s - 111.55s |  This process is going to like continue. And so, she says, I've made some like a scientific
111.55s - 119.23s |  discovery. Maybe it's like, you know, this this drug works. Maybe it's here is a database that
119.23s - 124.75s |  can be freely published that I have made as a function of the original database, such that
124.75s - 129.87s |  if you were to evaluate all my same answers, you would get back approximately the same answer on
129.87s - 136.99s |  this new database. And you might want some kind of like a privacy preserving barrier at some point
136.99s - 142.59s |  in this process. And so, maybe it's between the database and the analyst if you don't fully like
142.59s - 147.87s |  trust this analyst. Maybe it's between between these sort of analysts and the outputs. If the
147.87s - 153.23s |  outputs are going to be like a publicly viewed, maybe you also want some some like boundaries
153.23s - 166.46s |  there. And the model that I think of in my work is a mental model about about privacy promising
166.46s - 173.42s |  people freedom from harm. Nothing bad is going to happen to you because you have shared your data
173.42s - 180.38s |  with this analysis process. Informally, we're going to say, or like informally,
181.82s - 189.10s |  some analysis of a data set is going to be private if an analyst can know almost no more about Alice
189.10s - 199.18s |  afterwards than he would before. Or sorry, if he had instead done done like the same analysis on
199.18s - 203.74s |  the same data set without Alice's data. And this allows us to learn things like, you know,
203.74s - 208.94s |  smoking causes cancer, but it doesn't allow us to learn things about your specific health
208.94s - 222.56s |  information. So the so the like a differential privacy definition visually looks like this.
223.20s - 228.88s |  Imagine if I have a database containing data coming from lots of people, and I plug that
228.88s - 235.84s |  into my favorite randomized algorithm, that will that will produce some some like curve,
237.20s - 243.44s |  which is a like a PDF of the outputs of this algorithm when instantiated on this particular
243.44s - 249.04s |  database. And so on the x axis, imagine like all possible things and sort of like a might
249.04s - 256.16s |  be a produced by by this algorithm on the y axis is going to be how likely is that output to occur
256.16s - 271.58s |  if I input D. I'm going to move this a little bit closer. See if that helps. Good. So now we're
271.58s - 278.30s |  going to imagine if I change one person's data, say they change Alice into being Javier, and I
278.30s - 285.58s |  plug my new database that is the same except for one person's data into my same randomized algorithm,
285.58s - 293.02s |  I'm going to get out a slightly different curve. And our hope in a privacy space is that like the
293.02s - 301.50s |  ratio between these two curves is going to be bounded point wise along this x axis. So now why
301.50s - 305.66s |  is this this privacy, I've talked about like, you know, freedom from harm. And now I'm talking about
305.66s - 311.34s |  like, you know, bounding ratios as to the PDFs of algorithms. Well, imagine there was a particular
311.34s - 318.70s |  output observed by some adversary, who then wanted to decide that I observed this output because of
318.70s - 327.26s |  Alice's data, or, or like because of Javier's data, or that adversary information theoretically
327.26s - 335.66s |  could not make a guess or the individual's data, and the input beyond the like a small amount
335.66s - 345.36s |  allowed, allowed by the ratio. So now in math, it says this. So an algorithm M, the maps the maps
345.36s - 352.00s |  from a like a data set of size n, that is like a data data coming from n people into some arbitrary
352.00s - 357.84s |  output range R is going to be parametrized epsilon differentiate private, if it is the case,
358.96s - 364.40s |  that for all neighboring databases, D and D prime, they're the same except for one person's data.
364.96s - 371.12s |  And for all possible things I may produce as a result of the analysis for all sets of S,
372.08s - 379.20s |  I'm going to produce something in that set S with about the same probability under D and,
379.84s - 385.84s |  and like a D prime. And so we're saying if I change one person's data, I'm going to output
385.84s - 392.64s |  the same thing with about the same probability. And so notice that our epsilon parameter occurs
392.64s - 399.28s |  in the exponent, which is probably not how you think about privacy, which makes it harder to
399.44s - 405.28s |  explain to people without a like a technical background. I'm like imagining somebody's
405.28s - 409.68s |  grandmother might like and have a hard time with this, unless you have a like an extremely,
409.68s - 417.36s |  extremely like a mathematical grandmother, maybe. And we're going to get back to that
417.36s - 425.20s |  point in the talk, that's going to be sort of a core point. Before we get there, why do we care
425.20s - 433.28s |  about this particular privacy notion? One thing is that it's used kind of, kind of ubiquitously.
434.24s - 440.08s |  No one talks about it as much because it isn't a super sexy privacy notion as we talk about like,
440.08s - 446.72s |  you know, bounding ratios and exponents, but it's used by like a virtually every like a big tech
446.72s - 458.19s |  company, and now sort of like a growing use in, in like government. So now let's return to,
459.15s - 467.79s |  to the like a core of my talk, which is, which is the field of like a differential privacy has
467.79s - 473.31s |  paid a whole lot of attention into like algorithms, guarantees, mathematics, theorems, proofs,
473.31s - 479.07s |  publishing academic papers, which is wonderful. And that's my entire background. But there has
479.07s - 485.47s |  been very little attention paid to these really core concepts of things like policies, regulation,
485.47s - 492.75s |  government, practice, how do organizations make decisions about these tools, communication,
492.75s - 497.71s |  how to tell people how they work, why they work and the nature of the guarantees.
499.63s - 506.83s |  So in particular, there is no step by step guide for a new organization who may be like,
506.83s - 511.71s |  here's my talk. And it's like, wow, wow, that's amazing. I want those tools for my data.
511.71s - 519.71s |  There's, there's no handbook for it. And in particular, a couple, four things that are
519.71s - 525.87s |  missing is there's no guidance in terms of how do you pick epsilon, it seems really important,
525.87s - 534.91s |  how do you pick it? How do you communicate guarantees about this, about these sort of
534.91s - 540.11s |  like a nature and strength of the privacy guarantees to the stakeholders? How do you
540.11s - 546.11s |  understand privacy accuracy trade off? Inherently, if you are doing something that gives stronger
546.11s - 552.35s |  differential privacy, you're gonna take a little bit of an accuracy loss. How do you how do you
552.35s - 558.67s |  understand that? And then finally, how do you align technical privacy guarantees with the like
558.67s - 566.35s |  policies and laws? And is that and this is also a sort of like an outline of my talk.
566.35s - 569.23s |  Realistically, I'm going to skip four so that they don't get mad at me.
572.03s - 580.03s |  But let's talk first about about one. So, so this is sort of like, you know, at the crux
580.03s - 585.39s |  of a sort of everyone's implementation questions. I always get asked, what's the right epsilon?
586.35s - 592.91s |  And the truth is, there is no single right answer. There's no easy answer. So this epsilon
592.91s - 600.43s |  parameter quantifies information leakage, how much can can be learned in a like, slightly
600.43s - 608.11s |  convoluted, definitional way about individuals, and the epsilon can be tuned to be any value
608.11s - 614.91s |  between zero and infinity. So, so like, you know, how, how much privacy leakage is acceptable?
619.15s - 626.91s |  Oops. And there's no easy answer, in part, because there are no best practices for it
626.91s - 633.15s |  is kind of a wild, wild west. Everybody just like picks something that that works. They also don't
633.15s - 637.15s |  don't really tell people what they do. Most of the time, there's a lot of like, you know,
637.15s - 643.15s |  closed door meetings about the right epsilon, but it's very infrequently published. Additionally,
644.19s - 650.67s |  this is sort of getting at the privacy accuracy trade off, they'll talk about more. And of course,
650.67s - 655.23s |  like a different applications are going to have have like a different privacy needs and different
655.23s - 660.51s |  accuracy needs. And so they're like, quote, unquote, right, epsilon should depend on the
660.51s - 665.79s |  use case on the specifics on the needs on the organization. And it might look like a very
665.79s - 673.31s |  different in the federal government bend versus in a startup versus versus in a large tech company.
673.31s - 678.35s |  It might look different if we're talking about advertising versus versus healthcare, it makes
678.35s - 685.87s |  sense that it should be different. So in fact, one thing that the field has has like, um, proposed,
685.87s - 691.63s |  but it's not quite done yet, is the notion of an epsilon registry. And this is an idea that
691.63s - 697.87s |  will sort of like increase our transparency around privacy parameters. So the organizations
697.87s - 707.07s |  can really share, share, share, share, share, share, not just what is their epsilon, but also
707.07s - 713.23s |  sort of like all the other features that are important in terms of why why that was their right
713.23s - 722.82s |  choice. One more reason why it's hard is because it's hard to articulate what this epsilon
722.82s - 728.34s |  means. No one like who has an intuitive, yes, 1.2 feels right in my soul.
729.86s - 737.06s |  But instead, it's really hard to explain. It's a unitless parameter. It is contextless. But as we
737.06s - 743.46s |  talked about, like a context is important. It's like a very mathematical, it involves a like a
743.46s - 749.70s |  logarithm on the worst case bound of the of the like, you know, probabilities of a randomized
749.70s - 753.94s |  algorithm. It's hard to like, you know, gain intuition about what is right there.
755.70s - 760.02s |  And it's sort of like it doesn't match what we think about when we think about privacy.
760.02s - 769.46s |  So you can sort of imagine there's a picture and it looks like this. And it's like, you know,
769.46s - 777.94s |  stronger privacy guarantees versus weaker privacy guarantees. And like an important detail is that
777.94s - 784.18s |  like the way in which in which like a differential privacy is achieved algorithmically is by sort of
784.42s - 790.02s |  sort of like I'm adding noise to the process. And you add noise in a carefully calibrated way.
790.02s - 794.34s |  And there's a lot of work thinking about how much noise what type of noise and like where
794.34s - 798.98s |  and in your process, you sort of like I'm always add a little bit of randomization in there.
798.98s - 806.66s |  And so the question is really about how much noise. This is a question that I have worked on,
806.66s - 812.10s |  but I think I'm gonna skip it. So they don't get mad at me. And I'm sorry that we started late.
812.90s - 821.46s |  I'm very happy to like, you know, share share things about this. But I'll say that are sort
821.46s - 826.18s |  of like, you know, main me like I'm finding of this work is that if you give people
827.54s - 832.26s |  math based explanations in terms of the like a likelihood of what will happen
832.26s - 837.78s |  under various choices, accompanied by some like a visualization of those choices,
837.78s - 845.86s |  it increases objective comprehension, and also increases feelings of self efficacy
845.86s - 850.18s |  in terms of like, you know, having enough information to make choices. And again,
850.18s - 856.50s |  this sort of right answer isn't global, but it is like, you know, what is the right choice for your
856.50s - 862.34s |  for you for your application. And so feeling as though you like you have enough information is
862.34s - 870.90s |  important. Let's go to number two, talking about like, you know, communicating guarantees to the
870.90s - 876.74s |  other stakeholders. So the previous sort of like an explanation of epsilon was really just about
876.74s - 883.38s |  the epsilon to people who are going to share data in some like an algorithmic process. There's lots
883.38s - 891.70s |  of other important people in this process. So we have so we have data subjects, who are the ones
891.70s - 898.98s |  who have to like share, share data. And like I decided, it's a good idea. We also have how
898.98s - 903.86s |  like I'm engineers who have to make decisions, and they have to like implement code and they
903.86s - 908.50s |  have to think about like, you know, parameter tracking, downstream implications of privacy,
908.50s - 915.86s |  and so on. We have how like I'm data curators, who are the ones with a power to either share
915.86s - 921.78s |  or to not share data sets with like, you know, various analysts, and they have to like a decide
921.78s - 926.98s |  what is the right time to share or to not share under what conditions under what agreements.
933.52s - 940.24s |  We have data analysts, who really don't care about the privacy part at all. They just care
940.24s - 944.96s |  about analyzing data and about maintaining exactly the same pipeline they were doing before.
946.40s - 950.88s |  And so if you're like a difference of private pipeline looks completely different, and you like
950.88s - 957.20s |  can't use Python, people will be mad at you. And so it's important you also incorporate privacy,
959.44s - 963.76s |  privacy tools in a way that is not not so like a destructive in that.
965.92s - 974.00s |  There's also like a privacy officers have to think about things like things like a compliance
974.08s - 982.24s |  in the organization, regulatory requirements. Executives have to make a decision. Is this good
982.24s - 986.72s |  for my company? What is the cost of I have to like, you know, hire a bunch of privacy engineers?
986.72s - 991.28s |  How much is that going to cost? If you talk to me about like, you know, accuracy loss,
991.28s - 1001.17s |  I don't want accuracy loss. There are lawyers who have to like a decide is the use of this
1001.17s - 1008.45s |  technology compliant with the privacy laws and the regulations around the use of data.
1009.09s - 1013.65s |  And there's also like a policymakers who get to make new laws around the use of data,
1013.65s - 1020.64s |  if we think our current ones are like not particularly, particularly adequate.
1022.56s - 1029.92s |  And so for sort of all of these entities, they have to make different types of decisions,
1030.48s - 1037.36s |  they have to, and they also have like a different amounts of sort of like a background expertise,
1037.36s - 1043.84s |  and, and like, you know, realistically, attention span. And so you want to tailor,
1043.84s - 1049.04s |  tailor like communication strategies to these individuals in a way in a way that sort of
1049.04s - 1056.24s |  really like a meets meets their, their, their needs. I have more to say about a lot of these.
1056.32s - 1061.52s |  But again, we're going to skip to the next topic. And like, talk to me afterwards,
1061.52s - 1070.93s |  if you want to hear more. Good. So, so for topic three, understanding this sort of a privacy
1070.93s - 1077.65s |  accuracy trade off is really important. Because of course, nobody likes to lose privacy, but also
1077.65s - 1086.96s |  nobody likes to lose accuracy. So we have this like a picture before. And this is really the
1086.96s - 1093.76s |  way it is like I described in academia is we say, on the left, there's like good privacy and bad
1093.76s - 1099.44s |  accuracy. And on the right, there is bad privacy and good accuracy. And like, you know, pick a
1099.44s - 1106.56s |  spot in the middle. And, and like, you know, you can pick where in the middle you feel like being,
1106.56s - 1112.72s |  but that's sort of what you get. But I think this is not really capturing what is happening.
1113.68s - 1118.16s |  Instead, I think the picture looks a little more like this, how there's some like, you know,
1118.16s - 1123.52s |  privacy accuracy curves. And of course, you can't get like, you know, amazing privacy,
1123.52s - 1129.60s |  amazing accuracy at the same, same time. And so that's infeasible. In the bottom left,
1129.60s - 1134.56s |  you get bad accuracy and bad privacy. And you can for sure do that, but it's not particularly helpful.
1135.44s - 1139.04s |  And then you have these like, you know, spaces in between, maybe there's some like, you know,
1139.04s - 1144.56s |  feasibility curve or some like, you know, Pareto frontier of the trade-off based on current,
1144.56s - 1149.84s |  like a technology. And then, you know, a lot of the work on the algorithmic side is sort of like,
1149.84s - 1156.56s |  you know, pushing this curve further out so that we can, we can get like, you know, better privacy
1156.56s - 1162.40s |  and better accuracy simultaneously. Like at a current moment in time, we have some,
1163.12s - 1167.60s |  some, some curve and the company can, can like pick, I feel like being there on this curve.
1169.60s - 1178.96s |  One important question, this is a cartoon. How do we quantify privacy? How do we quantify accuracy?
1178.96s - 1185.92s |  What do these things actually really mean so that we can like actually truly plot them? And the
1185.92s - 1193.36s |  truth is we're not super good at doing either of them. So, so we'll start with the privacy challenge,
1194.08s - 1201.76s |  which is, which is it like, yes, on the privacy axis, we can plot epsilon. But as I already
1201.76s - 1207.44s |  mentioned, epsilon is probably not what your company's executive thinks about as being important.
1207.44s - 1211.52s |  But rather they probably think about like, you know, privacy risk, privacy threats,
1211.52s - 1216.96s |  how will this privacy tool protect my data against various attacks, threats, and so on.
1216.96s - 1225.76s |  But, but you sort of like us, sort of, sort of like standard version of doing like a risk
1225.76s - 1232.08s |  assessment under a DP, it doesn't quite match what people think about. So this is typically
1232.08s - 1237.60s |  like a worst case. So we're thinking of an adversary that has complete knowledge of the
1237.60s - 1243.20s |  data set. And they're trying to guess Alice's data. Again, that's probably not the type of
1243.20s - 1250.72s |  adversaries that you're facing, can you do better against a more like no realistic adversary.
1251.44s - 1257.76s |  Additionally, like a differential privacy provides like a relative guarantees. So it doesn't say,
1258.40s - 1263.20s |  say things like, you know, there's a maximum 1% chance, but they say there's a maximum
1264.16s - 1269.12s |  1% multiplicative increase relative to the chance of an attack without this person's data.
1270.00s - 1274.72s |  Again, that's probably not, not what you're like an organization's security team is really
1274.72s - 1280.40s |  thinking about. And then finally, finally, like a differential privacy is really focused
1280.40s - 1286.56s |  at the individual level. And it's really thinking about like, I have to protect individual's data,
1286.56s - 1293.12s |  population level inferences are great. That's the goal. And therefore they are not considered
1293.20s - 1300.96s |  like a privacy violation. However, maybe your company's database of its customers
1301.52s - 1308.64s |  reveals something about it's like, you know, marketing strategy, or it's like, target audience
1308.64s - 1313.84s |  or some other like a proprietary information you don't want to be shared. That is like a
1313.84s - 1319.60s |  population level in the database rather than protecting one single individual. So again,
1319.60s - 1325.84s |  so again, all these properties come out very naturally from the like a differential privacy
1325.84s - 1330.56s |  definition, but it's probably not what is important at the organizational level.
1332.96s - 1339.20s |  So an important challenge includes like a developing risk frameworks that match like,
1339.20s - 1344.08s |  you know, realistic attacks, what are you actually worried about? And then sort of understanding
1344.08s - 1349.20s |  how does how does like a protection against those attacks match or not match like a differential
1349.20s - 1354.64s |  privacy and vice versa. I've done a little bit of work thinking about like a taxonomy of attacks
1354.64s - 1366.32s |  there. I'm happy to, to sort of like a chat about afterwards. And so now on the accuracy side,
1366.32s - 1372.88s |  I will say in general, organizations are really good at sort of like understanding how to like
1372.88s - 1377.36s |  track and evaluate their important metrics, things like revenue, things like things like,
1377.36s - 1381.76s |  you know, customer engagement, things like market share. Organizations are really,
1381.76s - 1389.76s |  really good at this. However, that doesn't match, match the sort of like accuracy guarantees
1389.76s - 1395.44s |  coming naturally out of the like a differential privacy literature, which are things like,
1395.44s - 1405.97s |  you know, high probability accuracy, additive accuracy bounds, bounds on some like, you know,
1405.97s - 1412.29s |  on some like a particular statistic that is computed. How do you map that into revenue?
1413.25s - 1418.45s |  Um, I imagine there's a lot of like, you know, consultants in the world who will happily do that.
1418.45s - 1423.41s |  But I think also, this is a chance to like, you know, think a little bit more about the impact
1423.41s - 1430.85s |  of the privacy guarantees onto the like operational functioning and things like, you know, if the
1430.85s - 1438.85s |  width of my like, you know, 95% confidence interval of a value shrinks by half, how does that impact
1439.81s - 1448.98s |  revenue? And like, you know, those are the important types of questions. So again,
1448.98s - 1453.14s |  there's a couple like, you know, research challenges here. One of them is a better
1453.14s - 1459.22s |  understanding of the pipeline of sort of like how to map like a differential privacy guarantees
1460.74s - 1466.74s |  into these needs. But importantly, maybe we can pitch it better, especially like, you know,
1466.82s - 1474.18s |  to these executives, rather than saying, you're going to lose accuracy, maybe we can say using a
1474.18s - 1478.26s |  differential privacy is going to like an increase, like, you know, customer trust, and they're going
1478.26s - 1483.46s |  to share data, and they're going to opt in more, and you'll get like, you know, better data, better
1483.46s - 1494.42s |  PR, and so on. So my fourth topic is about laws, I was told I'm done. But I will just say, say,
1495.06s - 1502.66s |  say there's like a huge gap between the like a privacy, privacy, sort of like a mathematical
1502.66s - 1510.34s |  guides, and the in the like a legal framework. And so I think there's a lot of interesting work
1511.38s - 1516.66s |  to be done there. So I will like, you know, flash my slides of the question, when is like,
1516.66s - 1522.82s |  no differential privacy compliant with like a various laws, and some reasons why it's hard.
1522.82s - 1528.66s |  And I will thank you for your time, and your attention. Come see me at the like a crypto
1528.66s - 1535.86s |  privacy village immediately after this, if you want to chat, or my like a surprise talk at 130.
1535.86s - 1540.02s |  Also in the like a crypto privacy village, where I will talk more about like, you know,
1541.38s - 1545.54s |  about like a privacy laws and privacy tools. So thank you.