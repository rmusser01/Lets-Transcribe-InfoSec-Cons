{
  "webpage_url": "https://www.youtube.com/watch?v=pTSEViCwAig",
  "title": "DEF CON 32 - On Your Ocean's 11 Team, I'm the AI Guy (technically Girl) - Harriet Farlow",
  "description": "One of the best parts of DEF CON is the glitz and glam of Vegas, the gambling capital of the world. Many have explored hacking casinos (on and off stage). Unfortunately, it\u2019s just not like it is portrayed in the Oceans franchise.. in real life there\u2019s much less action, no George Clooney, and it\u2019s a lot harder to pull off a successful heist.\n\nFortunately I\u2019m not your typical hacker, I\u2019m an AI hacker. I use adversarial machine learning techniques to disrupt, deceive and disclose information from Artificial Intelligence systems.\n\nI chose my target carefully: Canberra Casino. It\u2019s the best casino in my city.. It\u2019s also the only casino but that\u2019s not the point. The casino industry is at an interesting inflection point. Many large casinos have already adopted AI for surveillance and gameplay monitoring, smaller casinos are starting to make the transition, and there\u2019s only a couple of companies in the world that provide this software. It\u2019s ripe for exploitation.\n\nIn this talk I\u2019m going to show you how I bypassed Casino Canberra's AI systems - facial recognition, surveillance systems and gameplay monitoring. AI Security is the new cyber security threat, and attacks on AI systems could have broad implications including misdiagnoses in medical imaging, navigation errors in autonomous vehicles.. and successful casino heists.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2261,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.08s - 2.76s | This text was transcribed using whisper model: large-v2

 Are there any Australians here in the room?
2.76s - 8.40s |  Yes, so you might be able to tell from my accent I'm from Australia and back home
8.40s - 11.80s |  we have a drinking tradition as well for special occasions and it's called a
11.80s - 17.56s |  shoey. I don't know if any of you have heard of it but basically you pour like a shot in a
17.56s - 21.72s |  shoe and you drink it from the shoe which sounds disgusting but it's a thing
21.72s - 26.24s |  that Aussies do so you know this this shot is a lot of fun but you'll have a
26.24s - 32.08s |  lot to learn from Australians. All right so yeah thank you very much for being
32.08s - 38.12s |  here I'm really stoked. The presentation today is called On Your Oceans 11 Team
38.12s - 42.80s |  I'm the AI Guy even though I'm a girl that it just has a better ring to it I
42.80s - 47.92s |  need to maybe find a better title but this is my second time at DEF CON and my
47.92s - 52.44s |  first time speaking and I'm a little bit jet-lagged because of the the trope and
52.44s - 57.48s |  I'm speaking four times this week so forgive me if I'm a bit tired like
57.48s - 60.64s |  forgetting all the the order of the slides for various presentations you
60.64s - 64.92s |  sort of submit things and I didn't actually expect to get all of them in so
64.92s - 68.48s |  I'll be giving this talk a little later in the AI village as well and I gave a
68.48s - 74.48s |  similar one at B-Sides but yeah as you can probably tell from the the
74.48s - 79.04s |  description and the title of the talk I'm an AI person so maybe the casino
79.04s - 83.52s |  part has kind of been shoehorned into an AI security talk but I think it makes
83.52s - 88.32s |  sense for Vegas right we want it to be to be sort of relevant I thought it'd
88.32s - 92.42s |  be an interesting topic to sort of go through AI security but before I get
92.42s - 98.16s |  into the topic can I get a show of hands if you're like a cyber person if you can
98.16s - 103.68s |  see yourself a cyber person okay and what about the people who would consider
103.68s - 110.00s |  themselves an AI person cool okay and anyone who finds these robot tests
110.00s - 113.28s |  these days really hard and isn't sure if they're a person at all I feel like
113.28s - 117.44s |  that's the camp I sometimes sit in okay so I suspected that the crowd would
117.44s - 125.24s |  mostly be cyber people so the talk is focused on sort of explaining AI
125.24s - 128.80s |  security in a way for for cyber folks so there's a little bit of theory at the
128.80s - 132.36s |  beginning and I apologize for that and if you're an AI person please don't come
132.36s - 138.92s |  at me but before I now I know a little bit about who you are who am I I'm
138.92s - 144.12s |  pretty sure I'm a human even though I failed the robot test sometimes but I'm
144.12s - 148.44s |  in AI security I've been working at the intersection of AI and security for
148.44s - 152.32s |  about 10 years now my undergraduate was in physics and I didn't really know what
152.32s - 156.16s |  to do with it so like many physicists I went into data science and so I worked
156.16s - 160.64s |  as a data scientist in Australia in a consulting company for a while mostly on
160.64s - 164.72s |  defense projects I had my quarter-life crisis and moved to the United States
164.72s - 168.76s |  and worked at a startup then during COVID I moved back to Australia and
168.76s - 172.16s |  worked in the Australian government and that's when I did my PhD in machine
172.16s - 176.48s |  learning security and I found that at the time when I started working in the
176.48s - 180.64s |  field in 2021 no one was really talking about machine learning security people
180.64s - 185.52s |  are now fortunately but that's why about a year ago I left my job and I started
185.52s - 190.52s |  my company more ever security labs and we're an Australian based startup we
190.52s - 193.48s |  provide workshops and training in AI security and we just got funding to do
193.48s - 197.68s |  our first tech product which is exciting but you can also find me at Harriet hacks
197.68s - 201.92s |  on various social media platforms maybe you can find me there if you want it's
201.92s - 206.20s |  it's a bit hacky like we all got to start somewhere so please be forgiving
206.20s - 211.52s |  but our objectives today are to hack the casino we've probably all heard of some
211.52s - 216.00s |  pretty high-profile casino related hacks in the cyberspace but I really wanted to
216.00s - 220.84s |  explore the AI dimension specifically because we are at the sort of inflection
220.84s - 224.16s |  point where so many organizations including casinos are adopting AI now
224.16s - 228.48s |  and most of them are inherently insecure so I wanted to test this so we're going
228.48s - 232.12s |  to start with some exploration work I didn't know much about casinos when I
232.12s - 236.64s |  like to come out with this topic so that that was part of it then we're going to
236.64s - 241.52s |  hack the facial recognition AI and then I guess we have to think of some lessons
241.52s - 246.20s |  learned in AI security right so why did I pick the casino apart from thinking it
246.20s - 251.48s |  would sort of fit for the Las Vegas DEF CON context so much AI is vulnerable
251.48s - 255.50s |  however people only seem to care when you lose a lot of money or when there's
255.50s - 259.92s |  a big hit on your brand reputation and casinos are a really good example of an
259.92s - 265.12s |  organization like this where you know the casino always wins right so bringing
265.12s - 268.88s |  people in the door and having people trust your brand and want to you know
268.88s - 271.80s |  have entertainment when you're definitely statistically going to lose
271.80s - 275.84s |  money is tough and so thinking about security from that context is really
275.84s - 279.68s |  important but it's also a good analogy for literally every organization right
279.68s - 285.72s |  now that's adopting AI so a few disclaimers before I start I worked with
285.72s - 290.52s |  casino Canberra on this and you know that I'm from Australia well I'm from a
290.52s - 296.92s |  town in Australia called Canberra does anyone know Canberra yay yeah Canberra's
296.92s - 299.76s |  I don't know I have a love-hate relationship with Canberra it's pretty
299.76s - 305.44s |  boring but it's got a great quality of life and you know it's got some things
305.44s - 310.16s |  going for it but we it's our political capital for those of you who might not
310.16s - 313.80s |  know that a lot of people think Sydney's our capital but it's it's actually
313.80s - 321.52s |  Canberra so imagine DC but like 1 14th of the size because Australia is 1 14th
321.52s - 325.20s |  of the population of the United States so we only have one casino and I'm
325.20s - 328.56s |  really glad that they were willing to be you know really cooperative with with
328.56s - 333.36s |  me on this so I'm you know massive thank you to casino Canberra so they are our
333.36s - 338.64s |  town's best casino so that that's pretty exciting as a side note in my uber ride
338.64s - 345.32s |  over here my uber driver said that his son vacationed in Australia and their
345.32s - 348.80s |  honeymoon was in Canberra and they loved it so much that they named their child
348.80s - 355.76s |  like Giovanna Canberra and I don't know any anyone who's familiar with Canberra
355.76s - 359.56s |  would just find that shocking but anyway um so going on with the disclaimers um
359.56s - 364.72s |  so obviously the attacks are real but you know I'm an ethical hacker I you
364.72s - 368.24s |  know I care about casino Canberra I'm not literally showing you how to attack
368.24s - 373.56s |  their systems or you know specific AI systems and like I said the talk is
373.56s - 380.16s |  aimed at cyber cyber folk so a little bit about casinos for the people like me
380.16s - 384.24s |  who might not have known about casinos they obviously like a lot of money they
384.24s - 387.60s |  have a controversial history and there's a lot of reasons why that history is
387.60s - 392.56s |  sort of controversial casinos are a hotspot for money laundering because
392.56s - 396.44s |  it's an easy way to move funds around particularly in Australia there's been
396.44s - 401.84s |  some controversy lately our two big casino brands star and crown had to pay
401.84s - 406.56s |  big fines because they didn't comply with any money laundering laws and
406.56s - 410.44s |  there's been Royal Commissions and stuff and it's the same in other countries as
410.44s - 415.52s |  well casinos are getting a little bit of heat so security is important they use
415.52s - 420.16s |  AI obviously otherwise this talk wouldn't have gone as I planned and they
420.16s - 425.56s |  use AI for lots of different reasons and as I'll go into later they rely
425.56s - 430.20s |  specifically on facial recognition and person detection but the landscape is
430.20s - 432.72s |  really interesting when it comes to casinos because just like any
432.72s - 435.96s |  organization they're trying to figure out exactly what the most strategic or
435.96s - 440.64s |  the best way of actually adopting and integrating AI is and just like any
440.64s - 444.08s |  organization they have to rely on third-party providers for the most part
444.08s - 446.44s |  or they can bring in their own consultants but that's sort of
446.44s - 451.04s |  expensive and not all casinos can necessarily do that and there are also a
451.04s - 457.16s |  few companies that provide AI services to casinos that are you know already
457.24s - 461.28s |  providing things like chips and and cards to casinos as well so it's a very
461.28s - 467.24s |  small space. When I first started this talk I thought that things like card
467.24s - 473.24s |  counting would be a use of AI in casinos and it's not so you're probably
473.24s - 477.44s |  familiar with card counting and it's not illegal it's considered advantage play
477.44s - 482.12s |  so a casino however does have the discretion to like prevent people from
482.12s - 485.80s |  playing there for any reason so if a person is very ostentatiously card
485.88s - 489.68s |  counting and winning a lot of money probably the casino is going to not want
489.68s - 494.12s |  to have them there but it was interesting because I expected that
494.12s - 501.08s |  things like card counting would be a big use case for AI and it wasn't. So how do
501.08s - 508.88s |  we actually hack an AI? Sorry this is the first bad joke of the day so when I
508.88s - 512.60s |  first told my mum I worked in AI she said oh darling what are you doing
512.60s - 517.60s |  working in artificial insemination because I know that was a pretty laugh
517.60s - 522.60s |  but thank you. She works in medicine and so to her that's what AI is and so
522.60s - 527.40s |  obviously for most people here AI is artificial intelligence but even though
527.40s - 531.28s |  we know it means those two words what people have in their mind when they
531.28s - 535.92s |  think of what artificial intelligence is definitely differs. Some people imagine
535.92s - 540.64s |  Skynet and things like artificial general intelligence, some people imagine
540.64s - 544.52s |  algorithms, machine learning systems, some people just imagine anything that's
544.52s - 548.64s |  sort of at the cutting edge of technology. So when we think about how to
548.64s - 552.32s |  hack an AI system we really have to think about like the actual technology
552.32s - 555.84s |  that we're trying to hack rather than sort of a some sort of AGI related goal
555.84s - 560.52s |  so I'm focusing very much on machine learning models but if we really think
560.52s - 564.40s |  about the history of AI and if we think about it as just something that does
564.40s - 569.52s |  something a human might otherwise have done I mean the idea of hacking AI has
569.52s - 575.72s |  been around for a long time. So this is an example from a casino context as we
575.72s - 578.88s |  know that there are no random numbers in computers right it has to be an
578.88s - 582.32s |  algorithm and so there are some pretty high-profile cases of people being able
582.32s - 586.76s |  to basically hack the algorithm. People who have access to sort of like the
586.76s - 591.08s |  seed information of these algorithms have been able to like steal money
591.08s - 595.92s |  pretty large sums. In 1995 Ron Harris was able to steal $100,000 this way
595.92s - 599.16s |  because he knew about the algorithm was able to predict the next numbers very
599.16s - 603.72s |  successfully in in a game of Keno. I hope they're not having more fun than
603.72s - 607.84s |  us because they're very loud. Do I need more bad jokes? I don't think you really
607.84s - 612.84s |  want that either. So that's one example of hacking an AI I guess and the other
612.84s - 617.16s |  is sort of card counting. So if we think about it the reason that the casino
617.16s - 620.60s |  always wins right is because the game is statistically in their favor and that
620.60s - 625.88s |  goes anywhere from sort of a 2% to a 25% advantage where 25% would sort of be
625.88s - 629.80s |  like the slot machines and then 2% would be a game of blackjack and that's
629.80s - 635.00s |  why card counting and blackjack is most likely to be able to reduce the casinos
635.00s - 640.40s |  favor because if you if you use like perfect strategy which is just doing the
640.40s - 644.84s |  the plays that are statistically most likely to have you win plus card counting
644.84s - 649.48s |  you can reduce that to like closer to closer to zero and the only reason that
649.48s - 653.80s |  the slot machines are 25% max is because of regulation anyway otherwise the
653.80s - 656.92s |  casinos would probably want them to be higher but that's an example of algorithm
656.92s - 660.00s |  hacking too because you're hacking the statistical process of being able to do
660.00s - 664.92s |  that. So when we think about the modern iteration of hacking an AI system we
664.92s - 668.44s |  come to the realm of adversarial machine learning and everyone probably has seen
668.44s - 671.96s |  this picture before so I feel like I have to show it but basically the idea
671.96s - 677.76s |  is that you're able to create specifically crafted adversarial
677.76s - 682.26s |  examples that you can then input into a machine learning model to make it not do
682.26s - 687.14s |  what it's meant to basically. So this example is from ten years ago and this
687.14s - 691.90s |  is like an image of a panda where you can have an adversarial example
691.90s - 696.86s |  superimposed on top of that which is just perturbations to the image
696.86s - 700.62s |  specifically crafted based on the target model that you're trying to attack and
700.62s - 705.54s |  it can disrupt or deceive the model really successfully even though a human
705.54s - 709.78s |  can't recognize it. So adversarial machine learning is the field that my
709.78s - 714.86s |  PhD was in and you can consider it sort of the offensive side of AI security
714.86s - 719.38s |  it's basically the field that is coming up the attacks but it's traditionally
719.38s - 723.70s |  been very academic. Recently we're starting to see more examples of these
723.70s - 730.10s |  kinds of things in real life so being able to actually you know implement some
730.10s - 733.66s |  change in the physical world that's able to prevent say autonomous vehicles from
733.66s - 738.62s |  being able to recognize stop signs so they drive straight through. And in terms
738.62s - 743.38s |  of the landscape of adversarial machine learning attacks it's massive it's just
743.38s - 747.42s |  exploded over the last ten years and if we think about all of the different
747.42s - 752.34s |  attack surfaces represented by AI systems and so on the top things like
752.34s - 755.98s |  convolutional neural networks which is a machine learning model really good at
755.98s - 761.02s |  computer vision versus something like a transformer along the bottom which is
761.02s - 764.98s |  very good at natural language processing in the backbone of things like GPT.
765.82s - 769.90s |  They're very different attack surfaces but many different adversarial machine
769.90s - 774.58s |  learning methods now exist that are able to very successfully do bad things at
774.58s - 779.62s |  each stage of that attack at that attack surface. So most of the time these
779.62s - 783.70s |  attacks require you to actually change something about the object itself right.
783.70s - 788.90s |  So you have the panda example before we have things like adversarial glasses
788.90s - 793.38s |  that prevent a person being recognized from detection. You have like a jumper
793.42s - 798.26s |  that can also hide a person from person detection. And the example I love on the
798.26s - 803.18s |  bottom right is this DARPA model that was meant to be used in sort of urban
803.18s - 807.14s |  warfare environments to detect people and they asked their Marines to try and
807.14s - 810.74s |  red team that model and they found that the Marines could just act not like
810.74s - 814.10s |  people and very successfully deceive the model. So they did things like put
814.10s - 817.62s |  cardboard boxes on their heads. They waved branches around and acted like
817.62s - 821.10s |  trees whatever that means. And they were very successfully able to hack the
821.10s - 826.76s |  model. The the challenge with all of these things is that you're actually
826.76s - 830.20s |  required to change something about the object itself that's being classified.
831.40s - 835.52s |  There are some examples of like attacks where you don't need to change the
835.52s - 839.56s |  thing itself. This is called an adversarial sticker. So if I put this next
839.56s - 845.52s |  to something else a model is going to like predict that this is a toaster. So
845.52s - 848.96s |  if I put this next to a banana which is the sort of the example that they they
848.96s - 853.40s |  give instead of recognizing the banana the model is going to see a toaster
853.40s - 857.84s |  instead. However this is sort of challenging because like this is very
857.84s - 860.76s |  obvious like if I put this next to a banana I'd be like that's not meant to
860.76s - 866.60s |  be there. That's a banana. But a model doesn't see it that way. So I had a
866.60s - 871.00s |  thought I was like what if you could like change something about what's in
871.00s - 875.92s |  the image frame itself but you didn't have to perturb the actual image. And so
875.92s - 880.92s |  I came up with this technique called distributed adversarial regions. And this
880.92s - 885.00s |  is part of my PhD where I was sort of coming up with this attack. And so the
885.00s - 889.12s |  intent for this was largely in urban camouflage environments. So being able to
889.12s - 895.96s |  do things like have specially crafted buoys around ships for example like
895.96s - 899.96s |  military ships. So you think it's a container ship instead or having it
899.96s - 904.92s |  around other different kinds of military platforms. And if I can easily get the
904.92s - 914.22s |  video working. This is just a very quick example of what this would look like. So
914.22s - 918.82s |  if you have these sorts of buoys around that's a Japanese U-boat. You can
918.82s - 925.22s |  actually prevent the model from recognizing that platform. Also don't
925.22s - 927.94s |  come at me for them not being real buoys. It would have been a very expensive
927.94s - 933.38s |  endeavor to like put buoys around a vessel. The thing about being able to
933.50s - 938.70s |  digitally manipulate the images is that um you know it's a really good surrogate
938.70s - 941.86s |  for not being able to actually change the physical environment. Um and being
941.86s - 946.86s |  able to test it. So I I wanted to see if I could apply this technique to things
950.78s - 955.78s |  like um facial recognition for this casino context. So how does this relate to
957.06s - 962.06s |  casinos? I assume everyone here is familiar with the Oceans 11 franchise. Yes
963.42s - 966.82s |  I mean it's it's a heist. You know like we don't need to get too too thinking
966.82s - 971.90s |  about it. The idea is that we're basically just bypassing all of these different um
971.90s - 976.30s |  security levels that they have in a really cool way. Ideally there's um like
976.30s - 980.84s |  bourbon and and suits and George Clooney. Um I did not get to have a George Clooney
980.84s - 985.88s |  in my life. I'm doing this talk but that's okay. Um so we get the premise of the
985.88s - 990.88s |  heist. This is what I wanted to do. I wanted to understand uh casino Canberra's I
990.88s - 995.88s |  don't know AI operational environment. Um then I'm going to pick some target
998.12s - 1003.80s |  models to create this attack. And then going to you know implement the the the
1003.80s - 1007.54s |  DAS, the distributed adversarial regions. I'm going to test how I can sort of
1007.54s - 1012.54s |  disguise them in different ways and then lessons learned and sort of next steps.
1014.40s - 1019.22s |  So the first step. I I mentioned my PhD. I've been doing it part time for a really
1019.22s - 1024.02s |  long time. So I feel like I should be finished but I'm not. Um now this is annoying
1024.02s - 1028.30s |  because it's really hard to like simultaneously try and build a start up and also
1028.30s - 1033.00s |  finish a PhD. My supervisors only one of them knows I'm here right now. The other
1033.00s - 1037.68s |  thing comes in Canberra like working working from the university. Um so don't tell
1037.68s - 1042.54s |  them. But the good thing about still doing a PhD is not only do I get really good
1042.54s - 1048.18s |  student discounts but people are very helpful. So if I reach out to people and I tell
1048.18s - 1052.82s |  them that I want to do something as part of my PhD, people generally say yes. It
1052.82s - 1056.68s |  helps that I'm also sort of like nice and polite and you know people are always just
1056.68s - 1061.76s |  very helpful. Um so this is what I did with Casino Canberra. And they were very
1061.76s - 1067.40s |  willing to help which was nice of them. So this is Casino Canberra. Maybe it doesn't
1067.40s - 1072.90s |  look like the Bellagio or the MGM Grand but like they have a a red carpet. Um it's
1072.90s - 1076.48s |  very nice. It's right next to our civic centre. Um it's between there and the
1076.48s - 1081.88s |  convention centre. Um it's you know they they do okay. Um they're very excited
1081.88s - 1085.28s |  about a new upgrade that that they're doing. They're gonna get a lot bigger.
1085.28s - 1091.28s |  There'll be a hotel. Um they they've got it going on. So as part of this upgrade
1091.28s - 1095.70s |  they're thinking about implementing artificial intelligence for the first time.
1095.70s - 1100.96s |  Something I was really interested to learn through doing this sort of research was
1100.98s - 1105.98s |  that um facial recognition and person detection was by far the largest um like the
1108.62s - 1113.86s |  most important form of AI for casinos. But it's obviously very unequal in how casinos
1113.86s - 1120.00s |  are able to adopt them. So all of the casinos we have here in Vegas would very much rely
1120.00s - 1124.04s |  on using AI and they would have very mature practices around it. And in fact they
1124.04s - 1128.18s |  would actually build their casinos using this AI in mind. So when we walk around the
1128.20s - 1132.94s |  casino you might notice that sometimes there are like sort of narrow corridors that you're
1132.94s - 1136.24s |  forced to walk through. And that's to make sure that your faces are actually caught by
1136.24s - 1142.04s |  these cameras. So as as a casino like Casino Canberra is thinking about their redesign
1142.04s - 1146.78s |  this is very much inherent in how they actually decide to design the casino. So that they
1146.78s - 1152.22s |  can use things like facial recognition to identify money launderers or known criminals
1152.22s - 1157.40s |  or card counters I guess. Um also problem gamblers. Um there are some you know nice
1157.42s - 1160.52s |  use cases for it as well. They want to make sure that people aren't gambling too much.
1160.52s - 1165.26s |  Just the right amount. Um and things like number plate recognition for cars is also
1165.26s - 1169.80s |  really important. Um but in terms of AI this is very much a computer vision problem. So
1169.80s - 1173.50s |  being able to see things in the environment and understand it. They don't really have a
1173.50s - 1178.60s |  lot of use for other kinds of AI right now anyway. Not not things like chat bots or signal
1178.60s - 1183.88s |  processing. That's not that's not really in their realm. Um but right now they're also
1183.90s - 1188.80s |  very much dependent on people. Just being able to um like identify those known people and
1188.80s - 1193.40s |  then log them in the system. Um that's that's very much still dependent on on humans even
1193.40s - 1198.40s |  here. So what I did next was I need to choose a victim right? Um like I said Casino Canberra
1200.62s - 1205.92s |  doesn't currently use AI so I couldn't really use any like live facial recognition that
1205.92s - 1211.62s |  they were using right now. So instead I had to pick uh an appropriate victim that would
1211.66s - 1216.30s |  be close enough to something they would use. And you might think that this is a problem.
1216.30s - 1220.44s |  However it's not fortunately because of this principle in machine learning and its
1220.44s - 1226.88s |  convergence. I don't know if this works with a big crowd. Um but I might try some audience
1226.88s - 1230.42s |  participation. I don't know. Does anyone want to shout out and guess what this number
1230.42s - 1238.57s |  might represent? I actually can't hear anyone anyway. So I'm gonna assume that maybe one
1238.61s - 1243.61s |  of you was right. Um so this number actually represents um some significant research into
1246.95s - 1251.95s |  similarity of models. So if you have a model trained on similar data to do similar things
1254.45s - 1259.43s |  you end up with a similar model. Like basically the same model. Um and this is actually
1259.43s - 1264.43s |  really significant because a lot of the models that companies rely on and produce as part
1264.43s - 1269.43s |  of their IP are very much or like you know exactly the same as open source models. And in
1272.63s - 1276.17s |  fact a lot of the time they do actually just use open source models and then do a bit of
1276.17s - 1280.15s |  fine tuning. And that doesn't necessarily change the model itself. Certainly doesn't change
1280.15s - 1284.05s |  the architecture. It might change uh you know sort of how you apply it to different use
1284.05s - 1289.05s |  cases. But it a lot of research has been done to show that if you compare two models an open
1289.91s - 1294.91s |  source model with uh like a a model that is a company's IP to do things like facial
1296.75s - 1301.75s |  recognition. They are 95 to 99 percent similar. It's a minimum of 95 percent similarity.
1303.35s - 1308.23s |  Which is really significant because it means that I don't really need to you know do
1308.23s - 1313.23s |  anything to um difficult to be able to find a good victim model. So that's convergence.
1315.52s - 1320.52s |  So there's a lot of different victim models that I wanted to test. Um like I mentioned I
1322.62s - 1326.26s |  already did this attack for things like urban camouflage. So being able to do things like
1326.26s - 1330.26s |  object detection and testing some of those models. Um I also added some of these facial
1330.26s - 1334.94s |  recognition models um as well. These are just a few of the the open source models available
1334.94s - 1339.94s |  at the moment. So the next step is to create these distributed adversarial regions. Um and
1340.82s - 1345.82s |  this is a fairly boring diagram. So maybe I'll show you instead a slightly less boring video.
1348.34s - 1353.34s |  Um it it's very hard trying to create fun demos for this. I don't know. So um maybe I need to
1357.04s - 1362.04s |  find a way. So so this is the um the the video that I'm testing on right. This is me walking
1362.18s - 1367.18s |  up to the the detection. And this is uh me implementing the attack on one of my target
1372.02s - 1377.46s |  models. Um so I'll talk through the method because it takes a couple of minutes. But
1377.46s - 1382.90s |  basically all I need to do to implement this attack is to uh perform an optimization
1382.90s - 1389.74s |  algorithm on the video in question. To identify different regions where if I perturb them in
1389.74s - 1395.22s |  certain ways um they're going to cause a misclassification which sounds really easy. Um but
1395.22s - 1400.82s |  the the difficult thing is in sort of making those areas not too obvious or at least being able
1400.82s - 1405.82s |  to pick uh a few different regions that could reasonably actually be altered. And so this
1407.86s - 1412.86s |  code shows me being able to like perform that optimization uh based on one of these victim
1413.20s - 1418.20s |  models. Um making a prediction on on this video. If you're not a person who finds the code
1424.73s - 1428.59s |  interesting uh maybe at least the thing you can take away is that it only takes a minute to do.
1428.59s - 1434.57s |  Like it it's very easy. Um and I don't even need to use like an existing um pre-built library
1434.57s - 1438.03s |  or anything. It it's just an optimization algorithm. So so when I said before that there's
1438.03s - 1444.55s |  over a hundred different kinds of adversarial machine learning attacks. Um it's it's really
1444.55s - 1452.76s |  easy to just sort of build them yourself. It it's all optimization at the end of the day. So we
1452.76s - 1457.86s |  can see on the screen here that the the code read out no match found. And then when I add the
1457.86s - 1463.96s |  perturbed image um sorry so that there was a match found um for the clean image and then when
1463.96s - 1475.69s |  I added the distributed adversarial regions there was no match found. So the interesting thing
1475.69s - 1482.09s |  about being able to like do the testing or maybe not the most interesting thing actually. Um but
1482.09s - 1487.09s |  the the process of actually thinking about the attack surface and let me skip through here. Um
1490.63s - 1496.37s |  thinking about the actual attack surface and what we're like affecting in this attack. So all
1496.37s - 1500.87s |  computer vision most computer vision relies on convolution convolutional neural networks
1500.87s - 1506.47s |  because they're the most powerful sort of model to be able to do this. Um and being able to
1506.47s - 1510.75s |  create attacks that work for most of them it's actually pretty easy. So if we think about facial
1510.77s - 1517.71s |  recognition the way that it works is that it it sort of takes your facial geometry and then is able
1517.71s - 1523.05s |  to compress it into an embedding space. Um and being able to identify clusters and sort of
1523.05s - 1530.03s |  relationships between them and then um based on that mapping you know be able to identify me
1530.03s - 1535.47s |  versus not me I guess. And so when I have that information I can do things like create specific
1535.49s - 1540.49s |  patterns that change that distribution. So if I go back and play um this other recording I want to
1550.63s - 1556.07s |  be able to test doing things like um creating jewelry. So this is this is the example that I'm
1556.07s - 1561.07s |  testing. So the the challenging thing about building models or thinking about model
1561.99s - 1566.99s |  security is that all of these models that are that I'm testing on it doesn't actually take much to
1570.67s - 1575.67s |  be able to disrupt them. Um creating this attack is really very simple. Let me skip back to the
1590.01s - 1597.67s |  slides while this uh while this loads. So maybe it doesn't look so interesting but being able to
1598.21s - 1603.21s |  do that attack and like have it not recognize me um I guess that's a way of saying that I hacked the
1604.75s - 1609.75s |  model right. I mean it was able to not recognize me because I created these sort of adversarial
1611.59s - 1616.59s |  earrings that you know when the model looks at that that image of me um it doesn't recognize me.
1618.13s - 1622.83s |  When I test it um that's the important part of the process right from machine learning
1622.83s - 1627.93s |  perspective. This looks very boring too. Um I I tested all of those different models and I
1627.93s - 1632.41s |  wanted to see the extent to which the attack actually works and decreases the confidence level
1632.41s - 1637.41s |  of that attack. Um as a way of distilling all of the sort of the boring columns in the graph, the
1639.01s - 1644.01s |  number that we ended up with was a 40.4 percent reduction in like confidence. Um so what does
1646.11s - 1651.93s |  that actually mean? Is is that significant? Um when I implemented this attack on the urban
1651.95s - 1658.57s |  camouflage environments um it was more successful honestly. It's it's a bit easier for things like
1658.57s - 1664.81s |  ships and military platforms because those models tend to rely a lot more on context and in
1664.81s - 1670.11s |  facial recognition it's a little bit harder because a lot of models would tend to um look at the
1670.11s - 1676.49s |  geometry um slightly differently. But 40.4 percent on the whole of all of the models that I
1676.51s - 1682.09s |  tested is actually really significant because that means that if I have say a confidence
1682.09s - 1687.09s |  threshold of um like 90 percent. So a model looks at an image of me and says with 99 percent
1689.29s - 1694.29s |  confidence that it is me, being able to decrease that confidence by around 40 percent means that
1696.83s - 1701.97s |  if it was originally 99 percent then it's going to be um 40 percent less than that and it's not
1701.99s - 1708.37s |  going to meet that threshold. So I don't actually need to affect a massive change in the model to
1708.37s - 1714.67s |  be able to have it not recognize who I am. And I guess a way of thinking about that is by
1714.67s - 1719.91s |  calling it context. But really what it means is that most of the models that are already out
1719.91s - 1725.01s |  there, all of the open source models that we use and data scientists use and organizations in
1725.03s - 1732.03s |  fact use and then fine tune are vulnerable to an attack like this, which is really very
1732.03s - 1737.03s |  simple you know. Um what I did by adding uh you know regions of perturbation to myself is
1741.01s - 1746.01s |  really is definitely not the most sophisticated the most sophisticated kind of attack. Um
1746.01s - 1750.85s |  yesterday I was at an AI security forum and it was amazing because everyone was sort of
1750.85s - 1756.33s |  talking about all of the latest breakthroughs in in AI security and how we can make models
1756.33s - 1761.53s |  extremely secure and the technical and the governance things. The the really challenging
1761.53s - 1767.61s |  thing though is that most organizations don't have that existing level of maturity and the
1767.61s - 1772.05s |  kinds of models that they're using are these kinds of open source models that only take a few
1772.05s - 1777.05s |  regions to be disrupted and to cause a misclassification. And thinking about the uh I guess
1777.89s - 1782.89s |  thinking about what this actually means is really significant because while I've applied
1785.03s - 1790.57s |  this to a computer vision use case, so this could work for things like retina scanning,
1790.57s - 1795.07s |  number plate recognition as well, without actually having to change the object itself but
1795.07s - 1800.91s |  just changing things around the object um really just highlights how brittle most models
1800.95s - 1805.95s |  are. Um it also highlights that while I did this in a computer vision use case because
1809.05s - 1815.33s |  that's sort of what what works for this um facial recognition casino context, exactly the
1815.33s - 1820.43s |  same kind of attack could be applied to different domains as well. So things like uh
1820.43s - 1825.43s |  natural language processing and signal processing. And when we think about what natural
1825.67s - 1831.73s |  language processing and signal processing actually are in an AI uh use case context, natural
1831.73s - 1836.61s |  language processing does all the the cyber security applications as well. So being able to
1836.61s - 1841.61s |  identify uh malware, being able to um identify like good versus bad code, um they're natural
1844.15s - 1849.85s |  language processing problems. And if we can add perturbances that are actually not about
1849.85s - 1853.99s |  the the thing that we're trying to classify, but it's still able to disrupt the model, that's
1853.99s - 1859.33s |  a real problem. It's the same in the signal processing space, that's things like um RF and
1859.33s - 1864.33s |  audio and um and LIDAR, you know all the signal processing problems we have, medical
1865.67s - 1871.47s |  imaging, um being able to add perturbances that aren't even related to the thing that you're
1871.47s - 1876.47s |  trying to classify represent a massive problem. Um I did some other research that looked at
1876.67s - 1881.67s |  this in large language models as well. Uh this isn't exactly a cool demo, though I I wish it
1885.07s - 1890.35s |  could be, but basically I was looking at being able to add different sort of ideas to
1890.35s - 1895.35s |  language models and see if they could um if they would sort of take up that information and
1897.39s - 1902.39s |  have it alter the messages that they were giving. And we found that um after sort of a few
1902.39s - 1907.39s |  simulations and a few iterations of being able to add this additional kind of like noise
1909.13s - 1914.13s |  material, they did actually successfully adopt these other um these other ideas, even if
1916.13s - 1921.13s |  that wasn't um sort of encoded into them already. The when when we think about attacking
1923.11s - 1928.51s |  machine learning models, the the challenging thing as well is that it's not the same as
1928.53s - 1934.47s |  cyber security. Like when we think about testing a model and identifying uh what what makes a
1934.47s - 1939.45s |  good model, data scientists will use scores like this. Um so they look at accuracy, they look
1939.45s - 1944.25s |  at precision, uh recall an F1 score sort of related to precision as well. They look at all
1944.25s - 1949.25s |  the statistical um functions for a model that makes it um like largely accurate. But the
1952.59s - 1957.57s |  security is rarely ever considered and it's very it's a very different problem to a
1957.59s - 1963.37s |  cyber security problem. I mean what does it mean to hack an AI system itself? You know I I sort
1963.37s - 1970.01s |  of looked at that 40.4 percent number and felt a little bit disappointed. Like I sort of wished
1970.01s - 1974.27s |  that it was a bit more definitive. But the thing with hacking an AI system is that it's not
1974.27s - 1980.31s |  really a a binary um did it work or did it not. But it's more about on the whole we were able
1980.31s - 1985.99s |  to reduce the classification confidence of a model. So it's more it's better to think about it
1986.01s - 1992.11s |  as part of the kill chain. And quite a lot of the attacks at the moment are very much um sort of
1992.11s - 1996.69s |  like a Stuxnet equivalent. You know they're they're very complicated, they're very difficult
1996.69s - 2001.85s |  to implement. That's why there's this idea that they're a bit more academic than than realistic
2001.85s - 2007.87s |  or practitioner based. Whereas even though being able to add like these these random
2007.87s - 2013.27s |  distributed regions to an image um isn't all that sophisticated that the fact that it works
2013.29s - 2017.49s |  and it actually disrupts a model most of the time is a real problem. And it's it's sort of more
2017.49s - 2023.23s |  akin to like the the DDoS attack equivalent in a machine learning sense. Because even though
2023.23s - 2028.23s |  that number was 40.4 percent um in our experience if you're able to leverage some sort of AI attack
2030.93s - 2036.21s |  or some sort of adversarial machine learning attack you're able to hack the system 100 percent
2036.21s - 2041.35s |  of the time. Because there's always a way that you can adapt it for whichever case study you need
2041.39s - 2047.07s |  to. So the fact that so many organizations are reliant on these like inherently insecure
2047.07s - 2053.86s |  models is a real problem. Because AI security is real. So if if we're thinking about like next
2055.16s - 2059.84s |  steps and and lessons learned from an attack like this um we already have a few companies
2059.84s - 2064.84s |  starting to put out reports about the incidence of AI security related attacks. Uh and it's it's
2066.74s - 2072.92s |  changing a lot like it's uh encouraging that since I started in the field it's definitely uh
2072.92s - 2077.84s |  being taken more seriously and we're starting to see these incidents. Um but it's it's really
2077.84s - 2082.50s |  unfortunate because we have so many lessons from the field of cyber security that we could bring
2082.50s - 2087.44s |  into the field of AI security but they just aren't. So it's really important that all of the
2087.44s - 2091.42s |  cyber folk in the room are thinking about how to solve these sorts of problems in AI security
2091.42s - 2098.42s |  as well. Some of the um as I mentioned the the the interview with uh Casino Canberra was part
2098.46s - 2104.90s |  of um many of the interviews we've done and we we've got these interesting insights already.
2104.90s - 2111.20s |  Um 94 percent of the like people and organizations we spoke to could articulate how they use AI.
2111.20s - 2115.74s |  Um even if maybe it's not what everyone would define as AI they they had an idea of how they
2115.74s - 2120.92s |  use AI. Um but only 8 percent could articulate how they secure their AI and most of the time
2120.92s - 2126.22s |  it's never really considered at all. The other reason I wanted to use the Casino case study was
2126.26s - 2133.26s |  because a casino is a really good sort of analogy for a society that it's starting to um be built
2133.26s - 2139.40s |  around surveillance as well. Like casinos are built with surveillance in mind and societies
2139.40s - 2144.84s |  are increasingly sort of moving towards using more AI specifically facial recognition AI to do
2144.84s - 2149.84s |  important things as well. So the the the important thing to think about from like the the the
2156.58s - 2162.02s |  perspective of actually what you can do with the knowledge of these attacks is that even now in
2162.02s - 2167.02s |  the casino environment it's it's still entirely reliant on people and process. And especially as
2171.44s - 2175.94s |  organizations like casinos are going through this inflection point in adopting different AI
2175.94s - 2181.72s |  technologies. Um being able to actually like govern that and implement it well is still really
2181.72s - 2187.48s |  hard. And even though over here we have lots of very um you know technical and and mature
2187.48s - 2191.62s |  conversations about it most organizations aren't are out there are still really grappling with how
2191.62s - 2196.62s |  to do it. It's you know it's still so so reliant on having people to surveil it. And at the end
2200.10s - 2204.76s |  of the day you know I went into you know Casino Canberra I asked very nicely if I could you know
2204.76s - 2209.90s |  try hacking their things and they said yes. So what does it really mean? Um in terms of what
2209.92s - 2214.62s |  Casino Canberra was able to do with this it's it's tough because they're reliant on third party AI.
2214.62s - 2219.26s |  So all you can do is sort of ask the questions and hope that you know you keep it in mind. But
2219.26s - 2224.00s |  there we're really so reliant on these organizations that provide the AI and there's no
2224.00s - 2228.64s |  regulation around how they have to actually secure them or make sure that they're robust. So it's
2228.64s - 2233.64s |  really challenging but at least being able to ask the question um it sort of helps. So um so
2234.02s - 2239.02s |  that's it. I am talking tomorrow as well um about my experience in the Australian government.
2241.66s - 2246.40s |  I worked at an intelligence agency so I'm also speaking in the policy village. Um I'm one of the
2246.40s - 2251.40s |  creative stages about um sort of the policy element of these AI security attacks as well. Um so
2253.90s - 2256.04s |  thank you so much for coming and please do keep in touch.