{
  "webpage_url": "https://www.youtube.com/watch?v=nGDTVPnTDOw",
  "title": "DEF CON 32 - The Interplay between Safety and Security in Aviation Systems - Lillian Ash Baker",
  "description": "Safety has been at the forefront of Civil Aviation since the formalization of DO-178, Software Considerations in Airborne Systems and Equipment Certification, in 1981. However, times have changed since then and we live in a world with seemingly limitless connectivity. DO-356A, Airworthiness Security Methods and Considerations, forms the cybersecurity bedrock in which aviation systems are designed and implemented. In this talk, participants will learn about how Safety and Security is applied to system design and how they interact with one another. Design Assurance Levels (DAL) and Security Assurance Levels (SAL) concepts are presented and explained what their purpose is. This talk is designed to appeal to the general cybersecurity community by introducing fundamentals of Safety analyses and discussing how Safety and Security interact with one another.\n\nThis talk will first touch upon fundamental documents that form the Certification basis for System Development (ARP4754B), System Safety (ARP4761A), and Security Considerations (DO-356A). From there, it walk through pieces that form a safety analysis and Design Assurance Level (DAL), walk through a system architecture under consideration, and learn about how Safety and requirements in a system can be used to inform the Threat Model for the system. From there, we end with a discussion on how Security Mitigations are assigned Security Assurance Level (SAL) and what this means for developers.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 1204,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 3.32s | This text was transcribed using whisper model: large-v2

 All right. Good afternoon, everyone. I am Lillian Ashbaker.
3.32s - 6.00s |  I'm a product security engineer at the Boeing company in
6.00s - 9.16s |  Whisk Aero. Today, I'll be discussing the most important
9.16s - 12.32s |  part of aviation systems development, standards and
12.32s - 16.56s |  certification. But what you I hope you get out of this is an
16.56s - 19.12s |  understanding of how the industry has developed safe
19.12s - 25.26s |  systems, and considers security in those designs. First, we need
25.26s - 29.76s |  to discuss the safety of these systems. And I'll make you all
29.76s - 33.02s |  aware of what safe actually means in the aerospace
33.02s - 36.98s |  industry. I hope you find this as interesting as I have. I
36.98s - 41.26s |  still can't shut up about it 15 years later. So the first
41.26s - 43.98s |  principles of safety that we have to talk about are failure
43.98s - 47.68s |  conditions and probabilities, right? All of this information
47.68s - 53.34s |  comes out of Advisory Circular 25 or 23. If you go read for the
53.34s - 57.02s |  pilots in the crowd, that's part of the CFRs out there that
57.20s - 64.88s |  you're legally supposed to know. So Advisory Circular 25-1309-1A
64.88s - 68.16s |  this is all public information, actually lays out all these
68.16s - 72.28s |  probabilities and kind of puts together this really nice graph
72.68s - 75.28s |  of probability versus consequence, right? So you have
75.28s - 79.56s |  probable, improbable and extremely improbable, as well as
79.56s - 83.76s |  an acceptability of those failures and faults. And there's
83.76s - 86.44s |  a line that runs right between it that says acceptable and
86.46s - 90.10s |  unacceptable, right? So you have to develop the system in order
90.10s - 93.82s |  to understand what the system effects are going to be of that.
95.38s - 98.82s |  So as we move forward, those probabilities, those failure
98.82s - 102.62s |  classifications actually have probabilities behind them as
102.62s - 106.94s |  defined in the advisory circulars, right? So when we
106.94s - 111.74s |  talk about safety, we talk about the probabilities of failure. So
111.74s - 115.22s |  for a minor classification, that's a probable failure
115.22s - 118.64s |  condition on the order of 10 to the minus fifth, major is 10 to
118.64s - 121.04s |  the minus ninth, that got reclassified as 10 to the
121.04s - 126.92s |  seventh. Hazardous is now 10 to the seventh. Catastrophic is 10
126.92s - 132.02s |  to the ninth or less. So that means that like probabilities of
132.02s - 134.32s |  10 to the ninth. So think about the systems that you currently
134.32s - 138.50s |  see in like IT and how often they have to stay up and
138.50s - 146.31s |  operate. Our stuff has to operate and not fail. So the
146.31s - 150.09s |  design assurance levels are a major component of how we
150.09s - 155.13s |  develop these systems, right? So what you have are a nice trace
155.13s - 160.37s |  of failure conditions with the severities, with the actual
160.37s - 163.47s |  definitions of what that is, extremely probable, extremely
163.47s - 167.61s |  remote, and the probabilities themselves that tie off to the
167.61s - 170.57s |  functional DAL assignments. So whenever you're talking about
170.57s - 173.21s |  aviation systems, if you see something that says that it is a
173.21s - 177.75s |  DAL-C system, this is what it means. It means that that
177.75s - 182.99s |  system has a major effect on the system operation of the
182.99s - 190.03s |  aircraft, has a remote possibility of failure, and has a
190.03s - 194.33s |  failure probability of somewhere between 10 to the fifth to 10 to
194.33s - 201.81s |  the seventh probability. So with all these probabilities, the way
201.81s - 205.35s |  that the industry works through that is that we have a bunch of
205.35s - 209.35s |  functional hazard assessments, which takes a functional look at
209.35s - 214.45s |  the individual functions that trace down throughout the system
214.45s - 217.65s |  and look at all the points in which that function can be
217.65s - 223.49s |  impacted. So if it moves through three or four systems, all the
223.49s - 226.17s |  systems in the chain have to have some sort of way of
226.17s - 233.41s |  preventing failure conditions from occurring. So once we do
233.41s - 237.97s |  that, these probabilities go into the functional DAL
237.97s - 241.59s |  assignments. If you go take a look, here's where the standards
241.59s - 247.23s |  start to come in. DO-178, DO-254, and DO-278 for ground
247.23s - 253.29s |  systems operations, as well as ARP-4754 for systems
253.29s - 258.35s |  development. What those standards actually define is not
258.35s - 264.73s |  how you develop a system. It is a very important piece here. The
264.73s - 269.53s |  development objectives that you have to meet. So that means that
269.53s - 272.57s |  you can't just go write code. You have to have requirements
272.57s - 275.77s |  for the code. You have to have traceability down to the code
275.77s - 278.71s |  itself. You have to have traceability to the actual
278.71s - 281.77s |  compiled code. And then you actually have to test all that
281.79s - 286.57s |  code. And it's defined in tables and design objectives throughout
286.57s - 293.73s |  these different, these different documents. Those design
293.73s - 296.91s |  objectives that you work through are the deliverables that go to
296.91s - 301.01s |  a regulatory body. In the U.S., that would be the FAA. Over in
301.01s - 303.85s |  Europe, that would be EASA. If you have dual use between the
303.85s - 307.19s |  two of them, it will be shared between the two of them. Those
307.19s - 311.49s |  documents are the proof that you did the work. You have to prove
311.51s - 316.29s |  that you did the operations and did the work. So as you have a
316.29s - 320.09s |  higher DAO level, so DAO A would have the most stringent. What's
320.09s - 324.43s |  unique about DAO A is that there is a required independence in
324.43s - 329.53s |  the testing. Which means the person who developed the system
329.53s - 332.53s |  cannot be the one who has tested the system. It has to be a
332.53s - 336.67s |  completely independent body that does the verification of your
336.67s - 340.71s |  code or of your system, of your hardware to ensure that there is
340.73s - 343.77s |  independence between the developers and the test teams.
343.77s - 350.66s |  All right. So now you all are experts on safety. So now as we
350.66s - 357.38s |  move forward, we start to talk about what DO356A is. So DO356A
357.38s - 360.74s |  defines the cybersecurity development objectives for
360.74s - 365.98s |  aircraft systems. One of the most important concepts in that
365.98s - 370.82s |  is this concept of IUEI, which is intentional, unauthorized,
370.84s - 375.34s |  electronic interactions. What it means is that someone has to be
375.34s - 380.22s |  doing an attack intentionally. They did not have authorization
380.22s - 384.76s |  for the system. It has to be electronic interactions. So no,
384.76s - 388.22s |  birds flying, we know that they're not real. Birds flying
388.22s - 393.60s |  into engines do not count. Right? That's not intentional by
393.60s - 398.60s |  the bird to do that. So this is what makes security threats
401.60s - 407.60s |  different from safety threats in our system development. But how
407.60s - 410.52s |  we develop our systems around the two of them kind of start to
410.52s - 415.52s |  touch. So safety is more of a kind of a what and why questions
415.52s - 419.96s |  of how all these unintended adverse effects happen within
419.96s - 424.36s |  systems operation. Security looks at more of the who and
424.36s - 429.44s |  where questions. Right? We care more about who's doing it and
430.08s - 432.62s |  what systems they're touching because they should not have
432.62s - 436.72s |  access to certain systems within security boundaries and other
436.72s - 442.26s |  places. Where the safety and security meet is in the how. How
442.26s - 446.00s |  these events can happen so that we can mitigate the effects of
446.00s - 452.69s |  those faults and the manipulation. All right? So 356
452.69s - 458.69s |  brings in a new concept of SAL. SAL is not directly tied to DAL
459.29s - 463.19s |  despite what a lot of people like to talk about with this.
463.19s - 466.47s |  But what it is is if you look at that table at the top, table 4
466.47s - 472.07s |  4 which is taken directly out of DO356A, it gives you a threat
472.07s - 476.11s |  condition effect severity which kind of looks a lot like the
476.11s - 481.21s |  advisory circulars with the definitions and gives you SAL
481.21s - 485.05s |  levels for that mitigation. But what's unique here is that
485.05s - 490.39s |  something that has an effect condition of catastrophic all of
490.39s - 494.43s |  a sudden has these two SAL levels. Right? That's kind of
494.43s - 499.73s |  weird. Why would you do two SAL levels? The big part about
499.73s - 503.13s |  security development is that we look at individual threat
503.13s - 506.17s |  vectors as it moves through a system because we have to look
506.17s - 510.07s |  at every point in which it could touch a system and have a final
510.07s - 514.25s |  effect that may or may not have a safety effect on the entire
514.25s - 518.49s |  aircraft. So if we do have an effect on the entire aircraft,
518.49s - 521.99s |  we have to go back through our safety analyses and we have to
521.99s - 526.79s |  place mitigations, two of them in this case, of SAL 3 and SAL
526.79s - 532.23s |  2 in our system to prevent a catastrophic threat condition.
532.23s - 539.11s |  So those mitigations have to be developed with security
539.11s - 544.31s |  objectives, the design objectives for SAL 3 and SAL 2.
544.31s - 549.11s |  So if you're developing a system and if you look at the actual
551.39s - 555.85s |  security assurance level objectives, they look very, very,
555.85s - 561.63s |  very similar to exactly what DO-178 and DO-254 because all
561.63s - 565.27s |  these documents are in concert with one another. They all feed
565.27s - 568.37s |  from one another. They all use a lot of same concepts from one
568.39s - 573.57s |  another. They all try to use the same structures. So that makes
573.57s - 578.17s |  it really easy for us to take our cyber security mitigations
578.17s - 584.31s |  and place them into software or hardware and have the process
584.31s - 590.10s |  look the same for development. Alright? So we're going to do a
590.10s - 594.66s |  little bit of a safety analysis on a system, right? So we have
594.66s - 597.68s |  system A, we have system B, we have system C. And then we have
597.70s - 601.34s |  a user display, right? It could be any sort of function here.
601.34s - 604.34s |  So we do our functional hazardous assessment and it says
604.34s - 610.58s |  that this system has a function that is DAL B. Okay? So we're
610.58s - 615.92s |  looking at a path of DAL B, uh, hazardously misleading data to
615.92s - 619.08s |  a display. And what we're going to do is we're going to apply
619.08s - 622.16s |  safety requirements onto this system. So we add in some
622.16s - 626.40s |  monitors for failures and we'll enunciate those. We check the
626.42s - 630.02s |  validity of the inputs. We check the, uh, at system B and system
630.02s - 633.12s |  C, right? System B is just going to do a really simple operation
633.12s - 636.96s |  of adding something on there. It could be either, uh, changing
636.96s - 643.96s |  data or manipulating data or, um, merging data at some points.
643.96s - 645.94s |  And now what we're going to do is we're going to look at a
645.94s - 649.74s |  threat. We have an attack that modifies code or memory and
649.74s - 655.82s |  changes that to 204. That's pretty bad. Um, don't know how
655.86s - 659.34s |  they managed to do that. But when we look at this system,
659.34s - 661.44s |  there's erroneous data that's now flowing through the system,
661.44s - 665.64s |  right? This is the hazard effect that we're looking at. We're
665.64s - 670.54s |  looking at how the, uh, we can cause hazardously misleading
670.54s - 676.86s |  data DAL B impacts to the system. So, fine, guess what?
676.86s - 678.98s |  We're going to hash the memory spaces so that none of that
678.98s - 682.12s |  changes, right? We're going to put hashes in place so that the
682.12s - 685.56s |  values that we're adding or subtracting or, uh, changing
686.30s - 690.28s |  around, the whole memory spaces are then hashed, right? And
690.28s - 694.08s |  we'll get back to why I picked hashing in the first place.
694.08s - 696.54s |  Safety analysis also said that there has to be a system B
696.54s - 700.82s |  prime, which is a secondary thread to the system so that,
700.82s - 705.16s |  um, system, uh, C merges data from both system B and system B
705.16s - 709.82s |  prime. Kind of looks a lot like what we do for pilot side,
709.82s - 715.06s |  copilot side in aircraft. Oh, there was another attack that
715.06s - 720.16s |  our, our pen testing team came up with. They flooded the
720.16s - 722.90s |  interface. They did a DDoS attack on our system C and
722.90s - 726.38s |  prevented data from going through to the user display.
726.38s - 730.88s |  Crashed the system. Well, I guess we need to go through and
730.88s - 733.72s |  put it together another mitigation. So now we're going
733.72s - 736.58s |  to need some sort of DDoS protection, some sort of rate
736.58s - 741.60s |  limiting in there. It's a pretty secure system so far. Except
744.16s - 748.50s |  they found a third attack, which is actually a mode command that
748.50s - 752.34s |  sent into systems, injected at system C, which then back
752.34s - 756.38s |  propagates through the system to system A, which changes a mode,
756.38s - 760.08s |  which then forces data to come out incorrectly without a pilot
760.08s - 765.28s |  commanding a mode change. So you see how there's kind of a
765.28s - 768.42s |  non-linearity through these systems where you could get
768.42s - 772.30s |  these system effects without having exactly, um, changed or
772.30s - 776.68s |  manipulated or without having commanded, uh, uh, commands be
776.68s - 781.64s |  sent and get unintentional data that was not supposed to be
781.64s - 784.74s |  there without some sort of other checks, right? So a lot of
784.74s - 788.38s |  aircraft systems, it's not just that we press a button and the
788.38s - 791.36s |  mode comes through. You press a button, it goes through, it
791.36s - 794.36s |  sends back bits that says, yes, I saw that you changed the
794.36s - 798.92s |  command of the mode. Here's the new mode information. But what's
798.92s - 802.88s |  unique about this is that that mode command may have also
802.88s - 806.52s |  propagated back through the B prime and A prime systems
806.52s - 811.70s |  through the secondary thread and has also manipulate, uh, changed
811.70s - 817.24s |  those modes, which gets around our cross-checking in system C.
817.24s - 820.80s |  So now a system, uh, so the, um, the primary thread and the
820.80s - 824.64s |  secondary threads are now sending the same data, valid
824.64s - 831.47s |  data, across and it's not being picked up by system C. So what's
831.47s - 834.97s |  interesting about this whole development here is that where
834.97s - 838.25s |  safety and, and security really touch is some of the
838.25s - 843.21s |  implementations that we actually have here. So you take a look at
843.21s - 850.99s |  system B and we applied hashing memory. Our safety analysis also
850.99s - 854.19s |  came back and said that B prime needed to be added as a safety
854.21s - 858.89s |  mitigation on the system. But what does that do for the safe,
858.89s - 863.63s |  or security of the system? Right? Now I have a secondary
863.63s - 867.19s |  thread that I can cross-check data against. An independent
867.19s - 872.27s |  thread that somebody may not have access to that I can ensure
872.27s - 876.21s |  that is part of the whole security, um, posture of this
876.21s - 882.05s |  whole entire system. So hashing memory is a great one, right? We
882.07s - 886.01s |  use it in, uh, cyber security in order to, um, prevent
886.01s - 889.75s |  manipulation, intentional manipulation of memory spaces.
889.75s - 893.79s |  Well, over on the aerospace, or on the safety side, they do that
893.79s - 896.99s |  sort of stuff to prevent accidental manipulations from
896.99s - 901.19s |  like, I don't know, uh, single event upsets from the sun,
901.19s - 906.67s |  right? Neutron events on memory spaces. Um, other areas would be
906.67s - 910.81s |  like duplicating or, uh, triplicating data stored across
910.83s - 913.97s |  the system. So that if you have manipulation in one place, you
913.97s - 916.33s |  can go pick it up in another place, and if that one's still
916.33s - 919.51s |  manipulated, you can go and pick it up somewhere else and
919.51s - 923.27s |  cross-check those three, uh, three different memory spaces.
923.27s - 928.29s |  So safety and security have to kind of be developed together in
930.39s - 938.11s |  these systems. Because you have these, uh, concepts in here of
938.11s - 941.11s |  inherited security principles, or, uh, properties of the
941.13s - 946.87s |  system. Say, uh, security is not the only developer of the
946.87s - 950.87s |  system. Safety's doing it. Implementers of code are doing
950.87s - 953.81s |  this. They're developing their own requirements with how they
953.81s - 957.81s |  either think the system should work, or how the system has been
957.81s - 962.39s |  developed in the past, and how other systems have been, uh,
962.39s - 967.29s |  have been developed. And so, we have to take a look at not just
967.31s - 971.95s |  security requirements, we have to look at the entire system
971.95s - 976.39s |  requirements pool to see which items might be part of the safe,
976.39s - 979.89s |  uh, security posture of the system. The reason why we want
979.89s - 981.99s |  to do that is because we're going to take credit for those
981.99s - 986.67s |  in our threat models at some point. And if you take credit
986.67s - 989.43s |  for them in your threat models, you have to provide the
989.43s - 992.87s |  documentation as part of the design objectives. And if you
992.87s - 995.67s |  have to provide data as part of your design objectives, it's
995.69s - 998.03s |  part of your certification baseline, and you want to take
998.03s - 1001.89s |  that credit for it. It all comes back down to the standards and
1001.89s - 1008.32s |  the certification of these systems. Another important, uh,
1008.32s - 1014.26s |  piece in here is that you can also have system effects where
1014.26s - 1018.76s |  safety and security may interact with each other in a negative
1018.76s - 1023.30s |  manner. Where one may trigger, you might want to, you know,
1023.32s - 1027.28s |  trigger a safety event, or you have an event that triggers some
1027.28s - 1032.10s |  safety safeguards that have been in place, and the security of the
1032.10s - 1034.76s |  system is reduced because systems have now been reset and
1034.76s - 1040.14s |  they lost state. You might have security implementations in
1040.14s - 1044.18s |  there, where it prevents data from cross flowing from one side
1044.18s - 1049.04s |  to another, or, um, you might have other pieces where data's
1049.04s - 1052.02s |  unavailable because it's been encrypted, that affects the
1052.04s - 1056.90s |  safety of the system. So it's very important to look at how
1056.90s - 1060.92s |  safety and security interplay in the requirements and determine,
1060.92s - 1064.48s |  like, is this a positive or a negative, um, part of our
1064.48s - 1069.48s |  system. So, um, in summary, aircraft cybersecurity is very
1071.72s - 1076.72s |  young. DO- uh, DO-356 was first released in about, in, uh, 2014.
1077.20s - 1084.64s |  In comparison to the advisory circular that I showed you at
1084.64s - 1089.64s |  first, AC, uh, 25-13-07, that was released in 88. We've had
1092.58s - 1098.94s |  these de- these definitions around in, um, in aviation for
1098.94s - 1105.36s |  almost 40 years. 40 years of development time to learn how to
1105.36s - 1109.24s |  do all these development principles with how to develop
1109.24s - 1113.58s |  safe or safe code, how to device, uh, develop safe
1113.58s - 1117.98s |  hardware, has been going on for a very long time. But the
1117.98s - 1120.38s |  security of these systems, because they're becoming more
1120.38s - 1125.28s |  and more connected and, um, even more highly connected in real
1125.28s - 1130.28s |  time in the future, these systems need better security
1130.28s - 1134.20s |  principles that work with how the systems are developed in
1134.20s - 1142.19s |  order to provide certification boundaries. To touch upon the
1142.19s - 1145.73s |  technological limitations, prior systems in the past used to have
1145.73s - 1150.43s |  federated systems where you had multiple single purpose systems
1150.43s - 1154.93s |  on board, which then moved into more highly integrated systems,
1154.93s - 1157.61s |  which then moved into more highly, uh, integrated
1157.61s - 1162.47s |  networking systems as well. So those boundaries of, um, data
1162.47s - 1167.17s |  that you couldn't get security issues across because of the
1167.17s - 1171.07s |  highly federated nature of these systems is now being whittled
1171.07s - 1173.81s |  away at as part of the development because of the
1173.81s - 1178.95s |  demands of higher speed buses. Airing 429 is not fast at all,
1178.95s - 1184.29s |  but Ethernet is, and we can do that deterministically. The way
1184.29s - 1188.73s |  that security, um, brings those direct impacts has to be
1188.75s - 1192.53s |  developed as part of the architecture because in the end,
1192.53s - 1196.53s |  you can't have a safe system with also having a cyber secure
1196.53s - 1198.03s |  system. Thank you.