{
  "webpage_url": "https://www.youtube.com/watch?v=BIjgd2mOEks",
  "title": "DEF CON 32 - Human Dignity in AI and Tech Policy  - Jan Trzaskowski",
  "description": "Social media have been a decade-long dress rehearsal in online manipulation. AI can create information, make predictions and take decisions that will affect human behaviour, including our behaviours as citizens, workers and consumers. Safeguards are needed, since generative AI will only exacerbate the personal, social and societal harms already caused by data-driven business models.\n\nWe examine the centrality of human dignity in tech law and policy and how our mindsets and legal frameworks must be informed by psychological, technological and societal perspectives. Based on insights from market dynamics, marketing techniques, design strategies, and human frailties we demonstrate how information asymmetries have reduced individual agency and the ability to create transparency.\n\nHuman dignity is a core value in liberal democracies that must also be reflected in tech policy. Protections are required when businesses interfere with our rights to freedom, property, privacy and non-discrimination. With the digitalisation of the human experience, users have become programmable objects. We cannot rely on regulation alone and need to discuss how we can act to reclaim our dignity.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2263,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 4.00s | This text was transcribed using whisper model: large-v2

 Thank you so much for sharing your attention with me today.
4.00s - 8.00s |  The technology is not really working, but I'll explain what's on the slides,
8.00s - 11.00s |  and you can download them if they don't get up here.
11.00s - 15.00s |  So, I'm a law professor at Copenhagen Business School and Aalborg University,
15.00s - 20.00s |  and I'm here to talk about hacking human behavior.
20.00s - 27.00s |  I will talk about big tech arrogance, human ignorance,
27.00s - 32.00s |  what human dignity can be used for, what to do about it,
32.00s - 34.00s |  so that's what I'm going to talk about.
34.00s - 37.00s |  Everything is based on the book you can see on the screen there.
37.00s - 43.00s |  It's an orange book. I'll leave a copy at the Policy Village for you to see.
43.00s - 47.00s |  So, the good thing is it's available online, and you can find the link on the slides
47.00s - 50.00s |  whenever you get there.
50.00s - 55.00s |  Okay, so you're there, so you have the link here, everything is working.
55.00s - 60.00s |  So, first of all, what we're talking about here is also artificial intelligence,
60.00s - 65.00s |  and if you look at the history of technology law, in the mid-90s,
65.00s - 70.00s |  there was a lot of focus on copyright, because what was being digitalized
70.00s - 74.00s |  back in the 90s was music, films, and so on.
74.00s - 77.00s |  What happened next was that we got social media,
77.00s - 81.00s |  and what happened with social media was that it was the human experience
81.00s - 84.00s |  that got digitalized.
84.00s - 90.00s |  And then the interest revolved around privacy, personal data.
90.00s - 94.00s |  And now with the latest hype of artificial intelligence,
94.00s - 102.00s |  we see that the focus in law is on copyright and privacy once again,
102.00s - 105.00s |  so these two areas, so that's curious.
105.00s - 110.00s |  So, the conclusions, to start with them, Norbert Wiener said it very nice,
110.00s - 116.00s |  more than 75 years ago, that what we have with artificial intelligence
116.00s - 120.00s |  or thinking machines, as he talked about, was akin to the atomic bomb,
120.00s - 123.00s |  he compared to the atomic bomb back then.
123.00s - 126.00s |  What he also said was that when we have technology, it's going to be used,
126.00s - 130.00s |  so it's not going to go away, so that is also something we can rely on.
130.00s - 134.00s |  And the last thing, which is the most important thing, is that the solution
134.00s - 138.00s |  is to think about something else than just buying and selling.
138.00s - 143.00s |  Market economy, but think about humanity instead of that.
143.00s - 147.00s |  And what is also interesting is that this book that came out,
147.00s - 152.00s |  Norbert Wiener came out in 1948, which was also the year that we got
152.00s - 157.00s |  the UN Declaration of Human Rights, that speaks about human dignity.
157.00s - 161.00s |  So, this is an old story being told again.
161.00s - 166.00s |  So, the first thing we should recognize is that when we are talking about
166.00s - 170.00s |  data and AI, they don't have any intrinsic value.
170.00s - 174.00s |  I can have full data sets of all of you on a USB stick,
174.00s - 179.00s |  and it doesn't have any value before I start using it.
179.00s - 184.00s |  So, what we should be curious about is, what is the technology being used for?
184.00s - 187.00s |  That is why my focus has been on data-driven business models
187.00s - 189.00s |  rather than the technology.
189.00s - 192.00s |  And I would argue that what we see with artificial intelligence
192.00s - 195.00s |  is just the same as what we've seen with data-driven business models.
195.00s - 198.00s |  So, it's been a dress rehearsal for more than a decade
198.00s - 202.00s |  that we have used technology for manipulation,
202.00s - 207.00s |  and it's just been exacerbated by new technology in AI.
207.00s - 211.00s |  So, what we should be curious about is, what incentives are there?
211.00s - 214.00s |  And when it comes to personalized marketing,
214.00s - 216.00s |  there are basically three incentives that businesses have.
216.00s - 219.00s |  They want to earn money, but what correlates with earning money,
219.00s - 221.00s |  making profits?
221.00s - 223.00s |  That is, getting as many users as possible.
223.00s - 225.00s |  That's why you want to have the network effect.
225.00s - 228.00s |  So, you're going to be the winner that takes it all.
228.00s - 231.00s |  You also want to have more attention.
231.00s - 234.00s |  So, the more addictive you can make your platform,
234.00s - 236.00s |  the more attention you will get,
236.00s - 238.00s |  and the more advertising dollars you can get.
238.00s - 240.00s |  And the last thing what you want to have
240.00s - 243.00s |  is as much data as possible of each individual,
243.00s - 246.00s |  because the more you know of each individual,
246.00s - 249.00s |  the higher price you can charge for the advertising.
250.00s - 253.00s |  And what is new with generative AI is basically
253.00s - 256.00s |  that the businesses also have an interest
256.00s - 258.00s |  in getting as much content as possible.
258.00s - 261.00s |  That is why we see social media platforms
261.00s - 266.00s |  trying to use our content to train their AI models on.
266.00s - 271.00s |  That is part of what we can talk about
271.00s - 275.00s |  when we say big tech arrogance, but I'll come back to that.
276.00s - 278.00s |  Yes.
278.00s - 281.00s |  So, in liberal democracies and in markets,
281.00s - 284.00s |  we have the idea of empowerment.
284.00s - 286.00s |  So, as citizens, we should be empowered
286.00s - 289.00s |  to be part of society.
289.00s - 292.00s |  In markets, we should be able to make rational choices
292.00s - 294.00s |  following our goals, values, and preferences
294.00s - 296.00s |  that may be different from each of us,
296.00s - 298.00s |  but we should be free individuals.
298.00s - 300.00s |  We should be empowered.
300.00s - 303.00s |  And that is what I will challenge in this talk to begin with.
303.00s - 306.00s |  So, what I came up to in my research is that
306.00s - 308.00s |  there are three things that need to be in place
308.00s - 311.00s |  in order to talk meaningfully about empowerment.
311.00s - 313.00s |  First, we need agency.
313.00s - 316.00s |  Some capability to understand and act in the world
316.00s - 318.00s |  that we are living in.
318.00s - 321.00s |  We need to have some sort of transparency.
321.00s - 324.00s |  And then we, of course, need the absence of manipulation,
324.00s - 327.00s |  because manipulation is the opposite of freedom.
327.00s - 329.00s |  And if we start with agency,
329.00s - 331.00s |  we can say we know from psychology
331.00s - 333.00s |  that we are bounded rationally.
333.00s - 335.00s |  That means that we are not completely rational
335.00s - 337.00s |  in whatever we do.
337.00s - 340.00s |  And there's been a lot of research in how we are irrational
340.00s - 343.00s |  that can be used against us.
343.00s - 345.00s |  When it comes to transparency,
345.00s - 347.00s |  the only thing I will say now is that
347.00s - 350.00s |  information and transparency is not the same.
350.00s - 352.00s |  I'll come back to transparency soon,
352.00s - 354.00s |  because that is key to what we are talking about.
354.00s - 357.00s |  The last thing, when we talk about absence of manipulation,
357.00s - 360.00s |  I've borrowed a definition from Cass Sunstein
360.00s - 363.00s |  when speaking about whether you're sufficiently engaging
363.00s - 365.00s |  or appealing to the user's capacity
365.00s - 367.00s |  for reflection and deliberation.
367.00s - 370.00s |  Just think about you get a cookie consent pop-up
370.00s - 373.00s |  and you get the choice between managed preferences
373.00s - 375.00s |  and accept all.
375.00s - 377.00s |  You would, of course, accept all,
377.00s - 380.00s |  because there's less friction.
380.00s - 383.00s |  So you could do a lot with design,
383.00s - 385.00s |  and there's a lot of focus on design
385.00s - 387.00s |  both in law and in technology.
387.00s - 391.00s |  How do we design the reality we enter online?
391.00s - 394.00s |  So this slide I've taken here
394.00s - 398.00s |  just to signal or give you some insights
398.00s - 402.00s |  into the toolbox of advertisers.
402.00s - 407.00s |  These levels of influence are compiled by Sheldini,
407.00s - 410.00s |  who is a professor in marketing.
410.00s - 412.00s |  And this is just to illustrate
412.00s - 415.00s |  how easy it is to manipulate us.
415.00s - 418.00s |  So if you take reciprocation, for instance,
418.00s - 421.00s |  if we get something from a company or from a person,
421.00s - 424.00s |  we feel obliged toward that person.
424.00s - 428.00s |  So if you get a free Google Maps, that's great.
428.00s - 431.00s |  You feel you're obliged to do something for Google.
431.00s - 433.00s |  If you take scarcity,
433.00s - 436.00s |  three other people are looking at this limited offer right now.
436.00s - 438.00s |  Oh, we better buy it right now.
438.00s - 440.00s |  So scarcity works.
440.00s - 442.00s |  This is written down in books,
442.00s - 445.00s |  so it's easy, accessible, but nobody reads it.
445.00s - 447.00s |  So when we talk about markets,
447.00s - 449.00s |  we want to make the distinction
449.00s - 451.00s |  between persuasion and manipulation
451.00s - 453.00s |  because in a market economy,
453.00s - 455.00s |  we need to have marketing,
455.00s - 459.00s |  and we need to be able to persuade consumers,
459.00s - 461.00s |  but we should not manipulate them.
461.00s - 464.00s |  And that is what I've spent almost 30 years of my life
464.00s - 466.00s |  finding out where is the fine line
466.00s - 468.00s |  between persuasion and manipulation.
468.00s - 471.47s |  When it comes to technology,
471.47s - 475.47s |  I should just add for behavior modification for marketing,
475.47s - 480.47s |  the first book I've come across on scientific advertising
480.47s - 482.47s |  is more than 100 years old.
482.47s - 485.47s |  So this is an old discipline in manipulating people.
485.47s - 487.47s |  It's definitely not new.
487.47s - 489.47s |  A newer discipline is
489.47s - 492.47s |  how can you use technology to persuade people?
492.47s - 495.47s |  Some of you might be familiar with B.J. Fock,
495.47s - 499.47s |  who wrote a book on persuasive technology or captology.
499.47s - 501.47s |  How can you use computers
501.47s - 505.47s |  to persuade people to buy stuff and do stuff?
505.47s - 507.47s |  And what I'll just conclude here
507.47s - 509.47s |  is that design is extraordinarily powerful
509.47s - 512.47s |  in how we can manipulate people.
512.47s - 514.47s |  One of the things he mentioned
514.47s - 518.47s |  is that computers can go where humans cannot go.
518.47s - 520.47s |  So I would assume that most of you
520.47s - 523.47s |  will have Amazon, Google, Facebook, etc.
523.47s - 525.47s |  with you everywhere you go,
525.47s - 527.47s |  in your bedroom, in your bathroom, and so on.
527.47s - 530.47s |  So that is definitely a testament to the fact
530.47s - 534.47s |  that you can go where nobody else can go.
534.47s - 536.47s |  Also, another important thing,
536.47s - 538.47s |  if you look into psychology,
538.47s - 542.47s |  if you want to conclude on bounded rationality,
542.47s - 544.47s |  the fact that we're not completely rational,
544.47s - 546.47s |  the reason why we're not completely rational
546.47s - 548.47s |  is because we are more emotional.
548.47s - 551.47s |  A lot of the choices we make are emotional,
551.47s - 553.47s |  and then we rationalize them afterwards.
553.47s - 555.47s |  And the interesting thing about computers
555.47s - 558.47s |  is that it can actually evoke feelings.
558.47s - 561.47s |  So when you open your computer and you get a smiley,
561.47s - 563.47s |  you feel better with yourself.
563.47s - 566.47s |  That's how easy we are to persuade and manipulate.
566.47s - 569.47s |  And if you fuse BJ Fogg with Cialdini,
569.47s - 571.47s |  we get online influence.
571.47s - 573.47s |  We get Cialdini on steroids
573.47s - 576.47s |  because you can actually use the technology
576.47s - 579.47s |  to apply the levels of influence.
579.47s - 581.47s |  The good thing about being in academia
581.47s - 583.47s |  is that you decide yourselves which books to read.
583.47s - 585.47s |  And I thought it would be interesting
585.47s - 587.47s |  to read books on magic.
587.47s - 589.47s |  And it actually turns out that
589.47s - 591.47s |  the two foundations of magic
591.47s - 593.47s |  is misdirection
593.47s - 596.47s |  and the impression of having a choice.
596.47s - 598.47s |  And that's what's going on here.
598.47s - 600.47s |  That's why we could say it's kind of magic.
600.47s - 603.47s |  So where does that all lead?
603.47s - 606.47s |  It leads to information asymmetries.
606.47s - 607.47s |  This is not new.
607.47s - 609.47s |  What I've done in my book is
609.47s - 613.47s |  I developed the model of information asymmetry.
613.47s - 617.47s |  So Akerlof wrote in 1970 an article
617.47s - 620.47s |  about information asymmetry.
620.47s - 623.47s |  That is what I call Tier 1 information asymmetry.
623.47s - 625.47s |  The fact that the seller knows much more
625.47s - 628.47s |  about him or herself than the buyer
628.47s - 630.47s |  and also about the product.
630.47s - 632.47s |  So if I'm going to sell my used car to you,
632.47s - 634.47s |  I know everything about it, you know nothing.
634.47s - 637.47s |  In law, we solve that with information.
637.47s - 639.47s |  I have a duty to inform you about
639.47s - 641.47s |  the condition of the car.
641.47s - 644.47s |  And if I fail to do that, you can come after me.
644.47s - 648.47s |  So that is the Tier 1 information asymmetry.
648.47s - 650.47s |  The Tier 2 information asymmetry
650.47s - 653.47s |  is what the business know about how to persuade us.
653.47s - 656.47s |  How to persuade people in general.
656.47s - 658.47s |  That is Sheldini's technology.
658.47s - 660.47s |  That is PJ Fox's technology
660.47s - 663.47s |  on the techniques about how to influence people
663.47s - 666.47s |  and using computers to do that.
666.47s - 668.47s |  So that is Tier 2.
668.47s - 672.47s |  Tier 3 is where we get information of each individual.
672.47s - 674.47s |  That is where personal data come in.
674.47s - 676.47s |  So if I don't see you as a whole crowd
676.47s - 679.47s |  but see you as individuals
679.47s - 682.47s |  and I have in my records
682.47s - 686.47s |  thousands, hundreds of thousands, maybe millions of users.
686.47s - 688.47s |  You can go into my shop
688.47s - 690.47s |  and you can click a little bit around
690.47s - 694.47s |  and I can easily find a group of people that match you.
694.47s - 696.47s |  Meaning I can start creating
696.47s - 699.47s |  an artificial intelligence model of you.
699.47s - 702.47s |  So just a few clicks, I have a model of you
702.47s - 706.47s |  and I can use that model to test how to better persuade you.
706.47s - 709.47s |  So that is Tier 3 information asymmetry
709.47s - 712.47s |  and that is why we should care about personal data, of course.
712.47s - 714.47s |  One of the things we do with that
714.47s - 716.47s |  is we create personalized realities.
716.47s - 718.47s |  That means that my reality may be different
718.47s - 720.47s |  from each of yours reality.
720.47s - 723.47s |  That means that it is difficult to compare
723.47s - 726.47s |  how we are being persuaded or manipulated.
726.47s - 729.47s |  So how do we level that information asymmetry?
729.47s - 732.47s |  Well, the first thing we should recognize, as I said earlier,
732.47s - 735.47s |  information and transparency are two different things.
735.47s - 737.47s |  So you use information,
737.47s - 741.47s |  you communicate information to create transparency.
741.47s - 745.47s |  And regulators have preferred to use information
745.47s - 747.47s |  as the regime to regulate markets
747.47s - 750.47s |  because it is almost free to do.
751.47s - 755.47s |  The problem is that if I'm obliged as a seller
755.47s - 757.47s |  to give you information,
757.47s - 761.47s |  your obligation will be to decode the information I've given to you.
761.47s - 764.47s |  That means I put an extra burden on you.
764.47s - 766.47s |  And we already have cognitive overload
766.47s - 768.47s |  when we are on the Internet.
768.47s - 770.47s |  So more information is not going to help us.
770.47s - 773.47s |  So that is one thing we should realize about that.
773.47s - 776.47s |  We should also realize that there's a lot of theory
776.47s - 778.47s |  about frames and storytelling.
778.47s - 780.47s |  Storytelling is how we understand the world.
780.47s - 782.47s |  We understand the world in stories.
782.47s - 784.47s |  So we tell each other stories
784.47s - 787.47s |  and we are being told stories by businesses.
787.47s - 790.47s |  And they are much better at making up stories
790.47s - 792.47s |  than we are ourselves.
792.47s - 797.47s |  One of the things or more misleading stories that are out there
797.47s - 799.47s |  is that we are paying with personal data.
799.47s - 802.47s |  I would argue that that is a misleading story.
802.47s - 805.47s |  It's a misleading framing of the situation,
805.47s - 807.47s |  which I will come back to.
807.47s - 813.47s |  So what we need to, as consumers, as regulators, and so on,
813.47s - 815.47s |  is we need to understand the deal.
815.47s - 818.47s |  So Article 22 of the GDPR
818.47s - 820.47s |  speaks about automated decision-making.
820.47s - 823.47s |  So that is the closest we get to regulation
823.47s - 826.47s |  of applied artificial intelligence.
826.47s - 828.47s |  And there the requirement is that you should
828.47s - 830.47s |  explain the data subject about the logic,
830.47s - 833.47s |  the significance, and envisage consequences.
833.47s - 835.47s |  In my book, that is a gold standard
835.47s - 838.47s |  for how you should inform people.
838.47s - 841.47s |  Neil Postman, one of my heroes,
841.47s - 843.47s |  wrote in the 80s and the 90s
843.47s - 846.47s |  about technology and media.
846.47s - 848.47s |  And what he said about technology
848.47s - 851.47s |  is that everything we should know about technology
851.47s - 853.47s |  is everything we should know about cars.
853.47s - 856.47s |  It is not how we use the cars or the technology.
856.47s - 860.47s |  It is how the technology or the cars are using us.
860.47s - 862.47s |  That is what we should be curious about.
862.47s - 864.47s |  Because that is where we can understand
864.47s - 867.47s |  the price that we are paying when we get something for free.
869.47s - 873.47s |  So what we need to understand is externalities.
873.47s - 877.47s |  So externalities, if you imagine you have a factory making shoes
877.47s - 880.47s |  and you have some waste from the shoe production
880.47s - 882.47s |  and you just throw it in the river,
882.47s - 884.47s |  that is an externality.
884.47s - 887.47s |  The business will sell the shoes and get the money from them.
887.47s - 890.47s |  But the rest of us will have to clean up
890.47s - 893.47s |  of the disaster they made in the river.
893.47s - 895.47s |  That is an externality.
895.47s - 897.47s |  Think about big tech companies.
897.47s - 901.47s |  How much damage, how much harm they are making to society
901.47s - 904.47s |  that we have to clean up after them.
904.47s - 907.47s |  So I think it makes sense to talk about externalities.
907.47s - 910.47s |  So what are the harms?
910.47s - 912.47s |  Well, first of all, you can say that
912.47s - 917.47s |  the whole idea is of hacking or hijacking the human experience.
917.47s - 921.47s |  And a lot of people talk about personalized marketing.
921.47s - 924.47s |  What is the problem that I am going to buy something I don't need?
924.47s - 926.47s |  We do that all the time.
926.47s - 928.47s |  Well, it might not be such a big harm.
928.47s - 930.47s |  Departing with your money,
930.47s - 932.47s |  depending on how rich or how poor you are,
932.47s - 934.47s |  of course, it makes a difference.
934.47s - 936.47s |  But what we should also care about
936.47s - 938.47s |  is that it is not only commercial marketing we are talking about.
938.47s - 940.47s |  We are talking about political marketing.
940.47s - 942.47s |  We are talking about religious marketing.
942.47s - 945.47s |  And that is where it gets a bit sketchy.
945.47s - 949.47s |  We should also understand that we are being affected by technology,
949.47s - 951.47s |  including artificial intelligence,
951.47s - 955.47s |  as citizens, as workers, and as consumers.
955.47s - 958.47s |  So every aspect of our life is being influenced
958.47s - 963.47s |  by how technology around us is being designed and sold to us.
964.47s - 968.47s |  So in the book, I divide the harms into three categories.
968.47s - 970.47s |  I talk about personal harms,
970.47s - 973.47s |  including people getting more lonely,
973.47s - 975.47s |  which correlates with having bad health.
975.47s - 977.47s |  We don't want people to be lonely.
977.47s - 979.47s |  We are social animals.
979.47s - 983.47s |  We also get worse at reading and understanding our attention span
983.47s - 985.47s |  due to the overload.
985.47s - 988.47s |  And I can see that with my students.
988.47s - 993.33s |  We are less able to communicate with other people
993.33s - 997.33s |  because we have an escape route for social interaction.
997.33s - 999.33s |  Social interaction is hard.
999.33s - 1001.33s |  There is a lot of friction.
1001.33s - 1004.33s |  But if you can go and look at cat videos on TikTok or YouTube,
1004.33s - 1006.33s |  that is easy.
1006.33s - 1007.33s |  That is frictionless.
1007.33s - 1009.33s |  And that is why we will often go there.
1009.33s - 1013.33s |  But that means also that we will not train our capabilities
1013.33s - 1015.33s |  of engaging with other people.
1015.33s - 1019.33s |  And that, of course, has consequences for democracy.
1019.33s - 1023.33s |  Because if we are not active in markets and democracy,
1023.33s - 1027.33s |  you know, it's going to hurt ourselves eventually.
1027.33s - 1031.12s |  So data-driven business models,
1031.12s - 1033.12s |  including those that use artificial intelligence,
1033.12s - 1035.12s |  whatever that means,
1035.12s - 1037.12s |  that affects markets.
1037.12s - 1040.12s |  Our ability to choose between different products.
1040.12s - 1045.12s |  New natural monopoly due to network effects and so on.
1045.12s - 1048.12s |  It also affects democracy, as I mentioned.
1048.12s - 1052.12s |  And more important nowadays, geopolitics.
1052.12s - 1054.12s |  We have a huge debate about TikTok.
1054.12s - 1057.12s |  Can we use TikTok because they're based in China?
1057.12s - 1062.12s |  And in my view, the problem with TikTok or similar providers
1062.12s - 1065.12s |  are not so much that they are looking into our data,
1065.12s - 1067.12s |  but the fact that they are actually making
1067.12s - 1069.12s |  all our children look at cat videos
1069.12s - 1073.12s |  instead of training their social engagement with each other.
1073.12s - 1077.12s |  I think that is the big overlooked danger in all this.
1077.12s - 1080.12s |  And then, of course, climate.
1080.12s - 1082.12s |  I mean, first of all, we need to understand
1082.12s - 1084.12s |  and realize the consequences of what we've been doing
1084.12s - 1086.12s |  for the last 100, 150 years.
1086.12s - 1088.12s |  What's going on there?
1088.12s - 1091.12s |  That means that we can think critically.
1091.12s - 1094.12s |  And if you can't think critically, we can just accept things.
1094.12s - 1097.12s |  Oh, you know, everything is fine.
1097.12s - 1099.12s |  But we also should realize,
1099.12s - 1101.12s |  especially when it comes to generative AI,
1101.12s - 1105.12s |  that the fact that we are talking about a bubble
1105.12s - 1107.12s |  when it comes to generative AI
1107.12s - 1109.12s |  is because they are running out of content
1109.12s - 1111.12s |  to train their models on.
1111.12s - 1116.12s |  But also the power grid doesn't have the proportions
1116.12s - 1119.12s |  to feed all the energy that is used for it.
1119.12s - 1121.12s |  And it's not going to happen in the next few years.
1121.12s - 1123.12s |  So should we have a discussion
1123.12s - 1126.12s |  about what our energy should be used for?
1126.12s - 1128.12s |  Or should we just have a market saying,
1128.12s - 1130.12s |  those that pay the most?
1130.12s - 1133.12s |  And I could imagine that is maybe on the West Coast somewhere
1133.12s - 1136.12s |  that they will have the most money to buy all the energy.
1136.12s - 1138.12s |  But how will that affect the rest of us?
1138.12s - 1140.12s |  Again, think about externality.
1140.12s - 1142.12s |  Who's going to clean up this thing?
1142.12s - 1144.12s |  And there we should also be aware
1144.12s - 1146.12s |  when we talk about climate,
1147.12s - 1150.12s |  there are people in other places
1150.12s - 1155.12s |  that will feel the harms more than we will, maybe.
1155.12s - 1158.12s |  But it's also our children
1158.12s - 1160.12s |  that will have to pay the price and their children.
1160.12s - 1162.12s |  We should also be aware of that.
1162.12s - 1165.12s |  That is part of rational choice
1165.12s - 1167.12s |  that is thinking about future generations,
1167.12s - 1169.12s |  which is very complicated.
1169.12s - 1171.12s |  And that is why it's much easier to just buy
1171.12s - 1174.12s |  what we think we need or feel what we need
1174.12s - 1177.12s |  and just rationalize it afterwards.
1177.12s - 1181.50s |  The whole idea of data-driven business models
1181.50s - 1184.50s |  is that you're basically, as such a company,
1184.50s - 1188.50s |  you're looking at humans as programmable objects.
1188.50s - 1191.50s |  So that is how we are seen.
1191.50s - 1193.50s |  You know, Mark Zuckerberg, Elon Musk and so on
1193.50s - 1195.50s |  don't care about their users.
1195.50s - 1197.50s |  They care about money,
1197.50s - 1199.50s |  which is completely legitimate in a market economy,
1199.50s - 1201.50s |  profit maximization.
1201.50s - 1203.50s |  They don't care about you.
1203.50s - 1205.50s |  So they will do whatever they can
1205.50s - 1207.50s |  as long as it's profitable,
1207.50s - 1209.50s |  including manipulate you to buy stuff
1209.50s - 1211.50s |  to stay longer on their platform
1211.50s - 1214.50s |  so they can sell more advertising and so on.
1214.50s - 1217.50s |  So you could make the argument
1217.50s - 1220.50s |  that by using technology the way we do,
1220.50s - 1223.50s |  we have been domesticated by technology.
1223.50s - 1228.50s |  We talk about augmented reality
1228.50s - 1231.50s |  and we talk about virtual realities,
1231.50s - 1234.50s |  but I think we should talk about abated realities.
1234.50s - 1237.50s |  So our real reality is being abated
1237.50s - 1239.50s |  by using technology.
1239.50s - 1242.50s |  And just to quote Postman once again,
1242.50s - 1246.50s |  to be against technology is like being against food.
1246.50s - 1248.50s |  It doesn't make sense.
1248.50s - 1251.50s |  So I'm not saying you shouldn't use technology.
1251.50s - 1253.50s |  I'm just arguing that we should be more conscious
1253.50s - 1255.50s |  about how we use it,
1255.50s - 1258.50s |  recognizing how it's being used against us.
1258.50s - 1262.50s |  So when I say we are not paying with personal data,
1262.50s - 1264.50s |  which we are, we are paying with personal data,
1264.50s - 1267.50s |  but we are also paying with attention.
1267.50s - 1270.50s |  We are paying with our agency,
1270.50s - 1272.50s |  our ability to make rational choices.
1272.50s - 1277.50s |  And attention and agency are scarce resources.
1277.50s - 1279.50s |  So when we talk about personal data,
1279.50s - 1281.50s |  you can take the same personal data
1281.50s - 1282.50s |  and get a lot of stuff for free.
1282.50s - 1284.50s |  I'm the clever guy, right?
1284.50s - 1286.50s |  I get all these things for free
1286.50s - 1288.50s |  and I use the same data.
1288.50s - 1290.50s |  But again, you should be curious
1290.50s - 1292.50s |  about what the data is being used for.
1292.50s - 1296.50s |  And if it's being used to make us addicted to technologies,
1296.50s - 1298.50s |  we are paying with attention.
1298.50s - 1299.50s |  When we are being manipulated,
1299.50s - 1301.50s |  we are paying with agency,
1301.50s - 1305.50s |  the scarce resources that we need to lead good lives.
1305.50s - 1309.06s |  So the solution I come up with is
1309.06s - 1311.06s |  that we should focus more on human dignity.
1311.06s - 1316.06s |  So we should design policy, business, and so on
1316.06s - 1318.06s |  based on human dignity.
1318.06s - 1320.06s |  Because human dignity is basically the idea
1320.06s - 1326.06s |  of not reducing the individual to an object.
1326.06s - 1328.06s |  And that's exactly what's going on here.
1328.06s - 1330.06s |  So what is human dignity?
1330.06s - 1332.06s |  Well, human dignity has a history
1332.06s - 1335.06s |  in philosophy and religion and so on.
1335.06s - 1340.06s |  But with the declaration of the fundamental rights
1340.06s - 1342.06s |  from the United Nations,
1342.06s - 1344.06s |  it was actually put into a law.
1344.06s - 1346.06s |  It's a legal value now.
1346.06s - 1349.06s |  And I suggest in my book that we see
1349.06s - 1351.06s |  in a liberal democracy,
1351.06s - 1355.06s |  we see our constitutions on fundamental human rights.
1355.06s - 1359.06s |  We see that as the operating system of our society.
1359.06s - 1361.06s |  And the reason why I use the metaphor
1361.06s - 1363.06s |  of an operating system
1363.06s - 1365.06s |  is because it's not a given.
1365.06s - 1366.06s |  You know better than I do
1366.06s - 1369.06s |  how you can change the operating system of a computer.
1369.06s - 1373.06s |  Just like we can change the operating system of a society.
1374.06s - 1376.06s |  Which some people would like.
1376.06s - 1382.44s |  When we see fundamental rights constitutions
1382.44s - 1384.44s |  as the operating system,
1384.44s - 1387.44s |  we can see secondary law legislation
1387.44s - 1389.44s |  as the application layer.
1389.44s - 1391.44s |  That means that we should understand everything
1391.44s - 1392.44s |  on the application layer
1392.44s - 1397.44s |  in light of the operating system of human dignity.
1397.44s - 1400.44s |  So we should design everything for human dignity
1400.44s - 1402.44s |  and have that in the back of our mind.
1402.44s - 1404.44s |  So what is human dignity?
1404.44s - 1408.44s |  So in the book I work with freedom.
1408.44s - 1412.44s |  So the idea that we should be free individuals having agency.
1412.44s - 1414.44s |  We should have not only the right
1414.44s - 1417.44s |  but also the ability to think critically.
1417.44s - 1420.44s |  We should make sure that
1420.44s - 1422.44s |  those people providing the software
1422.44s - 1424.44s |  or tools, apps to us,
1424.44s - 1426.44s |  that to some extent
1426.44s - 1430.44s |  that they align their products with our goals.
1431.44s - 1434.23s |  We should also make sure that
1434.23s - 1436.23s |  if you're using for instance AI
1436.23s - 1438.23s |  that should be voluntary.
1438.23s - 1442.23s |  Just think about when we got My AI in Snapchat.
1442.23s - 1444.23s |  Well that was to children.
1444.23s - 1447.23s |  Here, dear children, here is a chatbot.
1447.23s - 1449.23s |  That means that you can spend more time
1449.23s - 1451.23s |  and give us more data by using that.
1451.23s - 1453.23s |  Think about the incentives.
1453.23s - 1456.23s |  We should think about privacy.
1456.23s - 1459.23s |  Personal data and security.
1460.23s - 1462.23s |  And I would say one of the most successful
1462.23s - 1464.23s |  pieces of legislation from the EU
1464.23s - 1466.23s |  must be the GDPR.
1466.23s - 1468.23s |  And I hope you will get something akin to that
1468.23s - 1470.23s |  here in the US as well.
1470.23s - 1473.23s |  But just remember that we have
1473.23s - 1476.23s |  the UN principles on human rights as well
1476.23s - 1478.23s |  that protects privacy.
1478.23s - 1480.23s |  One thing I would emphasize here
1480.23s - 1482.23s |  when we talk about privacy and personal data
1482.23s - 1484.23s |  because that's basically two different things.
1484.23s - 1487.23s |  So personal data is information about
1487.23s - 1488.23s |  each individual.
1488.23s - 1490.23s |  That's about individuals.
1490.23s - 1492.23s |  And what we see when we are looking at
1492.23s - 1494.23s |  artificial intelligence,
1494.23s - 1496.23s |  especially generative AI,
1496.23s - 1499.23s |  you're building large language models
1499.23s - 1501.23s |  based on personal data.
1501.23s - 1504.23s |  But when you create the large language model
1504.23s - 1507.23s |  suddenly you're not dealing with personal data.
1507.23s - 1510.23s |  But does that mean that you can just do that?
1510.23s - 1512.23s |  And I don't think we can do that.
1512.23s - 1515.23s |  Because if I have a big model
1515.23s - 1518.23s |  of how consumers, how citizens
1518.23s - 1523.23s |  are reacting to certain things,
1523.23s - 1526.23s |  how you have correlations between certain things,
1526.23s - 1530.23s |  you can actually use that to invade people's privacy.
1530.23s - 1532.23s |  And that is why I think in the future,
1532.23s - 1533.23s |  in the near future,
1533.23s - 1536.23s |  we will talk less about personal data
1536.23s - 1539.23s |  and much more about privacy in general.
1539.23s - 1541.23s |  And just to illustrate this,
1541.23s - 1543.23s |  we have five or six court cases
1543.23s - 1547.23s |  from the Court of Justice of the European Union
1547.23s - 1549.23s |  dealing with data retention.
1549.23s - 1552.23s |  So in essence, they were about
1552.23s - 1555.23s |  police for fighting serious crime
1555.23s - 1558.23s |  wanting to get access to metadata
1558.23s - 1561.23s |  from the telecom operators.
1561.23s - 1563.23s |  And what the Court of Justice said,
1563.23s - 1565.23s |  that is too far-reaching.
1565.23s - 1567.23s |  That is too much of an interference
1567.23s - 1569.23s |  with people's privacy
1569.23s - 1572.23s |  that the police would get access to metadata
1572.23s - 1576.23s |  to have indiscriminate access to all the metadata.
1576.23s - 1579.23s |  And the question that is obvious to raise here is,
1579.23s - 1583.23s |  do we think that social media services, big tech,
1583.23s - 1586.23s |  do they have more or less personal data
1586.23s - 1588.23s |  or data in general about us
1588.23s - 1591.23s |  than telecom operators?
1591.23s - 1595.96s |  Probably with some magnitude more data.
1595.96s - 1597.96s |  The next question we should ask ourselves.
1597.96s - 1600.96s |  So fighting serious crime,
1600.96s - 1604.96s |  is that more important than serving personalized marketing?
1604.96s - 1607.96s |  And again, the answer should be obvious.
1607.96s - 1610.96s |  And that is also one of the reasons why it should be obvious
1610.96s - 1614.96s |  that the current models, under EU law at least,
1614.96s - 1616.96s |  are not viable, they are not legal.
1616.96s - 1618.96s |  That will be my argument.
1618.96s - 1621.96s |  But just the fact here that I think we should
1621.96s - 1624.96s |  still be curious about personal data,
1624.96s - 1626.96s |  but I think the general concept of privacy
1626.96s - 1628.96s |  will be much more important.
1628.96s - 1631.96s |  Then of course there is equality, nondiscrimination,
1631.96s - 1633.96s |  which is also part of human rights.
1633.96s - 1636.96s |  And we know that digital vulnerabilities
1636.96s - 1639.96s |  will come out of artificial intelligence.
1639.96s - 1642.96s |  Some people will lose their jobs and some won't.
1642.96s - 1645.96s |  And the whole idea of getting AI
1645.96s - 1647.96s |  is try to be more efficient so we can get rid
1647.96s - 1649.96s |  of people working for us.
1649.96s - 1652.96s |  Meaning also that we will lose potential whistleblowers.
1652.96s - 1655.96s |  Wouldn't it be great if we could just automate everything
1655.96s - 1657.96s |  so we wouldn't have anybody to blow the whistle
1657.96s - 1659.96s |  and say there's something going on here?
1659.96s - 1663.96s |  We will never see an Edward Snowden or Francis Haugen again
1663.96s - 1666.96s |  because everything is being automated.
1666.96s - 1669.96s |  And the last thing that comes with generative AI, as I said,
1669.96s - 1672.96s |  that is their hunger for getting more data
1672.96s - 1674.96s |  to train their models on.
1674.96s - 1677.96s |  And that is why they are looking into whatever content
1677.96s - 1681.96s |  you posted on Facebook 10 or 20 years ago,
1681.96s - 1683.96s |  and suddenly they consider it to be their property
1683.96s - 1685.96s |  and can just use it to train data.
1685.96s - 1688.96s |  Just like Apple can train their algorithms
1688.96s - 1691.96s |  on all the photos you put up there.
1691.96s - 1695.96s |  So intellectual property rights are extraordinarily important.
1695.96s - 1699.96s |  So what I will talk about here is big tech arrogance.
1699.96s - 1702.96s |  So big tech arrogance, in my view,
1702.96s - 1706.96s |  would constitute the fact that you have companies.
1706.96s - 1711.96s |  They can be small or large, but mainly big tech.
1711.96s - 1713.96s |  They will have an interest
1713.96s - 1718.96s |  and they will design their technology not in our interest.
1718.96s - 1721.96s |  On the contrary, against our best interests.
1721.96s - 1727.96s |  So they will make products that are not aligned with our interests.
1727.96s - 1730.96s |  Then they will use storytelling
1730.96s - 1734.96s |  to convince us that they're doing it for us.
1734.96s - 1739.96s |  And in addition, they're buying and gaslighting our politicians.
1739.96s - 1742.96s |  This is too complicated. Don't regulate.
1742.96s - 1744.96s |  You don't want to be in the EU, right?
1744.96s - 1746.96s |  It's too complicated. You're protecting all the users.
1746.96s - 1750.96s |  Don't go down that route. We're going to lose a lot of money.
1750.96s - 1755.96s |  And they do that to not be regulated, not to be taxed.
1755.96s - 1758.96s |  That is what I would consider to be big tech arrogance.
1758.96s - 1764.36s |  So what do we do about this?
1764.36s - 1768.36s |  So, of course, we need to have legislation, laws.
1768.36s - 1772.36s |  But we have realized how difficult it is to enforce law
1772.36s - 1776.36s |  when we are up against actors that are so big
1776.36s - 1781.36s |  and don't care about complying with the law.
1781.36s - 1784.36s |  That is very difficult. We've seen that over and over again.
1784.36s - 1788.36s |  So Meta in the European Union got a fine of half a billion euros.
1788.36s - 1791.36s |  Roughly half a billion US dollars.
1791.36s - 1795.36s |  That's a parking fine. That's a license fee for them.
1795.36s - 1800.36s |  They have profits of 40 billion each year.
1800.36s - 1803.36s |  And this was a fine for what they had done for six years.
1803.36s - 1806.36s |  Or maybe seven. It's a license fee.
1806.36s - 1810.36s |  It's not going to change anything.
1810.36s - 1815.36s |  So what I will propose here is that we focus on businesses.
1815.36s - 1818.36s |  So I come from a consumer law background.
1818.36s - 1821.36s |  But can we learn something in businesses from consumer law?
1821.36s - 1824.36s |  And here I will argue that businesses,
1824.36s - 1827.36s |  they are just as vulnerable as consumers.
1827.36s - 1830.36s |  Especially when it comes to AI and generative AI.
1830.36s - 1834.36s |  It's been hyped so much that many companies
1834.36s - 1840.36s |  will feel obliged to buy these licenses.
1840.36s - 1842.36s |  For instance, for Copilot.
1842.36s - 1844.36s |  Because their competitors are doing it.
1844.36s - 1848.36s |  The press are not critical about how much they can be used for.
1848.36s - 1850.36s |  What are the gains?
1850.36s - 1852.36s |  I mean, as a business, you should think about
1852.36s - 1855.36s |  what are the benefits and what are the downsides.
1855.36s - 1857.36s |  So businesses are vulnerable.
1857.36s - 1859.36s |  It's just like outsourcing stuff.
1859.36s - 1864.36s |  So when you use generative AI in your business,
1864.36s - 1867.36s |  it's like outsourcing.
1867.36s - 1869.36s |  And we've seen that.
1869.36s - 1872.36s |  There was a time when it was good to outsource all your production.
1872.36s - 1876.36s |  We could get cheap stuff we didn't need made somewhere else.
1876.36s - 1882.36s |  And now we realize there are some geopolitical issues about outsourcing.
1883.36s - 1889.36s |  So think about buying into the generative AI hype
1889.36s - 1892.36s |  as using the fear of missing out.
1892.36s - 1896.36s |  As we've used for social media, for human beings,
1896.36s - 1901.36s |  it's now being used as FOMO for businesses.
1901.36s - 1905.36s |  It is also being said that if you don't buy these licenses,
1905.36s - 1909.36s |  all your employees will go and use the unsafe version
1909.36s - 1913.36s |  that's freely available on the Internet.
1913.36s - 1917.36s |  Try and ask your boss whether they want to pay for World of Warcraft
1917.36s - 1919.36s |  or some other game.
1919.36s - 1924.36s |  Because if you don't buy it to us, we will use it.
1924.36s - 1928.36s |  So that is the pressure that is being put on businesses.
1928.36s - 1931.36s |  And we should just realize that there are risks
1931.36s - 1933.36s |  and associated costs with outsourcing.
1933.36s - 1936.36s |  It creates new dependencies.
1936.36s - 1940.36s |  Would we like to be as dependent in our core business strategy
1940.36s - 1945.36s |  as most companies are in using Microsoft Office?
1945.36s - 1948.36s |  When is the last time your company considered
1948.36s - 1953.36s |  which word processor to use for their employees?
1953.36s - 1956.36s |  Would you be in the same situation when it comes to artificial intelligence,
1956.36s - 1958.36s |  when you have fired people in the hope
1958.36s - 1961.36s |  that the AI would make you more efficient?
1961.36s - 1963.36s |  And it turns out that it's not more efficient.
1963.36s - 1965.36s |  It's actually more expensive.
1965.36s - 1968.36s |  Because who is going to pay the $1 trillion
1968.36s - 1973.36s |  that is supposed to be invested in a generative AI?
1973.36s - 1975.36s |  You and I. And who has the money?
1975.36s - 1976.36s |  The businesses.
1976.36s - 1980.36s |  So I think we should think about what businesses can do.
1980.36s - 1984.36s |  And we see that with the action taken by certain business
1984.36s - 1988.36s |  withdrawing advertising from the Social Media X.
1988.36s - 1990.36s |  That is what businesses can do.
1990.36s - 1992.36s |  Because businesses are big spenders.
1992.36s - 1996.36s |  They buy stuff in massive amounts.
1996.36s - 1999.36s |  So businesses can do something.
1999.36s - 2002.36s |  So one thing you could ask yourself is,
2002.36s - 2005.36s |  is cloud as cheap as social media or free?
2005.36s - 2007.36s |  Especially if you're in a situation
2007.36s - 2011.36s |  where the cloud content is being used to train AI.
2011.36s - 2014.36s |  And also don't believe everything you read on the internet.
2014.36s - 2018.36s |  Think about the incentives of all the consultants telling you
2018.36s - 2020.36s |  that you will become so much more efficient
2020.36s - 2024.36s |  if you adopt generative AI in your business.
2024.36s - 2028.36s |  Goldman Sachs had a very critical report out recently
2028.36s - 2032.36s |  saying that it probably overstated how much benefit
2032.36s - 2037.36s |  there will be to businesses from adopting generative AI.
2037.36s - 2041.36s |  So what we should do is we should sense the environment
2041.36s - 2043.36s |  we live in and act upon it.
2043.36s - 2046.36s |  That's a definition of agency.
2046.36s - 2047.36s |  As we talked about in the beginning,
2047.36s - 2050.36s |  which is a part of empowerment,
2050.36s - 2052.36s |  we should understand what's going on.
2052.36s - 2055.36s |  And I've tried to present that to you.
2055.36s - 2057.36s |  How does it work?
2057.36s - 2060.36s |  How are we being used by technology?
2060.36s - 2063.36s |  And again, I'm not saying technology is good or bad,
2063.36s - 2066.36s |  but the current data-driven business models,
2066.36s - 2069.36s |  including those based on AI,
2069.36s - 2072.36s |  are not necessarily in our best interest.
2072.36s - 2074.36s |  And we should be curious about that.
2075.36s - 2078.36s |  We should also use the human superpower.
2078.36s - 2080.36s |  What we can do as human beings
2080.36s - 2085.36s |  is that we can imagine things that don't exist.
2085.36s - 2087.36s |  That is our superpower.
2087.36s - 2090.36s |  We can imagine a building that is not built yet.
2090.36s - 2093.36s |  We can imagine value out of nothing.
2093.36s - 2096.36s |  We can create NFTs and suddenly they have value
2096.36s - 2097.36s |  and suddenly they don't.
2097.36s - 2100.36s |  But we can imagine stuff that doesn't exist.
2100.36s - 2106.36s |  So how can we as individuals imagine a better future?
2106.36s - 2111.36s |  How can we imagine the companies we're working for or with?
2111.36s - 2115.36s |  How can we talk to the people making decisions there?
2115.36s - 2117.36s |  Maybe it's not necessarily a good idea
2117.36s - 2121.36s |  to have generative AI licenses for all our employees,
2121.36s - 2126.36s |  especially not if data is being used to train these models.
2127.36s - 2131.36s |  We should recognize that democracy is the people ruling.
2131.36s - 2134.36s |  It's not big tech ruling.
2134.36s - 2139.36s |  So I would encourage and argue that human dignity
2139.36s - 2142.36s |  can be used for us as individuals
2142.36s - 2147.36s |  as a guiding policy for how we should see these things.
2147.36s - 2149.36s |  It can be used for businesses.
2149.36s - 2153.36s |  Would we accept to buy into generative AI
2153.36s - 2157.36s |  that has been built on other people's content,
2157.36s - 2160.36s |  infringing their copyrights?
2160.36s - 2164.36s |  And I also think we should be more careful about recognizing
2164.36s - 2167.36s |  that we are actually still living in a liberal democracy
2167.36s - 2171.36s |  and we have human rights and take them seriously
2171.36s - 2174.36s |  and have that into tech policy.
2174.36s - 2180.36s |  So I emphasize that policy is also what you think, say, and do.
2180.36s - 2184.36s |  And the best thing we can do for big tech at this moment
2184.36s - 2189.36s |  is to ensure that our children will not be able to do anything
2189.36s - 2191.36s |  without big tech.
2191.36s - 2195.36s |  Wouldn't it be great if people can't read, they can't think,
2195.36s - 2198.36s |  they can't talk, they can't write
2198.36s - 2201.36s |  without being dependent on generative AI?
2201.36s - 2203.36s |  Wouldn't that be great?
2203.36s - 2206.36s |  Then at least we will have the dystopian fantasy
2206.36s - 2209.36s |  that we see from California.
2209.36s - 2212.36s |  So what I will end this talk on is basically just saying
2212.36s - 2214.36s |  don't out-solve humanity.
2214.36s - 2216.36s |  We have to do something about this.
2216.36s - 2221.36s |  And if we don't do it as employees, as business owners,
2221.36s - 2225.36s |  as government officials, whatever, as hackers,
2225.36s - 2228.36s |  if we don't take these things serious,
2228.36s - 2232.36s |  it's going to be even worse than it is already now.
2232.36s - 2234.36s |  In the book I ask the question,
2234.36s - 2237.36s |  do we really have 10 years?
2237.36s - 2239.36s |  And that was three years ago.
2239.36s - 2243.36s |  Do we have seven years to get the balance right?
2243.36s - 2246.36s |  You make the decision, you can go and do a lot of stuff,
2246.36s - 2250.36s |  talk to people, talk to your friends, talk to your family,
2250.36s - 2252.36s |  just have discussions about these things,
2252.36s - 2254.36s |  revealing what's really going on.
2254.36s - 2256.36s |  Thank you so much for your attention.