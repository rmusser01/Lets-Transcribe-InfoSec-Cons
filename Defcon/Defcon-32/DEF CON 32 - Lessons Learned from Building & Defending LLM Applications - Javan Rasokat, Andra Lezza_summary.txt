**Introduction to LLM Applications**

- **Lessons Learned**: Insights from building and defending **LLM applications**
- **Focus**: Challenges, vulnerabilities, defense strategies, and best practices in **AI security**

**Presenters**

- **Andra Lezza**: Principal AppSec Engineer at Sage, OWASP chapter lead for London
- **Javan Rasokat**: Works with Andra at Sage, teaches secure coding at DHBW University, Germany

**AI Boom and LLMs**

- **Technological Revolution**: Increase in **LLM integrated apps**
- **Functionality**: Enhanced capabilities across industries
- **Vulnerabilities**: Introduction of new and exacerbation of old vulnerabilities

**Focus on Chatbots**

- **AI Chatbots**: Use **conversational natural language processing** 
- **Vulnerabilities**: Current vulnerabilities and future security designs for emerging technology

**Threat Modeling in LLM Applications**

- **Threat Types**: Development, threats through use, and runtime threats
- **AI Exchange Threat Model**: Comprehensive overview of threats and controls

**Key Vulnerabilities**

- **Data Disclosure**: Importance of **data integrity**, confidentiality, and accuracy
- **Prompt Injection**: Common issue in chatbots and LLM applications

**Examples and Challenges**

- **Hallucinations**: Incorrect outputs from AI, emphasizing the need for accuracy
- **Reputational Damage**: Risks from rogue chatbots

**Defense Strategies**

- **Combining Chatbots**: Use of rule-based and **gen AI chatbots** with secure APIs
- **Language Safety**: Additional models to detect toxic language
- **Red Teaming**: Continuous testing to uncover vulnerabilities

**AI Security vs. AI Safety**

- **AI Security**: Protects against malicious threats
- **AI Safety**: Ensures systems operate without unintended harm

**Prompt Injection Case Study**

- **Example**: Smart doorbell enhanced with AI demonstrated vulnerabilities
- **Lessons Learned**: Importance of **AI safety** and security

**Defense Techniques Against Prompt Injection**

- **Output Monitoring**: Use of regex to filter specific words
- **Input Restrictions**: Limiting topics or using stop sequences
- **False Leads**: Introducing fake secrets as a defense mechanism

**OpenAI Documentation and Tools**

- **AI Red Teaming**: Simulate attacks to identify vulnerabilities
- **End-to-End Testing**: Prompt misuse tests to prevent injection

**Secure LLM Architecture**

- **Preprocessing**: Input checks using regex and harmful content models
- **Output Checks**: Ensuring safety by monitoring LLM-generated content

**Takeaways**

- **Threat Model Everything**: Be aware of trust boundaries and injection threats
- **Continuous Testing**: Keep up with changes in AI security landscape
- **Zero Trust**: Treat LLM models with strict boundaries and safeguards