{
  "webpage_url": "https://www.youtube.com/watch?v=WsRSzJsrWAw",
  "title": "DEF CON 32 - Defeating Secure Code Review GPT Hallucinations - Wang Zhilong, Xinzhi Luo",
  "description": "In this talk, we will discuss the strengths and limitations of LLMs for code analysis tasks like code search and code clone detection. We will show when the LLMs make mistakes and what kinds of mistakes they make. For example, we observe that the performance of popular LLMs heavily relies on the well-defined variable and function names, therefore, they will make mistakes when some misleading variable name is given. Anyone interested in exploring the intersection of AI and code security analysis can attend this talk.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 1077,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 8.00s | This text was transcribed using whisper model: large-v2

 Alright everyone, thank you very much for joining us. Our next talk is SDLC Nightmares Defeating
8.00s - 26.70s |  Secure Code Review H- er GPT Hallucination with Julo and April. Hello my name is Julo and uh I
26.70s - 33.70s |  just graduated from the Pennsylvania State University last year and now I'm working at the
33.70s - 40.70s |  security engineering industry and uh my research interest is focused on applied deep learning in
40.70s - 49.14s |  cyber security. And I'm April, Carnegie Mellon University graduate and currently a security
49.14s - 56.14s |  engineer as well. A fun fact about me is I learn and apply my infosec knowledge during writing
56.14s - 65.14s |  science fiction. Okay let's get started. As most of us might have noticed, nowadays large language
65.14s - 72.14s |  models have been playing an increasingly important role in our daily work and lives. Today we'll
72.14s - 79.14s |  discuss how LLMs can be applied to security code analysis and evaluate their reliability in
80.14s - 89.65s |  understanding program logic. Uh overview of our talk today, first we'll introduce LLMs and their
89.65s - 96.65s |  strengths in SDLC, then we'll uncover their limitations through an experiment. Based on our findings
96.65s - 103.65s |  we'll further discuss the root causes and their other related issues. Finally we'll summarize key
103.65s - 114.11s |  takeaways on how to better utilize LLMs in our work. So first of all what is large language model?
114.11s - 123.11s |  Basically LLM is advanced machine learning models that understand and generate human like text uh by
123.11s - 132.11s |  learning from fast data sets. They use deep learning especially uh transformer architectures to predict
132.11s - 140.11s |  the most probable word in a sentence. For example in this figure it estimates uh 91% probability that
140.11s - 149.11s |  the word blue follows the phrase the color of the sky is. LLMs generate coherent and contextually
149.11s - 156.11s |  appropriate text enabling tasks like answering questions, uh translation and our topic today
156.11s - 166.11s |  code analysis. Now let's talk about SDLC. A typical software development life cycle involves various
166.11s - 176.11s |  stages where LLM can significantly assist. By understanding semantics and recognizing patterns, LLMs
176.11s - 185.11s |  can comprehend natural language and analyze code. Performing tasks like code generation, test case
185.11s - 194.11s |  creation and also documentation. From a security perspective they also help with threat modeling and
194.11s - 203.11s |  identifying vulnerabilities. Ultimately boosting the efficiency and uh quality throughout the SDLC.
203.11s - 211.11s |  However LLMs can make mistakes. Here are two famous examples. When Google introduced their chatbot
211.11s - 220.11s |  bot it made a factual error during its very first demo. Another instance involved chat GPT which
220.11s - 229.11s |  confidently asserted that 9.11 is greater than 9.9. A mistake even an elementary student will recognize.
230.11s - 239.11s |  These errors stem from their tendency to state incorrect information as facts. LLMs often hallucinate like
239.11s - 245.11s |  what I do every time before sleeping or conceiving a science fiction story. They tend to make up
245.11s - 254.11s |  information because they're essentially autocomplete system based on probability rather than facts. Now
254.11s - 263.11s |  let's see their performance in code analysis. Before diving into the experiment let's first take a closer
263.11s - 271.11s |  look at the code itself. Technically source code is a sequence of tokens consisting of three types.
271.11s - 280.11s |  Keywords, operators and user defined names. Keywords and operators are defined by the specification of
280.11s - 288.11s |  each programming language and they control the actual logic flow of the program execution. So we categorize
288.11s - 297.11s |  them as logical features. Meanwhile user defined names, annotations and comments which don't influence the
297.11s - 307.11s |  program flow once it's compiled are categorized as literal features. So our goal for the experiment is to
307.11s - 315.11s |  determine what types of features in the code LLMs can actually learn and process. We'll mask the literal features
315.11s - 323.11s |  by replacing the changeable names that don't affect the program flow and then use the LLM to perform code
323.11s - 332.11s |  analysis tasks on this modified code. By evaluating the performance we'll assess whether LLM can effectively
332.11s - 341.42s |  process the logical features when literal features are absent or even misleading. To conceal the literal features
341.42s - 350.42s |  we'll adopt three kinds of anonymization. So here is an example of the bubble sort function in Java and the first
350.42s - 358.42s |  anonymization is for function name. So here we can replace the bubble sort string with our customized string.
359.42s - 367.42s |  And the second one is for variable names. Two examples here is the begin and end which we can replace with our
367.42s - 376.42s |  strings. And the last one is the method invocation name. Here we have a function pret as one of the arguments for
376.42s - 385.77s |  the bubble sort function so we can replace the invocation name with our customized string. Meanwhile to understand
386.77s - 394.77s |  how literal features affect LLM comprehension we also use two name replacement strategies. The first one is just
394.77s - 403.77s |  replacing it with some randomly generated string while the second one is shuffling the names with other meaningful
403.77s - 414.77s |  names from the code making it more misleading for users. For readers, sorry. So after literal features are concealed
414.77s - 424.77s |  the LLM performs two main tasks related to code analysis. The first one is code clone detection which identifies and
424.77s - 433.77s |  measures the similarity between different code fragments to see if they generate the same result when given the same
433.77s - 443.77s |  input. The second one is code search where it tries to retrieve the code fragment from a larger code corpus that
443.77s - 455.11s |  closely match a developer's intent which is expressed in natural language as the input. So here are the results we got.
455.11s - 463.11s |  It shows that anonymizing names significantly decreases model performance especially when we replace all three kinds of
463.11s - 472.11s |  names. In the code search task we also notice that anonymizing master definition name causes more degradation than
473.11s - 481.11s |  anonymizing master invocation name indicating a stronger correlation between the master definition name and their
481.11s - 491.11s |  purpose. As for different languages, anonymizing a single attribute affects Python more but changing all naming
491.11s - 501.11s |  attributes impact Java the most probably because the verbosity and the structure in Java code that can mitigate partial
501.11s - 512.10s |  anonymization are fully disrupted. Compared to the code clone detection task, the performance degradation is more
512.10s - 520.10s |  pronounced in code search task since will define variable and method names are crucial for relating the code snippet to its
520.10s - 531.10s |  descriptive purpose. So overall the results clearly show that while logical features are what matters for program to
531.10s - 541.10s |  execute, what LLMs processed are mostly the literal features from the code. Now I'll hand it over to Dr. Zhilong to discuss the
541.10s - 544.10s |  reasons and possible solutions.
544.10s - 557.52s |  Okay. So let's discuss some root cause for the misleading names and comments in the code. Basically we just observed three
558.52s - 566.52s |  types of the root cause for these misleading names and comments and they are malicious code committers and outdated comments
566.52s - 580.36s |  and bad coding practice. Malicious code contributor can contribute some malicious code to the open source projects and to avoid
580.36s - 590.36s |  being detected by the code reviewers and they usually can change the naming to some misleading names to avoid being detected by
590.36s - 607.10s |  the code reviewer. Secondly, in software development programs often change frequently update their code to fix some bugs or add
607.10s - 619.10s |  some new features. Well the comments corresponding to the code may not be updated in time. In this example, partial code was
619.10s - 632.06s |  removed during a new commit and however the corresponding comment was not removed accordingly. Comments that are not updated can
632.06s - 642.06s |  cause inconsistency with the code and we call them outdated comments. And outdated comments can mislead the light learning model
642.06s - 656.87s |  during code analysis. Also in bad coding practice, confusing variable names and misleading variable names could be given by even a benign
656.87s - 673.26s |  programmer and developer which can mislead the code reviewer too. So next we are going to cover some other issues when using the light
673.26s - 690.51s |  learning model to do the code analysis. Besides aforementioned limitations we also observe two limitations. The first one is lack of context
690.51s - 708.20s |  and the second one is diverse performance on different program language. We will show the detail. Most light learning models can only
708.20s - 725.20s |  handle a sequence of limited length. For example, GPT-3 can handle a sequence with 2,048 tokens at most. And the GPT-4 can handle 32,000 tokens
725.20s - 739.20s |  at most. However, the code for even small projects can have thousands lines of code and each line of code contains usually more than 10 tokens.
739.20s - 752.45s |  Therefore, the light learning model cannot consume all the code at once which will lead to lack of context issue. Let's show through this figure.
752.45s - 767.45s |  We have a big project here showing this figure and let's consider context of the light learning model through this blue box.
767.45s - 792.35s |  When we use the light learning model to analyze this whole project, we need to analyze part of them individually. In most cases, the light learning model is used to
792.35s - 807.35s |  analyze each function individually and cannot have a big picture of the whole project. So, we actually have written an ARXIV preprint to dive into this problem.
807.35s - 830.20s |  We also propose some solution to solve this problem and for people who are interested in the details can read this ARXIV preprint. Also, we observe that light learning model has a diverse performance
830.20s - 850.20s |  across different program languages and different light learning models also have different strengths across different program languages. We did a quick experiment to evaluate the light learning model's ability to infer the program logic on two program languages.
850.20s - 878.26s |  The result is shown in the table here. We can observe that the GPT 3.5 has shown the best results on PHP and Go. However, Gemini can show the best results on PHP and Python.
878.26s - 904.89s |  Next, we will discuss how to avoid this issue in using light learning model for code analysis. We start from some ad hoc solutions. If the code is committed by a trusted developer, we should expect them to improve the quality of the name convention.
904.89s - 921.89s |  If the code is committed by some untrusted developer, we should pay special attention in case the light learning model is fooled by the malicious code committer, which can easily be achieved based on our experiments.
921.89s - 952.76s |  For example, we could deploy some naming logical inconsistency detection and use it to detect the inconsistency before we perform the code review. However, to really solve this problem, we believe that we need to change the model a little bit specifically for the code analysis.
952.76s - 966.76s |  A current popular pre-training task for the light learning model, as far as we know, is mask token prediction. In the mask token prediction task, it masks off some tokens in the sequence.
967.76s - 985.76s |  Take this as an example. It masks off some tokens from its sequence and uses neural network to predict this token. This is how current light learning model is trained, as far as we know.
985.76s - 1006.76s |  The mask token prediction task mainly focuses on the literal feature when applied to the source code analysis. But if we apply this model to the neural language NLP, it's okay because it does not have too much of the logic features.
1006.76s - 1025.76s |  But for the source code, we have a lot of the logic features. So, to really solve this problem, we think that we need to add some more pre-training tasks to train a new large program language model.
1025.76s - 1048.76s |  We should add some logic feature prediction tasks so that it can better capture the logic feature, not the literal feature. Because, as we show, relying too much on the literal feature will result in some issues, as we have demonstrated in the experiments.
1048.76s - 1072.24s |  So, for the audience who are interested in the details of this talk, you can refer to this ARX that we pre-printed for more details. Here is our contact information. And that's all of our talk.