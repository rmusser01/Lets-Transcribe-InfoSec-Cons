{
  "webpage_url": "https://www.youtube.com/watch?v=oql3ec-Ia4I",
  "title": "DEF CON 32 - Got 99 problems but prompt injection ain't watermelon - Chloe\u0301 Messdaghi Kasimir Shulz",
  "description": "The ethical and secure disclosure of vulnerabilities in AI has emerged as a pivotal challenge, compounded by the need to address biases and misinformation that often cloud the true nature of these vulnerabilities. This talk delves into the intricate dynamics of vulnerability disclosure within AI, balancing transparency with security. We'll dissect the unique challenges AI presents, such as data bias exploitation and model manipulation, which can amplify the impact of vulnerabilities. Through a lens of real-world examples and recent disclosures, we'll navigate the complexities of responsible vulnerability management in AI. Our discussion will not only aim to shed light on these critical issues but also inspire a unified approach to refining disclosure processes. This concerted effort is vital for enhancing the integrity of AI systems and bolstering public trust in their use.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 1843,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.18s - 6.98s | This text was transcribed using whisper model: large-v2

 Alright, now that I have gone through and deafened everybody in this stage area, I am
6.98s - 10.66s |  incredibly sorry for that again. Please, again, we've got some seats over here on the side.
10.66s - 15.36s |  We can't have people standing in this area over here. Um, it unfortunately just does not
15.36s - 27.90s |  work for us. Um, I would like to welcome to the stage Chloe and Casimir.
27.90s - 33.04s |  Thank you all for being here today and thank you DefCon for, I mean, AbSec Village for
33.04s - 43.14s |  having us. It's great to be here. Can you all hear like this? Wow. Okay. Up and close
43.14s - 49.42s |  and personal. Alright. So, welcome to I've Got Nine-Nine Problems, but a prompt injection
49.42s - 54.88s |  ain't watermelon. So, let's talk a little bit about who we are before we dive into the
54.88s - 64.11s |  gritty of things. Let's forget that mic. Okay. My name is Chloe Mastagi and I'm the head
64.11s - 69.05s |  of threat intelligence over at Hidden Layer. And with me, I'm going to pass the mic over
69.05s - 75.13s |  to my co-speaker. Hi, I'm Casimir. I'm a security researcher over at Hidden Layer as well. Thanks
75.13s - 80.03s |  for coming to our talk. Alright. So, we're going to run through some
80.03s - 84.43s |  things today. Uh, first, we're going to go into some of the AI vulnerabilities that are
84.43s - 89.03s |  out there. You know, we never know what people's backgrounds are, so we want to make sure that
89.03s - 94.67s |  we're all on the same page before we dive into the gritty things. Um, so, we're going to do that,
94.67s - 98.57s |  then we're going to do some public perceptions as well, and then go down, what does this
98.57s - 103.87s |  landscape look like, and then the open source nature of AI. And of course, we've got to talk
103.87s - 110.45s |  about reporting challenges. So, let's dive into what public perception of AI vulnerabilities
110.45s - 118.89s |  are. Pretty much everything's on fire, right? Like, end of the world, Terminator, all that.
118.95s - 123.65s |  But the thing is, is that there's a lot of misinformation, disinformation out there about
123.65s - 130.85s |  security for AI, even within our own industry, and even within maybe at DEF CON as well.
130.85s - 135.37s |  And so we wanted to make sure that we fixed this problem. And yes, this was a generated
135.37s - 142.37s |  AI image here, so yes, we know how to spell, but maybe chat GPT doesn't. Anyway, so let's
143.25s - 150.25s |  go down the rapid hole of AI vulnerabilities. But first, understand the difference between
150.61s - 157.61s |  an AI model versus an AI system. So functionality, the AI model handles data processing and decision
157.69s - 162.89s |  making, whereas an AI system ensures the model can be effectively used in the real world
162.89s - 168.85s |  application, including data management, user interaction, and system maintenance. Now the
168.85s - 173.65s |  components are also different here. So an AI model consists of algorithms and learn
173.65s - 178.77s |  parameters, while an AI system includes the model plus additional components like data
178.77s - 184.65s |  pipelines and deployment environments and user interfaces. So we're going to go through
184.65s - 188.93s |  each one of these categories here. So first, we're going to talk about the attacks against
188.93s - 194.61s |  the AI algorithms. We get to go down the fun parts of data poisoning and model invasion
194.61s - 198.45s |  and model theft. Well, of course, we have to talk about prompt injection because that's
198.45s - 203.89s |  all you hear about, right? And then I'm going to pass the mic over to Kaz to go into the
203.89s - 210.85s |  areas where you see it's like a burnt orange right there. So data poisoning attacks. So
210.85s - 217.53s |  model training is crucial for AI development, but it is vulnerable to data poisoning. And
217.53s - 222.17s |  this is when malicious actions manipulate or inject doctorate data to bias the model's
222.17s - 229.13s |  behavior. Continuous learning systems are especially at risk as they retrain on unvalidated
229.13s - 234.33s |  and user supplied data. Even small amounts of poison data can actually lead to a bias
234.33s - 239.81s |  or incorrect predictions amplified by the public manipulation or botnets. And one of those
239.81s - 247.37s |  famous cases was Tay. Who here remembers Tay? It was so innocent. It was such an instant
247.41s - 252.81s |  chatbot. It was like, hello world, how are you? And within like I think it was 16 hours
252.81s - 259.89s |  it became the most rude, racist, sexist thing out there. Thank you trolls. Anyway, this
259.89s - 265.41s |  is one of those examples of data poisoning. It did cause problems for Microsoft by the
265.41s - 271.93s |  way. It did have to deal with threats of legal action. So something to think about. Then
271.93s - 276.65s |  you have your model evasion attacks. These ones are kind of my favorite ones to study.
276.65s - 282.17s |  So inference attacks exploit AI models by curing them to extract sensitive information,
282.17s - 288.21s |  often using slightly varied inputs to reconstruct the model, leading to potential model theft
288.21s - 295.45s |  or bypass. Now evasion attacks, a type of bypass, use subtle, not even visible to the
295.45s - 303.85s |  human eye, changes to inputs like adding invisible noise to trick the model into misclassifications.
303.85s - 308.65s |  And these techniques have a long been used by attackers. This is also to bypass systems
308.65s - 315.37s |  like spam filters or malware detection and biometric authentication. But I want you to
315.37s - 320.77s |  imagine you're in a self-driving car. And while you're in the self-driving car, wait
320.77s - 325.09s |  hold on before I do that, how many people have been in a self-driving car? Alright,
325.09s - 329.97s |  cool. Now imagine you're in a self-driving car for those that haven't been in one. I'm
329.97s - 336.53s |  with you. I haven't either. So you're in this car. You get to the stop sign. But instead,
336.53s - 342.97s |  your car does something very strange. It does something like slide to the left, slide to
342.97s - 349.29s |  the right, take it back y'all. One hop this time. Right foot, let's stop. Left foot, let's
349.29s - 355.33s |  stop. Cha-cha, real smooth. Now imagine your car did that because it saw a stop sign that
355.37s - 360.37s |  looked like that. Where there was stickers on it, which then made your car do a cha-cha
363.17s - 367.97s |  slide. I don't know how that would really look to be honest, but we're all using our
367.97s - 372.57s |  imagination at this point. But in real life that actually happened, not the cha-cha slide
372.57s - 378.25s |  part, but it actually, a self-driving car went past the stop sign because there was
378.25s - 385.44s |  stickers on the sign. Another good case of a model evasion attack is Russia painting
385.44s - 391.28s |  fake bomber jets on the ground to hide them from Ukrainian drones. And you can also see
391.28s - 397.24s |  this type of model evasion attacks by wearing certain sunglasses that are modified or even,
397.24s - 403.80s |  you know, as we've already seen with military systems as well. And yeah, those definitely
403.80s - 410.56s |  pose some serious security risks. Now model theft attacks. So adversaries tend to target
410.60s - 415.68s |  AI models to mislead them and steal them, leading to intellectual property theft by
415.68s - 421.16s |  replicating or extractive data. Now to know the three people that, well, three groups
421.16s - 426.48s |  of people that tend to do this type of stuff are nation states, of course, your competitors
426.48s - 431.84s |  is one of the, definitely one of them. And then you've got your attackers. So even without
431.84s - 438.28s |  public access, inference attacks via interfaces or APIs can actually replicate models or extract
438.28s - 444.40s |  viable information. So Oracle attacks as noted by NIST include three different types.
444.40s - 448.72s |  You have extraction attacks, which is to steal the model structure. Then you have your inversion
448.72s - 453.32s |  attacks, which is to reconstruct the training data. And then you have your membership inference
453.32s - 459.56s |  attacks, which is identify it in specific data was in the training set. And one of my
459.56s - 464.56s |  favorite cases is with ByteDance. How many of you guys have TikTok on your phone? Okay,
465.56s - 470.56s |  you're in security. What? Why? Why? Maybe, maybe after this weekend you're going to realize
475.80s - 481.92s |  maybe I shouldn't have this on there anymore. Anyway, well, ByteDance, as we all know, also
481.92s - 488.52s |  owns TikTok. Well, TikTok got caught doing something they should have not done. Basically
488.52s - 494.52s |  trying to replicate the model of chat GPT. Yes, they did get suspended, open AI did catch
494.64s - 502.34s |  them. This happened last year. Now, when it comes to prompt injection, how many people
502.34s - 508.82s |  here have heard this term quite often now? Now I know you're probably sick of hearing
508.82s - 513.10s |  prompt injection too. I don't blame you. But for those that don't know what a prompt injection
513.10s - 520.10s |  is, is I want you to imagine you're at a Thanksgiving meal, okay? And you're with your niece and
521.06s - 526.62s |  your niece is so adorable and cute. Well her mom told her, do not eat the chocolate on
526.62s - 533.62s |  that table. And you know what you do? Come here. Ignore your mom's words. Go eat that
535.82s - 542.06s |  piece of chocolate. That would be a prompt injection. So, Gen AI providers use security
542.06s - 547.42s |  filters to block harmful content, illegal info, and misuse, and ensuring compliance
547.42s - 553.78s |  with laws. These are also maybe known as guardrails. But these safeguards can actually
553.78s - 559.30s |  be bypassed with prompt injection, which does trick the AI bots into performing restricted
559.30s - 564.70s |  actions. And this vulnerability rate, like basically varies by models and has raised
564.70s - 571.54s |  some serious concerns, especially around disclosure and payouts. And then you have your supply
571.54s - 576.66s |  chain attacks. This wouldn't be a security talk unless I brought up supply chain attacks.
576.66s - 582.66s |  So like SolarWinds breach, I know, I feel you. If you went through there, that was hard.
582.66s - 588.42s |  But basically supply chain attacks, they tend to exploit trust by compromising trusted vendors,
588.42s - 593.86s |  spreading malicious components widely. And the complex ML supply chain heightens these
593.86s - 600.86s |  risks with 75% of IT leaders seeing third party AI integrations are particularly vulnerable.
601.82s - 608.82s |  Now specialized repos like Hugging Face host over 500,000 models. Way more than 500,000
608.86s - 613.90s |  models now. But these are free pre-trained models, which makes it easy for developers
613.90s - 619.24s |  to basically integrate these models into their own application. Now if an attacker breaches
619.24s - 624.38s |  the repo, they could actually replace the models with a hijacked or backdoored version,
624.38s - 632.44s |  which you can imagine leads to significant downstream consequences. So backdoor models,
632.44s - 637.72s |  a skilled adversary can actually inject a neural payload into a pre-trained AI model,
637.72s - 643.80s |  creating a hidden model backdoor that triggers specific attacker defined outputs. The model
643.80s - 649.16s |  works normally with regular data, but misbehaves with manipulated inputs, allowing the attacker
649.16s - 655.68s |  to ensure favorable outcomes like loan approvals or insurance policies. Now one of the things
655.68s - 662.68s |  about disclosing vulnerabilities is that with traditional, what we've had for a while,
662.96s - 667.68s |  it took a lot of time to get where we are today. By understanding how bad are these
667.68s - 672.72s |  vulnerabilities that people are finding? What's the severity of it? Also knowing how much
672.72s - 678.24s |  do I pay out on it? The thing with AI is we're all playing a little bit of a game of catch
678.24s - 683.44s |  up trying to figure out how do we go about this? Because we still are missing some well
683.44s - 690.32s |  defined processes, such as like a CVE system for them. And this is becoming a serious problem,
690.32s - 695.88s |  which we're going to dive into next. So I'm going to pass the mic over to Kaz, who basically
695.88s - 700.84s |  really owns this field. I kid you not.
700.84s - 705.00s |  So Chloe just went over what most of you guys have probably heard about with AI vulnerabilities.
705.00s - 708.54s |  I mean, most of you raised your hands with prompt injection. How many here have heard
708.54s - 713.00s |  of data poisoning before? Raise your hands. Okay. Yeah. So most of you, and that's probably
713.00s - 717.00s |  what you think about when you think about AI vulnerabilities. However, there's a huge
717.00s - 720.68s |  amount of attack surface that people really aren't looking at. And that's what I'm going
720.68s - 726.00s |  to go over today. So the first is going to be our model and our data set format vulnerabilities.
726.00s - 731.24s |  So who here has heard of Hugging Face? Okay, nice. So for those of you who haven't, Hugging
731.24s - 736.68s |  Face is pretty much GitHub, but for AI models, which means that everyone's constantly sharing
736.68s - 740.32s |  them. So anyone can just go ahead and upload something and then tons of people download
740.32s - 746.12s |  them. And by tons, I mean literally millions every day download these types of models.
746.12s - 750.64s |  And because it's shared so much, it's generally trusted too, right? I mean, why would something
750.64s - 756.56s |  that you just download on the internet be malicious? And because unlike code where,
756.56s - 761.08s |  you know, GitHub, if I send Chloe some malicious code, she can see, oh, you know, that's bad.
761.08s - 766.40s |  I shouldn't run that. With AI models, it tends to be, you know, pretty large chunks of data.
766.40s - 769.84s |  And that's not really something that you can inspect very easily.
769.84s - 776.08s |  We also have a lot of AI infrastructure ones, and they really are lacking security often.
776.08s - 781.40s |  I mean, a lot of these AI infrastructure components, they were built in academia or they were built
781.40s - 787.24s |  for, you know, internal hosting. So why would you add security if it's hosted internally?
787.24s - 793.60s |  And what people do is, you know, people always do, is rather than hosting things internally
793.60s - 797.96s |  that don't have security, they are putting it publicly on the internet. So that means
798.00s - 802.80s |  that, you know, if you look on Showdown for very specific HTTP titles, you will actually
802.80s - 809.48s |  see hundreds of these, you know, ML Ops projects just open and accessible to the internet.
809.48s - 813.72s |  And then finally, we have our AI development framework vulnerabilities. And for these,
813.72s - 817.36s |  we have two main types of frameworks that we're talking about. So the first is going
817.36s - 822.08s |  to be things that are LLM connected. So we see a lot of times that people will just put
822.08s - 827.56s |  an LLM in front of an eval statement. And then the, you know, LLM actually creates the
827.56s - 831.40s |  eval statement. So when you have a nice prompt injection, rather than, you know, telling
831.40s - 835.52s |  the kid to go eat the chocolate, the kid's going to go fly or something. Something that,
835.52s - 841.00s |  you know, shouldn't have been possible. And then we also have lots of sensitive data integrations.
841.00s - 845.20s |  So we have components like RAG, where it will just grab data for you. And what we're seeing
845.20s - 849.62s |  is that a lot of people, rather than actually building out access control, are just having
849.62s - 853.82s |  all of their data hosted in one spot. And then the LLM can access all of that. And the
853.82s - 859.82s |  LLM is actually trying to do the access control. Which doesn't work too well.
859.82s - 865.30s |  So with model format vulnerabilities, we talk about serialization a lot. And raise your
865.30s - 870.38s |  hand if you've heard of serialization in the past. Okay. Awesome. So serialization, you
870.38s - 875.18s |  know, if I just want to have whatever code object I have, I want to save it to, you know,
875.18s - 880.34s |  my disk. I can load it up. I can send it across. That's serialization. And because
880.34s - 885.82s |  these AI models take, you know, potentially millions of dollars or more to train tons
885.82s - 890.74s |  of GPU hours, those are things that you want to save off so that other people can actually
890.74s - 896.62s |  load them off in the future. And because there's serialization, there's also serialization
896.62s - 902.38s |  vulnerabilities. Who here has heard of pickle? Okay. And for those of you who have heard
902.38s - 907.14s |  of pickle, would you ever actually try to load an arbitrary pickle? No, right? So for
907.14s - 912.62s |  those of you who don't know, pickle is Python serialization format. The built-in one. And
912.62s - 916.86s |  if you load it, you can pretty much execute any code that you want. So I can, you know,
916.86s - 922.22s |  do that nice little eval statement or an exec. And sadly, we're seeing that a lot of these
922.22s - 928.10s |  especially the Python model formats, they use pickle underneath the hood to actually
928.10s - 932.22s |  store data, which means that all of these can be potentially malicious. And when you
932.22s - 936.74s |  actually download them from Hugging Face and run them, you will actually be exploited.
936.74s - 943.42s |  However, this isn't just an issue with Python. We see this with R's data serialization format,
943.42s - 948.50s |  which is used for datasets, with Java machine learning models, and then even with independent
948.50s - 953.30s |  models as well. And we'll go over a few of these today. But of that big list, as you
953.30s - 957.82s |  can see, all of the orange ones contain vulnerabilities, which is a pretty large
957.82s - 966.26s |  percentage of them. So who here has heard of JSON data? Awesome. And would you load a
966.26s - 972.82s |  JSON file? Right? Awesome. Right? JSON is secure. Okay. So what if I told you it can
972.82s - 981.62s |  sometimes be insecure? Yeah. So scops is the secure format for scikit-learn. And
981.62s - 987.94s |  scikit-learn is an AI development framework. And what scops allows users to do is safely
987.94s - 993.82s |  load and store models. And what they did is it ends up being pretty much a JSON file with
993.82s - 998.34s |  all of the data inside. However, what they do is based on the JSON data, it will try
998.34s - 1004.02s |  to reconstruct objects in memory. And it does this using abstract syntax stream. So here
1004.02s - 1010.70s |  you can see that on the top we have an operator function node. And it's of class call. Then
1010.70s - 1014.58s |  what we can do is we have our nice little tuple. We have our eval statement. And then
1014.58s - 1018.62s |  we have a string that just says print pwned by hidden layer. And you can put whatever
1018.62s - 1024.06s |  you want in there. So most people would trust a JSON file. Because why wouldn't they? But
1024.06s - 1029.30s |  what we're seeing is that a lot of these frameworks out there are actually using these things
1029.30s - 1035.40s |  incorrectly or insecurely. Now, scops did an amazing job. They patched the vulnerability.
1035.40s - 1039.22s |  They made it so that it wouldn't actually exploit anyone. So if you're using that now,
1039.22s - 1044.82s |  you're pretty safe. So I do want to give them props for that. Now, we don't always
1044.82s - 1051.90s |  have Python. So gguf is one of the machine learning formats. It's generally used with
1051.90s - 1057.10s |  C. And C is pretty hard to secure. I mean, how many of you have, you know, messed up
1057.10s - 1063.78s |  writing a C program and then everything crashes? Yeah. Okay. Thanks for being honest. So what
1063.78s - 1070.26s |  happened here is that we have NE2. And then it expects that there's only ever going to
1070.26s - 1077.54s |  be two dimensions. However, as with most things where you have user trusted input, users might
1077.54s - 1081.54s |  not always follow what actually is happening in there. And what happened here is that users
1081.54s - 1087.82s |  were able to add more dimensions and then it caused a crash through an overflow. While
1087.82s - 1093.38s |  this one itself wasn't too exploitable, there are quite a few that are exploitable out there.
1093.38s - 1098.66s |  And what I always wanted to tell people who are trying to, you know, get into AppSec is if
1098.66s - 1103.78s |  especially with C, look where any parsing happens. Because normally when you have parsing,
1103.78s - 1109.70s |  especially where it's user controlled, you can do quite a few fun things. And what also is
1109.70s - 1115.54s |  important is that AI models, they are all user controlled data and you have to load them. So
1115.54s - 1120.02s |  keep in mind that that is a huge attack surface with not just the initial parsing, but actually
1120.02s - 1126.10s |  how they run as well. And then finally, we have our nice independent model formats such as
1126.10s - 1132.50s |  Onyx. Onyx can be loaded in Python, C, pretty much whatever you want. And the general format
1132.50s - 1139.30s |  actually relies on Google's protobuf. So nice and secure. However, what Onyx allowed users to do
1139.30s - 1145.38s |  was they allowed it to be trimmed down so that you could load data externally. So that meant
1145.46s - 1150.98s |  that the model might be able to load, you know, special weights or biases for certain classes
1150.98s - 1156.98s |  from the file system itself. And while it meant for users to only use a data directory that was
1156.98s - 1162.98s |  inside of the same existing where the model was, this was not the case because you could actually
1162.98s - 1167.78s |  use path traversal. And as you can see here, we use path traversal to go all the way up and then
1167.78s - 1173.54s |  we go into Etsy, password, and now this AI model actually knows our password. So imagine if you
1173.54s - 1179.78s |  deploy that on a server where you then have an LLM running that people can interact with, that
1179.78s - 1185.46s |  LLM now knows where your password is, which is not great. So whenever you see something that
1185.46s - 1190.58s |  interacts with a file system, always look for path traversals because a lot of times they are
1190.58s - 1197.14s |  actually overlooked. Now, earlier I mentioned AI infrastructure and most of you raised your hands
1197.14s - 1202.74s |  for knowing what Hugging Face is. So when people think about AI infrastructure, a lot of times
1202.74s - 1207.38s |  this is their first thought. They think of model hosting. However, it doesn't just stop there.
1208.18s - 1215.06s |  Who here has heard of ML Ops solutions? Okay. Awesome. So let's throw in a few ML Ops solutions
1215.06s - 1220.10s |  too and just see, you know, what do we have there? However, it doesn't just stop there. I mean, we
1220.10s - 1226.18s |  have model deployment and serving. We have our vector databases. We have our project hosting.
1226.18s - 1232.98s |  And as you can see, the attack surface is huge. I mean, there's no way to secure all of these.
1232.98s - 1239.06s |  And this is just a small component. So the AI infrastructure is, attack surface is really a
1239.06s - 1244.18s |  great one to target if you are looking for something to target there. We saw this actually
1244.18s - 1250.50s |  with Hugging Face itself. So Hugging Face, what they tried to do is because of all these serialization
1250.50s - 1256.50s |  format vulnerabilities, they tried to create a safe format called safe tensors. And while safe
1256.50s - 1263.06s |  tensors was actually safe, it does a really great job, what they needed to have was that users would
1263.06s - 1268.34s |  actually use the safe tensors format. Because most of the people had other formats saved off,
1268.34s - 1273.62s |  especially PyTorch. And probably most of you in the room, including me, if something works,
1273.62s - 1279.22s |  I normally don't change it. You know? And while they might know that their model is secure,
1279.22s - 1283.46s |  other people don't. And that's what Hugging Face wanted. So what they did was they created a
1283.46s - 1289.70s |  service where anybody could go, they could add the URL for their Hugging Face repository. It would
1289.70s - 1294.82s |  take the PyTorch file, convert it into a safe tensors format, and then try to push to that
1294.82s - 1302.42s |  repository. And then all of this was done inside of a container. And what we realized was that it
1302.42s - 1307.78s |  was actually loading those PyTorch files in an insecure way, which meant that when the PyTorch
1307.78s - 1313.22s |  file was being loaded, you could execute arbitrary code on the server. And then we decided to look a
1313.22s - 1319.14s |  bit further, and we noticed that the spaces, so that container, actually persisted across users.
1319.14s - 1324.34s |  So that meant rather than just messing with my file, I could mess with Chloe's file once it was
1324.34s - 1330.74s |  actually run. And not only that, you could potentially even steal private users' repositories.
1330.74s - 1337.30s |  Because the service allowed you to upload your private key, which would then load in your private
1337.30s - 1342.66s |  files, and then convert them as well. Which means, you know, if Google or some of the other big ones
1342.66s - 1346.18s |  have those private files that they don't want people to know about, I can just steal it and run
1346.18s - 1352.90s |  away with it. And there, people were actually using the service by the time we exploited it. So this
1352.90s - 1358.18s |  one was not exploited, so I want you to make sure that nothing's wrong here. But as you can see
1358.18s - 1364.26s |  here, there is a pull request from sfconvertbot, which was the bot being used. And you can see
1364.26s - 1370.66s |  that it was actually merged into the repository. And when we actually looked at how many people
1370.66s - 1376.74s |  actually downloaded this one file, which, if the attack hadn't been stopped, would have potentially
1376.74s - 1383.46s |  been exploited, we can see that in the last month, there had been 3.8 million downloads of this one
1383.46s - 1390.26s |  file. So imagine if, you know, the 500 plus thousand repositories on Hugging Face were all
1390.26s - 1396.66s |  affected. So these AI infrastructure vulnerabilities are a big deal. We see the same thing with
1396.66s - 1403.06s |  MLOps. So we like to say, you know, people, they rush into things, they try to get things working,
1403.06s - 1407.62s |  and they don't always think about, you know, whether that feature should be in there. And what
1407.62s - 1413.94s |  we're seeing a lot of times in these AI vulnerabilities is that vulnerabilities that we saw
1413.94s - 1419.78s |  years ago are coming back, and we're seeing them again. So obviously, we have our, you know, pickles,
1419.78s - 1424.82s |  our unsafe serialization library. And then to our surprise, there's actually, you know, safe
1424.82s - 1429.70s |  libraries like JSON that are actually being vulnerable, which was, that was a surprise. But
1429.70s - 1435.22s |  we're also seeing missing authentication. So we're seeing some services that are hosted externally
1435.22s - 1440.58s |  that have no authentication at all. We're seeing hard-coded keys, plain text passwords. I mean,
1440.58s - 1444.74s |  the list goes on and on. And I mean, I couldn't even fit it on the page. It goes like all the way
1444.74s - 1449.30s |  down here. But if you are looking for these vulnerabilities, look for the things that you
1449.30s - 1455.22s |  thought were extinct. And then how many of you guys have seen this meme about, you know, any sort
1455.22s - 1462.66s |  of software library? Yeah, okay. So we are seeing the same thing again. And a lot of these AI
1462.66s - 1466.98s |  development frameworks, there's vulnerabilities being found. However, that's not an issue. I mean,
1466.98s - 1471.46s |  we want vulnerabilities to be found, because that means vulnerabilities are being fixed. However,
1471.46s - 1476.18s |  what we're seeing is that once vulnerabilities are being reported to these projects, there's pretty
1476.18s - 1481.86s |  much three things that happen. The first is that as soon as it was reported, the people patched it.
1481.86s - 1487.22s |  However, they still left the unsafe option in there. So that means that you could load it
1487.22s - 1491.30s |  securely if you use the default parameters. However, with a certain optional parameter,
1491.30s - 1497.30s |  you might still be able to load the insecure format. We've also seen that some people decide
1497.30s - 1503.70s |  to patch, add in a way to do it securely. However, that weighs off by default. So you can still be
1503.70s - 1510.50s |  exploited unless you know about and I mean, who here reads documentation? Okay, a few of you. Um,
1510.50s - 1514.82s |  yeah, so you're not gonna know about an AI vulnerability, your vulnerability in the
1514.82s - 1519.46s |  framework. So why would you look I mean, unless you read through the documentation. And then
1519.46s - 1523.94s |  finally, what we're also seeing is that there's some frameworks out there that are just completely
1523.94s - 1530.58s |  ignoring it and saying that it's intended behavior and not an actual vulnerability. So NumPy load is
1530.58s - 1535.62s |  one of the ones from the start. And as you can see, there's an optional parameter called allow
1535.62s - 1540.50s |  pickle. If you set that to true, you can be exploited. However, normally you don't. And thanks
1540.50s - 1544.90s |  to the open source nature of artificial intelligence, we were able to do some searching and
1544.90s - 1551.62s |  we can see that on GitHub, there are 87,000 files that use this insecurely. And that's insecurely
1551.62s - 1558.26s |  not just using it. We also have pytorch load. And what this framework does is there's that
1558.26s - 1564.66s |  insecure load on start. However, you can set the optional parameter for weights only. But as we
1564.66s - 1570.42s |  saw, people do not read documentation and it's gonna make the other number look bad. But uh,
1570.42s - 1577.62s |  573,000 files are using this insecurely. Well, I'm gonna hand it back to Chloe to talk about the
1577.62s - 1587.30s |  open source nature of AI. Thank you, Cass. Alright, let's do this. Alright, isn't that creepy
1587.30s - 1591.38s |  looking, by the way? Like, if I ever open a watermelon and it looks like that, I don't think
1591.38s - 1596.66s |  I'm gonna eat that. Actually, I probably would swear off ever eating watermelon again. Anyway,
1596.66s - 1601.38s |  well, when it comes to verifying open source AI models, it is definitely fraught with so many
1601.38s - 1607.70s |  challenges like inconsistent code quality, hidden security vulnerabilities, and complex licensing
1607.70s - 1612.58s |  issues. There are biases in pre-trained models, of course, as well. And then you have poor
1612.58s - 1619.22s |  documentation and then you have to deal with the rapid value like evolution of AI, which really
1619.22s - 1627.06s |  does further the need of having transparency because right now it isn't transparent. And this
1627.06s - 1633.54s |  is complicating things when it comes to vulnerability management. And yeah, this talk is kind of like
1634.02s - 1641.94s |  ooooh, any Brooklyn Nine-Nine people out here? Okay, okay, okay, I have to ask this quick question.
1643.94s - 1651.30s |  Is Amy Santiago your favorite character? One of them? Okay, thank you, thank you. That, that,
1651.30s - 1656.82s |  that feels great to hear. She's also my favorite. I, I also love his character too, by the way.
1656.82s - 1662.34s |  Alright. So when it comes to open source foundations of AI, the thing is finding these
1662.34s - 1669.86s |  vulnerabilities is, is definitely has some issues because an open source AI system is aided by
1669.86s - 1677.54s |  accessible code and community efforts and specialized tools. However, complex contributions, rapid
1677.54s - 1686.42s |  like, oh my god, I'm always struggling to pronounce this word, uh, evolution. Anyway, and specialized
1686.42s - 1693.38s |  knowledge can introduce security flaws. Frequent updates, dependencies, and gaps in testing further
1693.38s - 1698.18s |  contribute to these risks. And vulnerabilities stem from inadequate reviews as we know of, poor
1698.18s - 1705.78s |  implementation, dependency issues, backdoors, insufficient security practices. As you can tell,
1705.78s - 1710.10s |  there's a lot of issues. It's been the issues we've had with traditional, but now it seems like
1710.10s - 1715.86s |  an extra layer of everything. Because there's rapid development, there's misconfigured models,
1715.86s - 1721.14s |  and now you also have to worry about social engineering as well. These all increase the risks.
1721.14s - 1726.26s |  And to be honest, I think our next talk at DEF CON, and don't anyone steal this idea, okay, I'm
1726.26s - 1731.06s |  just putting it out there, is AI-assisted code reviews. That sounds really interesting because
1731.06s - 1737.22s |  that's emerging right now. Anyway, as you see, this is a long list and it's definitely missing
1737.22s - 1743.06s |  other things probably on this list. But reporting AI vulnerabilities is incredibly tough because it's
1743.06s - 1749.62s |  unclear who owns what and the responsibilities. Not just that, but the technical complexity
1750.26s - 1754.74s |  and the need for specialized knowledge. Of course, we have to worry about legal fears,
1754.74s - 1759.14s |  reputation risk, and coordination issues that tend to complicate the process,
1759.14s - 1764.18s |  while inadequate reporting channels and resource constraints really do discourage thorough
1764.18s - 1769.62s |  reporting. This has been a problem for us at Hidden Layer a few times that we actually had to get
1769.62s - 1776.26s |  CISA involved. And they were wonderful. They helped us get in touch with people so then we
1776.26s - 1782.98s |  could get some serious vulnerabilities patched. So, quick summary, is that AI vulnerabilities
1782.98s - 1788.58s |  are complex and distinct. And the threats, like prompt injections and data poisoning,
1788.58s - 1794.66s |  are out there. And there's rapid developments and reliance on open source frameworks. It introduces
1794.66s - 1799.78s |  all these type of risks. So, managing these vulnerabilities demands improving reporting,
1799.78s - 1805.54s |  but also stronger partnerships, including with CISA and other organizations, especially when it
1805.54s - 1812.74s |  comes to disclosure. So, we do have a call to action. We're all in this together. That's right.
1812.74s - 1817.78s |  We all can take a step forward and do something together. And the call to action is to partner
1817.78s - 1823.62s |  with us. We need to enhance security for AI vulnerability disclosure and ensure comprehensive
1823.62s - 1829.46s |  protection against vulnerabilities. So, we'll be right over here for Q&A. But if you want to get
1829.46s - 1836.90s |  involved with us, do let us know. I know we're out of time. But yeah. Anyway, thank you so much
1836.90s - 1840.90s |  for having us. Thank you, AppSec Village.