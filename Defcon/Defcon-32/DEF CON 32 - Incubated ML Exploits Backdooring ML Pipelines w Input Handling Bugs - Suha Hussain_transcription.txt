{
  "webpage_url": "https://www.youtube.com/watch?v=Z38pTFM0FyU",
  "title": "DEF CON 32 - Incubated ML Exploits: Backdooring ML Pipelines w Input Handling Bugs - Suha Hussain",
  "description": "Machine learning (ML) pipelines are vulnerable to model backdoors that compromise the integrity of the underlying system. Although many backdoor attacks limit the attack surface to the model, ML models are not standalone objects. Instead, they are artifacts built using a wide range of tools and embedded into pipelines with many interacting components.\n\nIn this talk, we introduce incubated ML exploits in which attackers inject model backdoors into ML pipelines using input-handling bugs in ML tools. Using a language-theoretic security (LangSec) framework, we systematically exploited ML model serialization bugs in popular tools to construct backdoors. In the process, we developed malicious artifacts such as polyglot and ambiguous files using ML model files. We also contributed to Fickling, a pickle security tool tailored for ML use cases. Finally, we formulated a set of guidelines for security researchers and ML practitioners. By chaining system security issues and model vulnerabilities, incubated ML exploits emerge as a new class of exploits that highlight the importance of a holistic approach to ML security.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 1781,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.75s - 8.11s | This text was transcribed using whisper model: large-v2

 Hey everyone, I'm Suha. I'm really excited to be here today. So let's get started. I
8.11s - 13.19s |  want to talk to you about ML security. Specifically, I want to talk about this
13.19s - 17.35s |  new class of exploits I identified called incubated ML exploits that
17.35s - 21.19s |  combine backdoors and input handling bugs. Don't worry if you don't know too
21.19s - 27.03s |  much about ML or ML security. I'll explain all the important stuff as we go along.
27.03s - 32.75s |  So who am I and why am I even talking to you today? I'm an engineer at Trail of
32.75s - 37.63s |  Bits where I focus on AI and ML security. I've been in the field for a few years
37.63s - 42.51s |  now. I graduated from Georgia Tech and I'm originally from Queens. Outside of
42.51s - 47.47s |  work, I like Brazilian jiu-jitsu, trying new restaurants, making things, and an
47.47s - 53.91s |  obscure card game called Q-Birds. So it's becoming pretty clear with ML and AI
53.91s - 57.63s |  popping up everywhere, people are figuring out how to trick these systems
57.63s - 61.95s |  based on how these models work. Maybe you've seen someone use prompt injection
61.95s - 66.03s |  to convince a chatbot to give them a refund. Or maybe you've seen this story
66.03s - 70.39s |  right here of protesters tricking self-driving cars with traffic cones.
70.39s - 74.43s |  Notice the fact that this trick is rooted in an understanding of the
74.43s - 80.59s |  training data for these models. So how can we actually construct our own useful
80.67s - 87.56s |  exploits against ML systems? Let's play a game of pretend real quick. You're a
87.56s - 91.28s |  college student and you really, really want the prize money for a robotics
91.28s - 97.48s |  competition. So naturally, you decide to sabotage another team. Side note, I don't
97.48s - 102.44s |  recommend this. I don't condone this. I've never done it myself. But anyway, the
102.44s - 106.76s |  competition requires teams to build a tiny autonomous vehicle that uses a
106.84s - 112.20s |  specific pre-trained model and stops at stop signs. So you find out that some of
112.20s - 116.04s |  these stop signs have these little stickers on them. And you also find some
116.04s - 119.96s |  flaws with how they've stored and distributed the model. Which, by the way,
119.96s - 125.28s |  isn't out of the question. ML artifacts are often widely shared
125.28s - 129.92s |  without any meaningful or substantial trust mechanisms. So you decide to grab
129.92s - 134.80s |  that file and inject a model backdoor in it using a file format RCE of some kind.
135.28s - 139.56s |  And then you put it back. Then on the day of the competition, you sit back and you
139.56s - 144.16s |  just watch as your competitor's robot, your competitor's vehicle just plows
144.16s - 148.80s |  through and ignores every stop sign with a sticker on it. What you just did is
148.80s - 155.20s |  execute an incubated ML exploit, which is what my talk is all about. So obviously,
155.20s - 159.68s |  the stakes of this story is just a lost robotics competition. But the idea of
159.68s - 164.72s |  attacking a real autonomous vehicle is a hallmark of model backdoor research. And
164.72s - 169.20s |  you can see that with the image on the left. So I'll let you use your imagination
169.20s - 174.84s |  to raise the stakes. So in my talk, first I'm going to tell you about this
174.84s - 179.28s |  framework that I've been using to bridge the gap between model and system security.
179.28s - 184.16s |  Because we can't continue to treat models as standalone objects. Next, I'll tell you
184.16s - 188.52s |  about these input handling bugs I found in model serialization and connect them
188.52s - 194.12s |  to backdoors. I'll do that by taking a page out of this subfield called LangSec.
194.12s - 197.92s |  So effectively, I'll be going through a bunch of examples of incubated ML
197.92s - 204.04s |  exploits and use LangSec to organize them. But first, I need to explain some stuff.
204.04s - 220.78s |  What even is a model vulnerability or an ML backdoor? So super briefly, you can
220.78s - 225.14s |  think of ML models as these squishy, flexible sequences of linear algebra
225.14s - 229.46s |  operations that are trained on tons and tons of data. There's a pretty popular
229.46s - 233.74s |  saying, all models are wrong, but some are useful. We're just saying that these
233.74s - 237.38s |  models aren't perfect. There are many different ways they can mess up or get
237.38s - 241.50s |  tripped up by something that might be unexpected to us. And that's the basis of
241.50s - 245.90s |  such model vulnerabilities. While popular examples of model vulnerabilities include
245.90s - 250.58s |  model inversion and membership inference, we're zooming in on one specific type,
250.58s - 256.58s |  model backdoors. So to be precise about it, a backdoor attack allows a malicious
256.58s - 261.42s |  actor to force an ML model to produce specific outputs given specific inputs.
261.42s - 264.94s |  Now, there's a couple of things that I think make backdoors really
264.94s - 269.14s |  interesting to study. First up, you can use them as primitives for other model
269.14s - 272.38s |  vulnerabilities, like membership inference. You can also identify
272.38s - 276.50s |  pre-existing, quote-unquote, natural backdoors in them. And there's also some
276.50s - 281.18s |  pretty strong evidence that suggests that this is an inherent threat.
281.18s - 285.54s |  Now, while there's a lot of awesome research on ML model attacks, they can
285.54s - 288.98s |  actually be pretty hard to exploit in the real world, with some exceptions,
288.98s - 292.98s |  of course. While there are multiple reasons for this, one thing that really
292.98s - 300.62s |  sticks out to me is the big gap between research and the real world. So for the
300.62s - 304.42s |  most part, many attacks and attack frameworks and tools restrict their
304.42s - 309.74s |  analysis to this formulation. An ML model receives an input and produces an
309.74s - 313.74s |  output. But this isn't an accurate representation of what an ML system
313.74s - 319.22s |  actually looks like. There's so much more going on in practice. Here's a software
319.22s - 323.54s |  architecture diagram for an ML system reviewed by Trail of Bits recently. This
323.54s - 327.94s |  is a system that uses the Ask Astro tool for RAG, and I've circled where the model
327.94s - 331.98s |  actually is in this photo. Do you see what I mean? We need to be looking at all
331.98s - 336.50s |  of this holistically. There's a large and evolving landscape of tools
336.50s - 342.54s |  being used in and for ML systems, and that brings me to the exploit framework.
342.54s - 347.98s |  So the title of my talk references an incubated ML exploit, but there's this
347.98s - 352.94s |  larger category of exploits called hybrid ML exploits that are important to
352.94s - 359.10s |  think about first. Specifically, a hybrid ML exploit chains a system security
359.10s - 363.02s |  issue with a model vulnerability. This can go in either direction, and you can
363.02s - 366.78s |  see that on the diagram. You can have a model vulnerability that exposes a
366.78s - 371.26s |  system security issue, or you could use a system security issue to exploit a model
371.26s - 377.34s |  vulnerability. So this part's pretty important. The big issue that I see with
377.34s - 381.90s |  ML security is that model security and system security are treated separately.
381.90s - 386.38s |  But what I need you to understand is that if you only know model
386.38s - 390.70s |  security, you're missing a big piece. And if you're only covering system
390.70s - 394.34s |  security, you're still missing a big piece. And if you're treating the two
394.34s - 398.58s |  processes completely independently, once again, you're missing a big
398.58s - 403.58s |  piece. You're then entirely ignoring the potential for hybrid ML exploits. This is
403.58s - 408.54s |  an emergent property. So your model is embedded in a system, and it's going to
408.54s - 412.42s |  interact with all of the different system components in new and exploitable
412.42s - 416.62s |  ways. So one thing you'll notice is that there's a lot of screenshots of paper
416.62s - 420.82s |  titles on this slide. That's because there have been specific instances of
420.82s - 425.30s |  hybrid ML exploits in the literature and in practice. They're just not called that
425.30s - 429.66s |  explicitly. Exploitable software gadgets have been used for backdoors. The
429.66s - 433.62s |  Summoning Demons paper up there at the top chained model evasion and memory
433.62s - 437.98s |  corruption. And the Learn System Security paper next to it has an example of a
437.98s - 442.34s |  poisoning attack that caused an exponential memory blow-up in an index
442.34s - 448.22s |  structure. But the ML security literature framework and tools are largely limited
448.22s - 452.90s |  to just that. Specific instances or implications. What I'm trying to do here,
452.90s - 456.54s |  what I want to be doing here, is treating this interaction explicitly and
456.70s - 462.38s |  systematically. And that's why I made this framework. So one kind of system
462.38s - 467.58s |  security issue is an input handling bug, and one kind of model vulnerability is a
467.58s - 474.30s |  model backdoor. Put that together and we get an incubated ML exploit, which
474.30s - 478.82s |  is a type of hybrid ML exploit where an attacker uses input handling bug to
478.82s - 483.10s |  inject a backdoor. So I made this diagram to make the distinction between the two
483.10s - 488.26s |  pretty clear. And here's the definition again. I'm going to leave the framework
488.26s - 492.90s |  here for now. We did end up going into a bit of a more formal model of
492.90s - 499.30s |  exploitation, including this exploit schema. But we'll return to that later. So
499.30s - 503.22s |  to backdoor a pre-existing model, the attacker should be able to change the
503.22s - 507.58s |  parameters or the architecture. Now at the level of extraction we're dealing
507.58s - 512.10s |  with, we can put input and component manipulation on the side for now. But how
512.10s - 517.22s |  this actually plays out can vary a lot. For example, sometimes the
517.22s - 521.54s |  attacker has control over some element of the training process, and they use
521.54s - 525.18s |  that to sneak in some manipulated data that changes the
525.18s - 529.86s |  model's parameters. That's usually called data poisoning. Or they might go a step
529.86s - 535.34s |  further and fiddle with the model source code to change the architecture. Now
535.34s - 539.42s |  before we talk about exploits, I want to explain a few things about input
539.42s - 554.42s |  handling bugs. So an ML model is stored as a file. To process these models, you
554.42s - 559.30s |  need parsers. And parsing these files into objects and back is deserialization
559.30s - 566.82s |  and serialization. But wait. Quoting Ange Albertini here, a file has no intrinsic
566.82s - 571.70s |  meaning. The meaning of a file, its type, its validity, its contents, can be
571.70s - 576.26s |  different for each parser or interpreter. Now this is the reason we can make cool,
576.38s - 580.94s |  potentially malicious file artifacts like polyglots and ambiguous files, which
580.94s - 586.34s |  I'll talk more about later. So I'm focused very specifically on bugs that
586.34s - 590.74s |  arise when you parse ML model files. There's also cool bugs in other parts of
590.74s - 594.62s |  the pipeline, but I'm picking ML model files for several reasons. The first
594.62s - 598.70s |  reason is, and I'm sure you all will agree with me on this, the most
598.70s - 604.22s |  important. I think it's fun. But more seriously, the security of ML file
604.26s - 609.42s |  formats has become increasingly important. Real malicious model files have
609.42s - 612.98s |  been found on the Hugging Face Hub, for example. And there's also just an
612.98s - 617.26s |  absolute ton of ML file formats out there. I've tried to list and organize
617.26s - 620.42s |  them in the repository in the middle, but what's important for you to take
620.42s - 624.10s |  away is that there's a large set of possibilities for these exploits, as well
624.10s - 627.98s |  as just fun hacks with these formats. And there's already a lot of great work
627.98s - 633.94s |  in this area, as shown on my slide. So file format tricks are within the
633.94s - 637.98s |  realm of LangSec, but this field actually thinks more abstractly about
637.98s - 642.70s |  inputs as a general class. LangSec applies formal language theory to system
642.70s - 647.30s |  security. It focuses on exploring input handling bugs or parser problems as a
647.30s - 651.38s |  big root cause for security issues. After all, lots of impactful
651.38s - 654.90s |  vulnerabilities like Heartbleed and Android Master Key have been parser
654.90s - 659.94s |  bugs. Now, while I like formal language theory, this talk isn't Theoretical
659.94s - 665.82s |  Computer Science 101. So just know that fundamentally what LangSec is saying
665.82s - 670.02s |  is, hey, let's treat all of the inputs as a specific language and make our
670.02s - 676.46s |  code just capable enough to understand that language properly. So my work is
676.46s - 680.54s |  centered around a specific taxonomy of input handling bugs, and these are all
680.54s - 684.42s |  the different bug classes. There are eight different types. Quick note, these
684.42s - 687.90s |  categories aren't actually completely distinct from each other. The one you
687.90s - 692.62s |  choose comes from a root cause analysis. So with the exception of one, I'm going
692.62s - 696.70s |  to show you multiple examples of each in ML tools and use them to build an ML
696.70s - 703.50s |  backdoor. So I'm going to run that one more time. In order to show
703.50s - 708.26s |  that input handling bugs are a vector, I identified issues with ML model
708.26s - 712.14s |  serialization across these different bug classes in order to construct ML
712.14s - 718.94s |  backdoors. So now we can dive into the most fun part, exploits. Now, for the
718.94s - 732.79s |  sake of time, I'm going to focus more on the useful gadgets for these exploits. So
732.79s - 736.67s |  these are some characters that play important roles in the ML ecosystem that
736.67s - 743.07s |  can help us understand the impact of exploits, of these exploits. So Alice, she
743.07s - 748.59s |  distributes models. She takes open source LLMs and fine-tunes them. The model she
748.59s - 753.59s |  distributes are what everyone else in our story is going to be using. Bob is a
753.59s - 757.91s |  frontline user who directly uses Alice's models in his own life. Maybe he has a
757.91s - 763.35s |  nice chat interface pulled up. There's Dave. Dave is a developer who's trying to
763.35s - 768.47s |  integrate these models into products. Frank is the end user who's relying on
768.47s - 772.67s |  Dave's products in his daily life. He's often, he might be unaware of the ML
772.67s - 779.39s |  models working behind the scenes. And now we have Chuck. Chuck is the attacker.
779.39s - 785.63s |  Our focus here will mainly be on how Chuck can impact Bob and Dave. So I'll
785.63s - 789.95s |  show, I'll describe some exploits involving the file formats associated
789.95s - 797.03s |  with PQL, PyTorch, TorchScript, ONNX, and SafeTensors. So this first category is
797.03s - 801.87s |  called non-minimalist input handling code. It sounds a little fancy, but all it
801.87s - 806.11s |  means is that the code used to check and parse these inputs is too complex. So an
806.11s - 811.23s |  attacker can potentially grab the necessary gadgets for their exploits. So
811.23s - 815.63s |  pickling is a serialization method that allows you to save arbitrary objects and
815.63s - 820.07s |  it's very, very common in the ML ecosystem. Recently, my co-worker Bojan
820.07s - 823.75s |  Milanov led the development of SleepyPickle, which is an incubated ML
823.75s - 829.35s |  exploit. And what it does is it chains a pickle RCE with model backdoors. So on
829.35s - 833.91s |  the right, you can see an LLM that has been backdoored to Phish users. The blog
833.91s - 838.39s |  post also has some examples of an LLM being used to
838.39s - 842.99s |  spread misinformation and even steal user data. But what's cool about this
842.99s - 847.15s |  exploit is that it can happen on the fly. So there's more room and possibilities
847.15s - 853.11s |  for an attacker than just uploading a malicious model. So what do I mean by
853.11s - 857.79s |  pickle RCE? Python pickles are compiled programs that run in a unique virtual
857.83s - 862.35s |  machine called a pickle machine, or PM for short. The PM interprets the pickle
862.35s - 866.83s |  file sequence of opcodes to construct an arbitrarily complex Python
866.83s - 872.99s |  object. But it has two opcodes, global and reduce, that can execute arbitrary code
872.99s - 876.87s |  outside of the PM, which makes it possible to construct malicious pickle
876.87s - 881.11s |  data. Now the underlying issue here is that the PM is more complex than
881.11s - 887.55s |  something that's only parsing ML models should be. So way back in the year 2021,
887.83s - 892.67s |  we released this tool called Fickling. This project was led by Evan Sultanik. To
892.67s - 896.47s |  our knowledge, Fickling was the first pickle security tool tailored for ML use
896.47s - 901.63s |  cases. It's a decompiler, static analyzer, and bytecode rewriter for Python pickle.
901.63s - 906.39s |  So it can help you detect, analyze, and even create malicious pickle files. So the
906.39s - 909.79s |  reason it's safe to run on potentially malicious files is because it
909.79s - 914.99s |  symbolically executes code using its own PM implementation. Relatively recently, I
915.11s - 918.91s |  added a PyTorch module to it to make it easy to statically analyze and inject
918.91s - 923.75s |  code into PyTorch files. But pickles are clearly bad for Bob. If Alice is
923.75s - 927.43s |  distributing models as pickle files, that makes it that much easier for Chuck to
927.43s - 934.39s |  inject a backdoor using a pickle RCE. So on to the next class. This term just
934.39s - 939.39s |  means you shouldn't try to correct invalid input, reject it altogether. I've
939.39s - 947.15s |  heard it referred to as the anti-robustness principle. So to deal with the issues
947.15s - 951.59s |  with pickling, many devs write these things called restricted unpicklers. Now,
951.59s - 955.71s |  those are subclasses of unpickler that try to enforce an allow list or a block
955.71s - 959.79s |  list. The thing is, they're actually not that hard to bypass. There's this
959.79s - 963.91s |  methodology called PainPickle that demonstrates how to automatically bypass
963.91s - 968.79s |  restricted unpicklers, which would enable arbitrary code execution and, to that
968.83s - 973.59s |  end, backdoors. So they identified eight different types of unpicklers and three
973.59s - 977.43s |  strategies that work against the vast majority of them. So much like pickle was
977.43s - 981.67s |  bad for Bob, restricted unpickling bypasses pose a problem for Dave, if he
981.67s - 988.19s |  relies on them in some fashion. Now we can talk about parser differentials. So
988.19s - 991.95s |  this happens when different parsers in a system read the same input but
991.95s - 997.23s |  interpret it differently. So when two parsers interpret the same file in
997.23s - 1001.91s |  different ways, that file is known as an ambiguous file. This is a very common
1001.91s - 1006.15s |  exploit technique. It's pretty good for bypasses. It means you can create an ML
1006.15s - 1011.15s |  model file that is benign for one system or system component but backdoored for
1011.15s - 1015.55s |  another. There's some more bigger implications for ML system exploitation
1015.55s - 1019.07s |  here that we'll talk a bit more about later. But just a quick note, whether or
1019.07s - 1022.99s |  not this is impactful at all depends on your system. So that's where you want to
1022.99s - 1028.63s |  do threat modeling, of course. So we actually were able to create two
1028.63s - 1032.55s |  differential proof of concepts with TorchScript. TorchScript is a popular
1032.55s - 1036.31s |  format to store ML models in for a couple of reasons, mainly performance and
1036.31s - 1040.91s |  portability. But you can make a parser differential with it and chain it to an
1040.91s - 1045.43s |  architectural backdoor. So that's because you can turn a PyTorch model into a
1045.43s - 1049.59s |  TorchScript one through tracing or scripting. And tracing doesn't incorporate
1049.59s - 1053.23s |  dynamic control flow. So all you have to do is represent the malicious
1053.23s - 1057.87s |  components for the backdoor with dynamic control flow. Now the second example was
1057.87s - 1062.91s |  found during the YOLO audit. Last year my team and I audited a popular open
1062.91s - 1066.91s |  source code base for computer vision called YOLOv7. They released standard
1066.91s - 1070.99s |  versions of their model and TorchScript versions for deployment. We noticed many
1070.99s - 1075.39s |  cases where tracing didn't capture the model accurately. After serialization and
1075.39s - 1079.35s |  deserialization, key information was lost and the usual PyTorch warnings
1079.35s - 1083.23s |  didn't show up. So to spot this differential, we used the TorchScript
1083.23s - 1088.95s |  automatic trace checker, TorchFX, and the TorchScript IR. So with what we found,
1088.95s - 1092.99s |  we created an input that made the two versions of the model act differently.
1092.99s - 1097.43s |  Effectively a backdoor attack. So once again, bad for Bob. He's getting a
1097.43s - 1101.03s |  fundamentally different model than the one Alice trained. So this breaks any
1101.03s - 1107.11s |  preexisting promises. So we also identified a parser differential with
1107.15s - 1111.67s |  SafeTensors. SafeTensors is another file format for ML models developed in
1111.67s - 1116.35s |  response to the insecurity of pickling. Last year I was on an audit of the
1116.35s - 1121.11s |  SafeTensors library where we identified the inclusion of JSON in the file format
1121.11s - 1125.03s |  as a source of parser differentials. Now JSON's pretty well known to be
1125.03s - 1128.63s |  underspecified. There's a bunch of exploits in the web security world that
1128.63s - 1132.55s |  leverage this. But the thing is, the reference SafeTensors implementation,
1132.55s - 1136.39s |  the main implementation, uses a cert parser, which is good. It's strict. It
1136.39s - 1139.99s |  rejects duplicate keys. But there's a bunch of external tools that use the
1139.99s - 1144.51s |  Python built-in JSON parser, which doesn't. So you can use a duplicate key
1144.51s - 1148.31s |  for the offsets to append backdoored weights and create manipulated SafeTensors
1148.31s - 1152.91s |  files that are rejected by the reference implementation but accepted by external
1152.91s - 1156.43s |  parsers. It has to be a weights-based backdoor because the weights and
1156.43s - 1159.91s |  architecture are stored separately here. And there's a bit more details and
1159.91s - 1163.67s |  caveats regarding the exploitability here. But just know the SafeTensors
1163.67s - 1167.67s |  parser differential is more impactful for Dave. He needs to make sure all the
1167.67s - 1171.79s |  parsers in his product agree. If his tool is using a more permissive SafeTensors
1171.79s - 1175.75s |  parser than the reference implementation, it might just accept manipulated
1175.75s - 1182.07s |  SafeTensors files that have backdoored models. So one big part of my research is
1182.07s - 1186.55s |  analyzing previous work and noticing trends. Now I don't want to get too into
1186.55s - 1190.15s |  the weeds here. I'd like to say formalisms for accompanying materials.
1190.75s - 1194.71s |  But one thing that stuck out to me is that from parser differentials, we get
1194.71s - 1198.67s |  these things called model differentials. Instances where the same model is
1198.67s - 1203.39s |  interpreted differently. As expected, this attack is dependent on the supply chain
1203.39s - 1209.83s |  component and lifecycle stage. But in an ML system, you can pre-process inputs or
1209.83s - 1213.91s |  you can apply model transformations before you deploy a model. So some
1213.91s - 1217.43s |  studies have exploited parser differentials right at the pre-processing
1217.43s - 1221.75s |  stage. These are things like image scaling or Unicode parsing. And those
1221.75s - 1225.63s |  attacks often change the weights. There have also been backdoor attacks that
1225.63s - 1230.35s |  take advantage of model transformations like compilation or quantization. And
1230.35s - 1234.75s |  those usually change the architecture. So I think it's very possible that most
1234.75s - 1238.99s |  transformations can be encoded in a loss function that can be encoded in a loss
1238.99s - 1242.91s |  function can result in an exploitable backdoor. But let's move forward from
1242.91s - 1260.23s |  here. Next up, we have shotgun parsing. This is just what happens when you don't
1260.23s - 1265.75s |  fully and properly check your input before beginning to process it. So
1265.75s - 1270.03s |  let's talk about polyglot files. These are files that can be validly interpreted
1270.03s - 1273.43s |  as two or more different formats. Polyglot files have been used to
1273.43s - 1277.07s |  distribute malware, bypass code signing checks, and enable other malicious
1277.07s - 1281.31s |  behaviors. So for ML model serialization, you can take these, put them in model
1281.31s - 1285.91s |  hubs, and confuse some downstream consumers. But more importantly, you can
1285.91s - 1289.19s |  have two different ML pipelines that interpret the same file as
1289.19s - 1294.59s |  different models. So you can smuggle a backdoor model in with a benign one. So
1294.59s - 1298.43s |  during our audit of the SafeTensors library, we were able to make multiple
1298.43s - 1304.11s |  polyglots. This includes zip, PDF, TF records, Keras native, and later on PyTorch
1304.11s - 1309.07s |  Marm. And the SafeTensors audit report itself is a PDF zip
1309.07s - 1312.99s |  polyglot, with a zip file containing all the polyglots we made during the audit.
1312.99s - 1317.19s |  So you can just slap on a waste-based backdoored model to one of these formats
1317.19s - 1321.51s |  to a benign SafeTensors model, open it up with SafeTensors, everything's fine,
1321.51s - 1325.79s |  load it up with like PyTorch Marm or some other system, and boom, there's your
1325.79s - 1330.23s |  backdoor. Now this is a real problem for folks like Dave, who's depending on these
1330.23s - 1333.71s |  models. Because now you've got malicious models sneaking in with benign ones,
1333.71s - 1338.23s |  right? The reason this is possible is because of a missing check. Specifically,
1338.23s - 1342.03s |  the program didn't check whether the start and end offsets corresponded with
1342.03s - 1345.95s |  the tensor size, so attackers could append arbitrary data to the file. And
1345.95s - 1349.35s |  that can be combined with the ability to change the header size to expand the
1349.35s - 1353.11s |  number of polyglots. Now this issue has since been fixed with SafeTensors,
1353.11s - 1360.18s |  however. So our next category is incompletes protocol specification. Just
1360.18s - 1363.82s |  think of it as underspecification for now. There are multiple examples of this
1363.82s - 1366.90s |  in the literature, but for the sake of time, I'm just going to talk about PyTorch
1366.90s - 1372.98s |  polyglots. So many people are unaware that PyTorch actually supports multiple
1372.98s - 1377.90s |  file formats. Some are deprecated, but still supported by external parsers. Now
1377.90s - 1381.78s |  one issue is that it does lack consistent versioning here. So it's not
1381.78s - 1385.22s |  that difficult to create polyglots of files that can be validly interpreted as
1385.22s - 1391.14s |  different types of PyTorch file formats. So same for ambiguous files, side note. So
1391.14s - 1396.50s |  you can just like add three files to get a polymock between version 1.3 and
1396.50s - 1400.58s |  TorchScript version 1.4, but a bigger issue is the reliance on zip and pickle.
1400.58s - 1405.98s |  So pickle is a streaming file format. It ends once it reaches the stop code. So
1405.98s - 1411.06s |  you can validly just append arbitrary data to it. Now on the other hand, most zip
1411.06s - 1415.50s |  parsers don't enforce their magic at the start. So you can prepend to it, and one
1415.50s - 1419.62s |  example of this is PyTorch more. So you can append a zip to a pickle file to
1419.62s - 1423.10s |  create a zip pickle polyglot, and that gives you some good PyTorch polyglots.
1423.10s - 1428.26s |  So fickling now has a polyglot module, so you can differentiate, identify, and
1428.26s - 1434.46s |  create polyglots for the different PyTorch file formats. Now on to the next
1434.46s - 1438.82s |  class. This one just means your input should be simple and well-defined, so you
1438.82s - 1443.66s |  can check it thoroughly. Take ONNX. It's a protobuf-based way to store ML
1443.66s - 1448.86s |  models. Adeline Travers just discovered a neat hack for the ONNX runtime. He
1449.30s - 1453.94s |  into a tool called Lobotomy. So ML runtimes and frameworks often let you add
1453.94s - 1459.86s |  custom ops to a model on the fly. And the language used for the ONNX runtime
1459.86s - 1465.74s |  custom ops at that point in time was complex. So even though the ONNX
1465.74s - 1470.06s |  protobuf specification officially disallowed side effects, in the ONNX
1470.06s - 1474.54s |  runtime, you can encapsulate arbitrary code into a custom op, and you can use
1474.58s - 1479.22s |  that to launch an architectural backdoor attack. Just like Pickle, this is not
1479.22s - 1485.54s |  good news for Bob. So to recap, Bob, our direct consumer, was affected by the
1485.54s - 1490.46s |  Pickle, ONNX, and TorchScript issues. Dave, on the other hand, was affected by
1490.46s - 1496.26s |  the PyTorch, SafeTensors, and restricted and pickling issues. Now, what a lot of
1496.26s - 1501.26s |  people miss is how important and how complex the ML stack is. The model you
1501.26s - 1505.66s |  choose changes the technologies in the stack. So whenever I'm assessing a system
1505.66s - 1508.94s |  or doing some kind of vulnerability research, I'm always trying to think
1508.94s - 1513.46s |  about what layer of the ML stack I'm dealing with. So the layers I have listed
1513.46s - 1518.02s |  here are hardware, infrastructure, low-level, compiler, high-level,
1518.02s - 1523.38s |  framework, model, and knowledge. And I just described a bunch of exploits, right?
1523.38s - 1527.86s |  So of the ones I told you about, the ones that are exposed and impactful at the
1527.86s - 1532.66s |  framework level are the restricted unpickler, ONNX runtime, and Pickle
1532.66s - 1537.06s |  exploits. Now, the compiler level corresponds to the TorchScript
1537.06s - 1541.78s |  differential, and the SafeTensors and PyTorch polyglot issues are impactful at
1541.78s - 1545.38s |  the infrastructure level. And this is just a starting point. There's going to
1545.38s - 1550.10s |  be exploits up and down this stack that impact ML systems. So if you want to get
1550.10s - 1554.18s |  into attacking ML systems, this is a good place to start. Are you really, really
1554.26s - 1559.10s |  good at breaking hardware? Go take a stab at a TPU. Do you happen to know a lot
1559.10s - 1563.58s |  about distributed system security? Go write some hybrid ML exploits at the
1563.58s - 1570.37s |  infrastructure level. So I made this schema for incubated ML exploits. This is
1570.37s - 1574.85s |  just one piece of a more formal model of exploitation I worked on. I'm just going
1574.85s - 1578.21s |  to talk about this at a very high level to shed some light on the terrain here.
1578.21s - 1582.45s |  If you want to pull off an incubated ML exploit, you want right primitives for
1582.45s - 1586.21s |  the weights or the architecture, right? And with the proof of concepts, we've
1586.21s - 1590.33s |  seen some additional useful capabilities. So side note, you probably want read
1590.33s - 1595.17s |  primitives as well. But with the SafeTensors parser differential, you saw
1595.17s - 1601.37s |  that access to the metadata could enable both kinds of attacks. And you also saw
1601.37s - 1604.61s |  that there's a lot of utility in exploiting model transformations and
1604.61s - 1608.65s |  model differentials. With that, you can construct exploits at different stages of
1608.65s - 1614.69s |  the pipeline that exploit existing procedures. So with ONNX, it also became
1614.69s - 1618.41s |  pretty obvious that you can use malicious custom ops in serialization
1618.41s - 1622.69s |  formats and potentially even in places like compiler dialects. I'll release more
1622.69s - 1627.89s |  details on this in accompanying materials. But I do want to make some
1627.89s - 1632.81s |  more explicit recommendations. I apologize for the busy slide here.
1632.81s - 1636.33s |  Models should be checked for integrity and their metadata should be
1636.37s - 1641.29s |  well parsed. We want good trust mechanisms. We want proper validation. We
1641.29s - 1645.01s |  want to minimize complexity. So we should be avoiding custom ops and
1645.01s - 1648.69s |  separating the weights and architecture storage. I also think we should be doing
1648.69s - 1652.45s |  a better job of following recommended practices for file formats. Like they
1652.45s - 1655.45s |  should have versions and checksums and magic signatures. And they should enforce
1655.45s - 1659.29s |  the signature at offset zero. And we really need to invest in just more
1659.29s - 1665.13s |  robust specifications and tooling. So I'm hoping we can see hybrid ML exploits
1665.13s - 1669.09s |  and incubated ML exploits addressed by more frameworks and tools. I'd love to
1669.09s - 1673.49s |  see this framework evolve and be applied to specific ML tools and contexts. I want
1673.49s - 1678.13s |  to see it use more bug classes and more model vulnerabilities. I'd also like to
1678.13s - 1682.01s |  see people investigate exploit persistence, reliability, and defense more.
1682.01s - 1685.77s |  And just generally, I think there's so much interesting work to be done here
1685.77s - 1690.29s |  with ML infrastructure security, with model differentials, and file formats, and
1690.29s - 1695.69s |  specifications, and even reverse engineering. But before we finish, what
1695.69s - 1700.29s |  helps me identify and make progress on ML security problems is understanding
1700.29s - 1704.93s |  these two root causes. First, we're building all of these new systems for ML.
1704.93s - 1709.49s |  New hardware, new programming languages, new compilers, new file formats. There are
1709.49s - 1713.85s |  conferences dedicated just to new ways to design ML infrastructure. And that
1713.85s - 1717.83s |  means all of these new systems are introducing new attack surfaces. At the
1717.83s - 1721.43s |  same time, it's becoming increasingly clear that the ML stack and supply
1721.43s - 1724.19s |  chain have not been subject to sufficient review. That's why we're
1724.19s - 1728.91s |  seeing pickles everywhere, right? Second, simply placing an ML model into a
1728.91s - 1732.87s |  program introduces all of these new vulnerabilities that stem from how the
1732.87s - 1737.19s |  model interacts with different components. ML is not a quick add-on, but
1737.19s - 1740.21s |  something that can and will fundamentally change your system
1740.21s - 1746.24s |  security posture. So I hope you leave this talk knowing that we need to
1746.24s - 1749.80s |  concurrently and holistically think about system security and model security.
1749.80s - 1753.96s |  I really recommend checking out the full audit reports for Safe Tensors and YOLO,
1753.96s - 1758.88s |  as well as the blog posts on Fickling and the file formats repo. I'll post more
1758.88s - 1762.24s |  details and accompanying materials, and we're hoping to release a paper on this
1762.24s - 1766.32s |  topic as well. You can find my contact info on my website, or just send me a
1766.32s - 1770.72s |  message on Twitter. But thank you all for coming. Thank you all for listening. If
1770.72s - 1775.36s |  you have any questions, feel free to just come up to me.