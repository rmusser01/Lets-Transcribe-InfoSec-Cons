**Machine Learning Security**

- **Machine Learning (ML) pipelines** are vulnerable to **model backdoors** that compromise system integrity.
- ML models are not standalone; they are part of pipelines with many interacting components.

**Incubated ML Exploits**

- Introduced by injecting model backdoors using **input-handling bugs** in ML tools.
- Utilizes the **language-theoretic security (LangSec) framework** to exploit ML model serialization bugs.
- Developed malicious artifacts such as **polyglot and ambiguous files**.
- Contributed to **Fickling**, a pickle security tool tailored for ML use cases.

**Model Backdoors**

- Allow a malicious actor to force an ML model to produce specific outputs from specific inputs.
- Can be used as primitives for other model vulnerabilities like **membership inference**.
- Pose an inherent threat with strong evidence suggesting their potential.

**Hybrid ML Exploits**

- **Chains system security issues with model vulnerabilities.**
- Can lead to **model vulnerabilities exposing system security issues**, or vice versa.
- The gap between model security and system security needs to be bridged for comprehensive ML security.

**Input Handling Bugs**

- **Model files are parsed into objects**, and parsing errors can lead to security vulnerabilities.
- Focused specifically on bugs arising from parsing ML model files.
- **Cool bugs** also exist in other parts of the pipeline.

**LangSec Approach**

- Applies **formal language theory** to treat inputs as a specific language.
- Focuses on **input handling bugs** or parser problems as root causes for security issues.

**Exploit Examples**

- **SleepyPickle**: An incubated ML exploit chaining a pickle RCE with model backdoors.
- **Parser Differentials**: Different parsers interpreting the same file differently.
- **Polyglot Files**: Files that can be interpreted as multiple formats, creating security risks.

**Security Recommendations**

- Models should be **checked for integrity**, and their metadata should be well-parsed.
- **Trust mechanisms** and proper validation should be established.
- Avoiding custom ops and separating weights and architecture storage can minimize complexity.

**Conclusion**

- ML systems introduce new attack surfaces and vulnerabilities due to their interaction with different components.
- A holistic approach to ML security, considering both system and model security, is essential.