{
  "webpage_url": "local:1731820006:699971c3:DEF CON 32 - Crypto Privacy Village - Differential privacy beyond algorithm - Challenges for successful deployment - Professor Rachel Cummings.mp4",
  "title": "DEF CON 32 - Crypto Privacy Village - Differential privacy beyond algorithm - Challenges for successful deployment - Professor Rachel Cummings.mp4",
  "description": "Local file",
  "channel_url": null,
  "duration": null,
  "channel": null,
  "uploader": null,
  "upload_date": null
}

0.00s - 6.72s | This text was transcribed using whisper model: large-v2

 All right, very fast intro. Welcome to, well, Creator Stage is one, but CPB's over that way
6.72s - 13.36s |  right afterwards. But go to our second talk first, over here, again, afterwards. But welcome,
13.36s - 21.20s |  and this is Professor, or Rachel. We're thrilled to have her. She's a professor, once again,
21.20s - 26.16s |  at Columbia University, and we are so excited for this talk. I got a chance to see these slides
26.16s - 30.88s |  last night. You're going to walk away thinking I know everything about differential privacy.
32.08s - 38.96s |  So, once again, here you go. Thank you. Thank you. We made it.
41.28s - 47.52s |  This is my first DevCon. Super excited to be here, all of that. Thank you for your extreme patience
47.52s - 52.96s |  as we got things going. As a result, I'm going to have to skip some things in my talk so that
52.96s - 58.40s |  we can end on time. But I will be hanging out afterwards at the Crypto Privacy Village. And so,
58.40s - 65.92s |  come see me and say hi. I'm giving a surprise talk there at 1.30, which I think the surprise
65.92s - 74.16s |  will be the slides that I skipped. So, come and see that. Great. So, my work is really about
74.16s - 78.72s |  differential privacy. If you haven't heard of it, you might be an expert by the end of this talk.
78.72s - 90.48s |  It's okay. You have to be over here. Oh, no. Okay. So, the mental model you should think about
90.48s - 95.28s |  as we talk about differential privacy should be something about there's a database, it's
95.28s - 100.64s |  important, and we want to analyze it and do scientific business analytics, whatever, on it.
101.44s - 106.00s |  And there's an analyst who's going to send some queries, receive back a few answers.
106.00s - 112.40s |  This process is going to continue. And so, she says, I've made some scientific discovery.
112.40s - 120.00s |  Maybe it's like, you know, this drug works. Maybe it's here's a database that can be freely
120.00s - 125.52s |  published that I have made as a function of the original database such that if you were to
125.52s - 131.28s |  evaluate all my same answers, you would get back approximately the same answer on this new database.
132.48s - 138.16s |  And you might want some kind of like a privacy preserving barrier at some point in this process.
138.16s - 143.12s |  And so, maybe it's between the database and the analyst if you don't fully like trust this
143.12s - 148.64s |  analyst. Maybe it's between these sort of analysts and the outputs. If the outputs are going to be
148.64s - 162.75s |  like publicly viewed, maybe you also want some like boundaries there. And the model that I think
162.75s - 169.23s |  of in my work is a mental model about privacy promising people freedom from harm. Nothing bad
169.23s - 175.95s |  is going to happen to you because you have shared your data with this analysis process.
176.51s - 183.95s |  Informally, we're going to say, or like informally, some analysis of a data set is going to be
183.95s - 192.27s |  private if an analyst can know almost no more about Alice afterwards than he would before.
193.23s - 200.75s |  Or, sorry, if he had instead done like the same analysis on the same data set without Alice's
200.75s - 206.11s |  data. And this allows us to learn things like, you know, smoking causes cancer. But it doesn't
206.11s - 218.45s |  allow us to learn things about your specific health information. So, the, so the like a
218.45s - 225.25s |  differential privacy definition visually looks like this. Imagine if I have a database containing
225.33s - 232.21s |  data coming from lots of people and I plug that into my favorite randomized algorithm that will,
232.21s - 241.57s |  that will produce some, some like um curve which is a like um PDF of the outputs of this algorithm
241.57s - 247.65s |  when instantiated on this particular database. And so on the x-axis imagine like all possible
247.65s - 253.25s |  things and sort of like a might be a produced by by this algorithm. On the y-axis is going to be
253.25s - 262.78s |  how likely is that output to occur if I input D. I'm going to move this a little bit closer.
269.23s - 275.23s |  See if that helps. Good. So, now we're going to imagine if I change one person's data,
275.23s - 281.31s |  say that I change Alice into being Javier and I plug my new database that is the same except
281.31s - 286.91s |  for one person's data into my same randomized algorithm, I'm going to get out a slightly
286.91s - 294.88s |  different curve. And our hope in a privacy space is that like a the ratio between these two curves
294.88s - 303.04s |  is going to be bounded point wise along this x-axis. So, now why is this this privacy? I've
303.04s - 306.88s |  talked about like you know freedom from harm and now I'm talking about like you know bounding ratios
306.88s - 314.40s |  as to the PDFs of algorithms. Well, imagine there was a particular output observed by some adversary
314.40s - 321.28s |  who then wanted to uh decide that I observe this output because of Alice's data or or like
321.28s - 328.72s |  because of Javier's data. Well, that adversary information theoretically could not make a guess
330.16s - 337.92s |  of the individual's data in the input beyond the like a small amount allowed allowed by the ratio.
340.10s - 347.70s |  So, now in math it says this. So, an algorithm m the maps the maps from a like a data set of size n
347.70s - 353.78s |  that is like a data data coming from n people into some arbitrary output range r is going to
353.78s - 361.14s |  be parametrized epsilon differentially private if it is the case that for all neighboring databases
361.14s - 367.06s |  d and d prime they're the same except for one person's data and for all possible things they
367.06s - 374.34s |  may produce as a result of the analysis for all sets of s I'm going to produce something in that
374.34s - 383.54s |  set s with about the same probability under d and and like a d prime and so we're saying if I change
383.54s - 389.62s |  one person's data I'm going to output the same thing with about the same probability and so
389.62s - 397.30s |  notice that our epsilon parameter occurs in the exponent which is probably not how you think about
397.30s - 403.22s |  privacy which makes it harder to explain to people without you like a technical background
403.22s - 408.18s |  I'm like imagining somebody's grandmother might like and have a hard hard time with this unless
408.18s - 416.42s |  you have a like an extremely extremely like a mathematical grandmother maybe um um and we're
416.42s - 423.70s |  going to get back to that point in the talk that's going to be sort of a core point before we um
423.70s - 431.38s |  get there why do we care about this uh particular privacy notion one thing is that it's used kind of
432.10s - 439.38s |  kind of ubiquitously um no one talks about it as much because it isn't a super sexy privacy notion
439.38s - 445.54s |  as we talk about like you know bounding ratios and exponents but it's used um by like a virtually
445.54s - 452.82s |  every like a big tech company and now sort of like a growing use in and like on government
456.34s - 466.42s |  so now let's return to to the like a core of my talk which is which is the field of like a
466.42s - 471.54s |  differential privacy has paid a whole lot of attention into like algorithms guarantees
471.54s - 476.90s |  mathematics theorems proofs publishing academic papers which is wonderful and that's my entire
476.90s - 483.14s |  background but there has been very little attention paid to these really core concepts
483.14s - 490.18s |  of things like policies regulation government practice how do organizations make decisions
490.18s - 496.02s |  about these uh tools communication how to tell people how they work why they work and the nature
496.02s - 506.26s |  of the um guarantees so in particular there is no step-by-step guide for a new organization who
506.26s - 511.70s |  maybe like here's my talk and it's like wow wow that's amazing i want those tools for my data
511.70s - 520.42s |  there's there's no handbook for it and in particular a couple four things that are missing
520.42s - 526.50s |  is there's no guidance in terms of how do you pick epsilon it seems really important how do you pick
526.50s - 535.86s |  it how do you communicate guarantees about this um about these sort of like a nature and strength
535.86s - 542.50s |  for the privacy guarantees to the stakeholders how do you understand privacy accuracy trade-off
542.50s - 548.66s |  inherently if you are doing something that gives stronger differential privacy you're gonna take
548.66s - 555.06s |  a little bit of an accuracy loss how do you how do you understand that and then finally how do you
555.06s - 564.34s |  align technical privacy guarantees with the like policies and laws and is that and this is also
564.34s - 568.90s |  sort of like an outline of my talk realistically i'm going to skip four so that they don't get mad
568.90s - 579.06s |  at me um um but let's uh talk first about about one so so this is sort of like you know at the
579.06s - 584.82s |  crux of a sort of everyone's implementation questions i always get asked what's the right
584.82s - 592.90s |  epsilon and the truth is there is no single right answer there's no easy answer so the epsilon
592.90s - 601.38s |  parameter quantifies information leakage how much can can be learned in a like slightly convoluted
601.38s - 609.14s |  uh definitional way about individuals and the epsilon can be tuned to be any value between zero
609.14s - 619.46s |  and infinity so so like you know how how much privacy leakage is acceptable um um oops
620.02s - 627.78s |  and there's no easy answer in part because there are no best practices for it it's kind of a wild
627.78s - 633.86s |  wild west everybody just like picks something that that works they also don't don't really tell
633.86s - 638.42s |  people what they do most of the time there's a lot of like you know closed door meetings about
638.42s - 646.02s |  the right epsilon but it's very infrequently published additionally this is sort of getting
646.02s - 651.54s |  at the privacy accuracy trade-off they'll talk about more and of course like a different
651.54s - 656.42s |  applications are going to have have like a different privacy needs and different accuracy needs
656.42s - 662.02s |  and so they're like quote unquote right epsilon should depend on the use case on the specifics on
662.02s - 668.58s |  the needs on the organization and it might look like a very different in the federal government
668.58s - 674.82s |  versus in a startup versus versus in a large tech company it might look different if we're talking
674.82s - 680.02s |  about advertising versus versus healthcare it makes sense that it should be um different
681.46s - 688.18s |  so in fact one thing that the field has has like um proposed but it's not quite done yet is the
688.18s - 694.50s |  notion of an epsilon registry and this is an idea that will sort of like um increase a transparency
694.50s - 702.58s |  around uh privacy parameters so the organizations can really share share share share share share
703.38s - 709.54s |  not just what is their epsilon but also sort of like all the other features that are important
709.54s - 719.36s |  in terms of why why that was their right uh choice one more reason why it's hard is because
720.16s - 727.52s |  it's hard to articulate what this epsilon means no one like who has an intuitive uh yes 1.2 feels
727.52s - 734.24s |  right in my soul but instead it's really hard to explain it's a unitless parameter
734.96s - 740.96s |  it is contextless but as we talked about like a context is important it's like you know very
740.96s - 748.24s |  mathematical it involves a like a logarithm on the worst case bound of the of the like you know
748.24s - 753.28s |  probabilities of a randomized algorithm it's hard to like you know gain intuition about what is
753.28s - 758.96s |  right there and it's sort of like it doesn't match what we think about when we think about
758.96s - 769.20s |  privacy so you can sort of imagine there's a picture and it looks like this and it's like
769.20s - 777.63s |  you know stronger privacy guarantees versus weaker privacy guarantees and like an important detail
777.63s - 783.39s |  is it like the way in which in which like a differential privacy is achieved algorithmically
783.39s - 788.91s |  is by sort of sort of like i'm adding noise to the process and you add noise in a carefully
788.91s - 793.55s |  calibrated way and there's a lot of work thinking about how much noise what type of noise and like
793.55s - 798.99s |  where and in your process you sort of like i'm always add a little bit of randomization in there
798.99s - 806.91s |  and so the question is really about how much noise um this is a question that i have worked on but i
806.91s - 813.15s |  think i'm gonna skip it so they don't get mad at me again i'm sorry that we started late um i'm
813.15s - 821.63s |  very happy to like you know share share things um about this but i'll say they're sort of like
821.63s - 829.23s |  you know main me like i'm finding of this work is that if you give people math based explanations
829.23s - 835.39s |  in terms of the like a likelihood of what will happen under various choices accompanied by some
835.39s - 843.55s |  like a visualization of those choices it increases objective comprehension and also increases
844.27s - 849.87s |  feelings of self-efficacy in terms of like you're having enough information to make choices and
849.87s - 855.87s |  again this sort of right answer isn't global but it is like you know what is the right choice for
855.87s - 862.19s |  your for you for your application and so feeling as though you like you have enough information
862.19s - 870.75s |  is important um let's go to number uh two talking about like you know communicating guarantees to
870.75s - 876.51s |  the other stakeholders so the previous sort of like an explanation of epsilon was really just
876.51s - 882.67s |  about the epsilon to people who are going to share data in some like an algorithmic process
882.67s - 890.19s |  but there's lots of other important people in this process so we have so we have data subjects
890.99s - 896.91s |  who are the ones who have to like share share data and like i decide if it's a good idea
897.55s - 903.31s |  we also have how like i'm engineers who have to make a decision and they have to like implement
903.31s - 907.95s |  code and they have to think about like you know parameter tracking downstream implications of
907.95s - 914.83s |  privacy and so on we have how like i'm data curators who are the ones with the power to
914.83s - 920.91s |  either share or to not share data sets with like you know various analysts and they have to like
920.91s - 926.91s |  decide when is the right time to share or to not share under what conditions under what agreements
933.46s - 940.18s |  we have data analysts who really don't care about the privacy part at all they just care
940.18s - 944.90s |  about analyzing data and about maintaining exactly the same pipeline they were doing before
945.86s - 950.34s |  and so if you're like a differential private pipeline looks completely different and you like
950.34s - 956.66s |  can't use python people will be mad at you and so it's important you also incorporate privacy
958.90s - 963.22s |  privacy tools in a way that is not not so like a disruptive in that
964.74s - 970.42s |  um there's also like a privacy officers who have to think about things like
970.74s - 977.06s |  like things like a compliance in the organization regulatory requirements
978.74s - 984.66s |  executives have to make a decision is this good for my company what is the cost if i have to like
984.66s - 989.86s |  you know hire a bunch of privacy engineers how much is that going to cost if you talk to me
989.86s - 998.99s |  about like you know accuracy loss i don't want accuracy loss um there are lawyers who have to
998.99s - 1005.47s |  like decide is the use of this technology compliant with the privacy laws and the regulations
1005.47s - 1011.95s |  around the um the um use of data and there's also like a policy makers who get to make
1011.95s - 1016.27s |  new laws around the use of data if we think our current ones are like not particularly
1018.91s - 1028.35s |  particularly adequate and so for sort of all of these entities they have to make different
1028.35s - 1035.63s |  types of decisions they have to and they also have like a different amounts of sort of like a
1035.63s - 1042.67s |  background expertise and and like you know realistically attention span and so you want
1042.67s - 1048.67s |  to tailor tailor like um communication strategies to these individuals in a way in a way that sort
1048.67s - 1056.27s |  of really like a meets meets their their their needs i have more to say about a lot of these
1056.27s - 1061.71s |  but again we're going to skip to the next topic and like i'll talk to me afterwards if you want
1061.71s - 1071.60s |  to hear more good um so so for topic three understanding this sort of a privacy accuracy
1071.60s - 1078.24s |  trade-off is really important because of course nobody likes to lose privacy but also nobody likes
1078.24s - 1087.68s |  to lose accuracy so we had this like a picture before and this is really the way it is like i
1087.68s - 1094.40s |  described in academia is we say on the left there's like good privacy and bad accuracy
1094.96s - 1100.00s |  and on the right there is bad privacy and good accuracy and like you know pick a spot in the
1100.00s - 1107.52s |  middle and and like you know you can pick where in the middle you feel like being but that's sort of
1107.52s - 1114.24s |  what you get but i think this is not really capturing what is happening instead i think
1114.24s - 1120.08s |  the picture looks a little more like this how there's some like you know privacy accuracy curves
1120.88s - 1125.52s |  and of course you can't get like an amazing privacy amazing accuracy at the same same time
1125.52s - 1132.40s |  and so that's infeasible in the bottom left you get bad accuracy and bad privacy and you can for
1132.40s - 1137.20s |  sure do that but it's not particularly helpful and then you have these like you know spaces in
1137.20s - 1141.20s |  between maybe there's some like you know feasibility curve or some like you know
1141.20s - 1148.64s |  Pareto frontier of the trade-off based on current like a technology and a lot of the work on the
1148.64s - 1155.12s |  algorithmic side is sort of like you know pushing this curve further out so that we can we can get
1155.12s - 1161.68s |  like you know better privacy and better accuracy simultaneously but at a current moment in time
1161.68s - 1167.52s |  we have some some some curve and the company can can like pick i feel like being there on this curve
1168.48s - 1177.76s |  one important question this is a cartoon how do we quantify privacy how do we quantify accuracy
1177.76s - 1185.12s |  what do these things actually really mean so that we can like actually truly plot them and the truth
1185.12s - 1192.40s |  is we're not super good at doing either of them so so we'll start with the privacy challenge
1192.72s - 1199.28s |  challenge which is which is it like yes on the privacy axis we can plot
1200.08s - 1205.76s |  epsilon but as i already mentioned epsilon is probably not what your company's executive
1205.76s - 1210.56s |  thinks about as being important but rather they probably think about like you know privacy risk
1210.56s - 1216.88s |  privacy threats how will this privacy tool protect my data against various attacks threats and so on
1217.84s - 1224.56s |  but but you sort of like us sort of sort of like um standard version of doing like a risk
1224.56s - 1231.20s |  assessment under a dp it doesn't quite match what people think about so this is typically like a
1231.20s - 1237.36s |  worst case so we're thinking of an adversary that has complete knowledge of the data set
1237.36s - 1242.88s |  and they're trying to guess alice's data again that's probably not the type of adversaries
1243.84s - 1250.64s |  that you're facing can you do better against a a more like no realistic adversary
1251.36s - 1257.60s |  additionally like a differential privacy provides like a relative guarantees so it doesn't say
1258.32s - 1263.12s |  say things like you know there's a maximum one percent chance but they say there's a maximum
1264.08s - 1269.04s |  one percent multiplicative increase relative to the chance of an attack without this person's data
1270.00s - 1275.04s |  again that's probably not not what you're like an organization's security team is really thinking
1275.04s - 1281.76s |  about and then finally finally like a differential privacy is really focused at the individual level
1282.88s - 1286.56s |  and it's really thinking about like you know i have to protect individual's data
1286.56s - 1293.36s |  population level inferences are great that's the goal and therefore they are not considered a like
1293.36s - 1303.28s |  a privacy violation however maybe your company's database of its customers reveals something about
1303.28s - 1309.36s |  it's like you know marketing strategy or it's like um target audience or some other like you
1309.36s - 1314.88s |  know proprietary information you don't want to be shared that is like a population level
1314.88s - 1321.52s |  in the database rather than protecting one single individual so again so again all these properties
1322.40s - 1327.36s |  come out very naturally from the like a differential privacy definition but it's probably
1327.36s - 1336.11s |  not what is important at the organizational level so an important challenge includes like a
1336.11s - 1341.79s |  developing risk frameworks that match like you know realistic attacks what are you actually worried
1341.79s - 1347.63s |  about and then sort of understanding how does how does like a protection against those attacks
1347.63s - 1352.75s |  match or not match like a differential privacy and vice versa i've done a little bit of work
1352.75s - 1359.23s |  thinking about like a taxonomy of attacks there i'm happy to to sort of like a chat about
1359.23s - 1370.56s |  afterwards and so now on the accuracy side i will say in general organizations are really good at
1370.56s - 1375.36s |  sort of like understanding how to like a track and evaluate their important metrics things like
1375.36s - 1381.20s |  revenue things like things like you know customer engagement things like market share organizations
1381.20s - 1388.00s |  are really really good at this um however that doesn't match match these sort of like you know
1388.00s - 1394.96s |  accuracy guarantees coming naturally out of the like a differential privacy literature which are
1394.96s - 1405.31s |  things like you know high probability accuracy additive accuracy get bounds bounds on some like
1405.31s - 1412.27s |  you know on some like a particular statistic that is um computed how do you map that into revenue
1413.23s - 1418.43s |  um i imagine there's a lot of like you know consultants in the world who will happily do that
1418.43s - 1423.47s |  but i think also this is a chance to like you know think a little bit more about the impact
1423.47s - 1431.07s |  the privacy guarantees onto the like operational functioning and things like you know if the width
1431.07s - 1438.83s |  of my like you know 95 confidence interval of a value shrinks by half how does that impact
1439.79s - 1449.25s |  revenue uh and like you know those are the important types of um questions so again there's
1449.25s - 1454.13s |  a couple like you know research challenges here one of them is a better understanding of the
1454.13s - 1459.17s |  pipeline of sort of like how to map like a differential privacy's guarantees
1460.69s - 1467.17s |  into these needs but importantly maybe we can pitch it better especially like you know to these
1467.17s - 1474.61s |  executives rather than saying you're going to lose accuracy maybe we can say using a differential
1474.61s - 1479.25s |  privacy is going to like increase like you know customer trust and they're going to share data
1479.25s - 1484.45s |  and they're going to opt in more and you'll get like you know better data better pr and so on
1487.14s - 1495.46s |  so my fourth um topic is about laws i was told i'm uh done but i will just say say say there's
1495.46s - 1503.62s |  like a huge gap between the like a privacy privacy sort of like a mathematical guides
1503.62s - 1510.26s |  and the and the like a legal framework and so i think there's a lot of interesting work
1511.30s - 1516.82s |  to be done there so i will like you know flash my slides of the question when is like no
1516.82s - 1523.38s |  differential privacy compliant with like a various laws and some reasons why it's hard and i will
1523.38s - 1529.86s |  thank you for your time and your attention um come see me at the like a crypto privacy village
1529.86s - 1536.34s |  immediately after this if you want to add chat or my like a surprise talk at 1 30 also in the
1536.34s - 1542.26s |  like a crypto privacy village where i will talk more about like you know about like a privacy
1542.26s - 1545.38s |  laws and privacy tools so thank you