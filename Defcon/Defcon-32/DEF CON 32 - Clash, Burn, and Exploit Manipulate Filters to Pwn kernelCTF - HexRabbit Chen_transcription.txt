{
  "webpage_url": "https://www.youtube.com/watch?v=_1DTkkaNqfM",
  "title": "DEF CON 32 - Clash, Burn, and Exploit  Manipulate Filters to Pwn kernelCTF - HexRabbit Chen",
  "description": "As the successor to the iptables, nftables stands as a crucial network component within the Linux kernel, managing packet filtering and other network-related functionalities. With continuous development and changes, features designed to increase its efficiency, such as batch commit, anonymous chains/sets, and asynchronous garbage collection, have been implemented, which in turn has significantly increased its complexity and made it an attractive target for attackers in recent years.\n\nSince the announcement of the kernelCTF bug bounty, multiple nftables 0-day vulnerabilities have been reported and patched to enhance its security. However, if not careful enough, the security patch may not only mitigate the bug but also introduce new security issues unintentionally. By researching the structural changes in the nftables codebase, we successfully uncover new vulnerabilities despite the intense competition in kernelCTF. Also, we managed to speedrun the exploitation just before Google removed nftables from LTS instance, becoming the last LTS nftables exploitation.\n\nIn this presentation, we will share three nftables vulnerabilities we discovered in a storytelling fashion. We start with a brief introduction on how nftables works under the hood to familiarize attendees with the basics. After that, we dive into nftables internals and dissect three vulnerabilities discovered during our journey, two of which involved utilizing hard-to-exploit race conditions to pwn the flag. Alongside details of the exploitation, we will also share the roller-coaster story of kernelCTF experiences, filled with dramatic highs and lows, making it a tense and exhilarating journey.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2260,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.02s - 8.42s | This text was transcribed using whisper model: large-v2

 And with no further ado, please continue.
8.42s - 12.14s |  Hello everyone and welcome to my talk,
12.14s - 13.82s |  Clash, Burn and Exploit,
13.82s - 16.26s |  Manipulate Filters to Pwn Kernel CTF.
17.74s - 19.58s |  I'm HexRabbit from Taiwan,
19.58s - 23.14s |  currently working as a security researcher at DefCon.
23.14s - 25.22s |  I specialize in binary exploit,
25.22s - 27.86s |  especially in Linux kernel exploitation.
27.86s - 30.98s |  I found some vulnerabilities in Linux kernel component
30.98s - 34.70s |  like Error Ring, KSMBD, NSTables and others.
36.46s - 39.66s |  Today, I will start from introducing kernel CTF
39.66s - 41.22s |  and NSTables.
41.22s - 44.58s |  After that, we will dive into the NSTable internals
44.58s - 46.18s |  and talk about three vulnerabilities
46.18s - 48.62s |  we discovered in NSTables.
49.58s - 52.30s |  And in between these three vulnerabilities,
52.30s - 55.46s |  I will also share some story behind the scene
55.46s - 58.95s |  during my kernel CTF journey.
58.95s - 62.55s |  So let's talk about, first talk about the kernel CTF.
62.55s - 64.99s |  Kernel CTF is part of Google VRP
64.99s - 68.99s |  and the way it works is quite similar to a CTF challenge.
68.99s - 70.55s |  For about every two weeks,
70.55s - 72.71s |  there'll be new kernel version release
72.71s - 75.95s |  and attendee will be given an unprivileged show
75.95s - 78.19s |  and you will need to exploit it,
78.19s - 81.15s |  exploit the Linux kernel to get flag
81.15s - 84.07s |  and submit it to Google to earn the bounty.
84.07s - 86.23s |  There are three instances can be pawned.
86.23s - 90.51s |  LTS and CLS both use newer and quite similar kernel config
90.51s - 93.91s |  while the mitigation one use an older kernel config
93.91s - 98.63s |  with custom security patch to accept more one day exploit.
98.63s - 100.95s |  But the most important thing is,
100.95s - 103.43s |  Google offers pretty big bounty reward.
105.15s - 107.91s |  With some simple math calculation,
107.91s - 110.75s |  we can quickly find for zero day LTS exploit,
110.75s - 114.19s |  the bounty can easily surpass 50,000 US dollar.
114.19s - 115.83s |  And with the same vulnerability,
116.63s - 119.31s |  you can earn both 21,000 max,
119.31s - 123.47s |  maximum in both mitigation and CLS instance,
123.47s - 127.03s |  which is quite appealing for the bounty hunter like me.
128.67s - 131.07s |  So why am I targeting nftables?
131.07s - 134.87s |  First, nftable config is enabled in kernel CTF.
134.87s - 136.39s |  And as you can see on the right,
136.39s - 138.51s |  there are multiple vulnerabilities discovered
138.51s - 140.67s |  just after kernel CTF is announced.
140.67s - 143.51s |  So for kernel CTF newbie like me,
143.51s - 145.39s |  I think it's the right place to start.
146.83s - 153.34s |  Okay, so first we take a brief intro to nftables.
153.42s - 155.50s |  What exactly is nftables?
155.50s - 157.58s |  Nftable is a package filtering framework
157.58s - 159.02s |  inside Linux kernel.
159.02s - 162.94s |  It's used to replace the legacy IP, ARP, EB tables,
162.94s - 165.46s |  merge them into a single entry point.
165.46s - 170.46s |  It use a simple VM design to implement its functionality
170.78s - 173.58s |  and support new syntax for CY interface.
173.58s - 174.82s |  As you can see on the right,
174.82s - 176.86s |  instead of doing thing one by one,
177.02s - 181.46s |  new syntax can now allow config multiple things at once.
181.46s - 184.38s |  And this capabilities come from its design,
184.38s - 188.80s |  which we will revisit later.
188.80s - 191.88s |  Nftable is constructed by a tree-like structure.
191.88s - 193.76s |  It contains five core modules,
193.76s - 197.12s |  table, chain, rule, expression, and set.
198.68s - 202.08s |  Table is the top level container for the below structures.
202.08s - 204.04s |  It's used for the measurement purpose.
204.76s - 208.96s |  Each table will only belong to a specific neural family,
208.96s - 211.40s |  and any object contained by the table
211.40s - 212.80s |  will follow the same family.
214.36s - 218.28s |  Chain in nftables will hook at predefined hooking point
218.28s - 220.88s |  provided by net filter infrastructure
220.88s - 223.00s |  to do the packet filtering.
223.00s - 226.56s |  When any packet flows through the hooking point,
226.56s - 229.00s |  the chain will execute the expressions in the row
229.00s - 231.48s |  it managed to accept, or to block,
231.48s - 233.32s |  or to modify the packets.
234.12s - 236.64s |  What is the rule, and what is the expression?
238.73s - 240.61s |  As I mentioned earlier,
240.61s - 243.25s |  nftable implements its functionality
243.25s - 245.29s |  by a simple virtual machine.
245.29s - 248.13s |  You can think a rule as a small function.
248.13s - 250.05s |  They can contain multiple expressions,
250.05s - 253.73s |  and the expression is the instruction for nftables.
254.85s - 257.33s |  There are different types of expressions.
257.33s - 260.49s |  Some can be used to manipulate VN registers,
260.49s - 262.45s |  and the others can be used to load,
263.09s - 267.28s |  to read, or write data within packets.
267.28s - 270.56s |  And thus, set is used to store a group of some type,
270.56s - 275.08s |  like IP address, or port, or any binary data.
275.08s - 277.24s |  It can also be used as a map
277.24s - 280.04s |  to map some type A to another type B.
280.04s - 282.12s |  And these data stored in the set
282.12s - 287.71s |  can be queried by expressions during packet filtering.
287.71s - 291.83s |  Okay, so after having some basic understanding of nftables,
291.83s - 294.79s |  I will now introduce the first vulnerability I discovered.
296.11s - 299.27s |  At the time, I'm not so familiar with nftables.
299.27s - 302.95s |  However, I spotted something weird in the code base.
302.95s - 304.47s |  These three functions are all used
304.47s - 307.39s |  to delete objects in nftables.
307.39s - 309.91s |  The nftable.deleteRuleDeactivate
309.91s - 311.71s |  will be called during rule deletion,
312.67s - 315.59s |  nft.flush will be called during full config flush,
316.99s - 320.07s |  and nft.setElementCacheAllDeactivate
320.07s - 322.75s |  will be called during set element deletion.
323.75s - 326.23s |  However, the pattern here is a bit different.
327.79s - 331.95s |  The first two use nft.isActiveNext
331.95s - 334.43s |  to check if the object is active.
335.35s - 338.99s |  But the last one, nft.setElementCacheAllDeactivate
338.99s - 341.03s |  use nft.isActive to check.
341.03s - 343.19s |  So, what's the difference?
344.91s - 346.99s |  Actually, these two functions
346.99s - 351.67s |  is used for object lifetime management in nftables.
352.27s - 354.07s |  In order to understand how it works,
354.99s - 356.95s |  we will need to first talk
356.95s - 362.43s |  about the batch request mechanism.
362.43s - 364.71s |  For a query request,
364.71s - 367.23s |  for example, the getRule operation above,
367.23s - 370.83s |  Netlink API will internally use netlink.receiveXKB
370.83s - 373.43s |  to get the raw information loglessly.
374.55s - 377.07s |  However, if it's some operation
377.07s - 378.63s |  that will modify the control plan,
378.63s - 382.07s |  like new table or delete chain,
382.11s - 384.91s |  nftables will use nfnetlink.receiveBatch
384.91s - 387.83s |  instead of the logless one to serve the request.
387.83s - 389.19s |  The difference between them
389.19s - 392.15s |  is that batch one can process a request
392.15s - 394.35s |  which contains multiple operations,
394.35s - 396.87s |  and they will be processed sequentially.
397.87s - 400.19s |  And also because of this design,
400.19s - 403.99s |  the COI interface can copy multiple things at once.
405.27s - 407.55s |  So, it sounds great,
407.55s - 410.63s |  but the thing become more complicated
410.63s - 414.63s |  when any operation fail in the middle of a batch request.
415.59s - 419.39s |  And what nftables do is to split batch request
419.39s - 423.91s |  handling into multiple phases.
423.91s - 425.83s |  There are three phases during the handling
425.83s - 427.83s |  of the batch request,
427.83s - 431.83s |  prepare phase, commit phase, and abort phase.
431.83s - 433.15s |  During the prepare phase,
433.15s - 435.59s |  if every operation finished successfully
435.59s - 437.03s |  in the batch request,
437.03s - 440.15s |  it will go to the commit phase and commit the changes.
441.15s - 443.39s |  However, if something went wrong,
443.39s - 445.47s |  after every operation is processed,
445.47s - 448.35s |  it will go to abort phase and revert all the changes.
449.39s - 451.99s |  Here's the lifetime management's concern.
451.99s - 457.49s |  Any object in nftable has two state variable,
457.49s - 460.45s |  current generation and next generation.
460.45s - 463.69s |  Only object activated in current generation
463.69s - 466.45s |  can affect the runtime behavior.
466.45s - 467.89s |  And you can think the next generation
467.89s - 470.57s |  as a temporary state for the prepare phase.
472.53s - 474.57s |  Every operation modify the control plan,
474.57s - 477.13s |  like new or delete operations,
477.13s - 480.05s |  will only make changes on the next generation state
480.05s - 481.61s |  in the prepare phase.
481.61s - 484.45s |  So, without affecting the current generation state.
485.93s - 488.41s |  Then, during the commit phase,
488.41s - 490.81s |  it will commit the modification.
490.81s - 494.41s |  So, the current generation will finally be set
494.41s - 496.25s |  as the next generation,
496.25s - 497.77s |  and the next generation state
497.77s - 499.53s |  will be reset to a clean state.
500.49s - 503.01s |  But in the case, if something went wrong
503.01s - 504.57s |  during the abort phase,
504.57s - 507.77s |  it only needs to clean the next generation state.
508.97s - 513.65s |  Let's go through some examples to know how it works.
513.65s - 516.01s |  After a batch request is received,
516.01s - 517.77s |  it goes into the prepare phase.
519.09s - 522.05s |  The first operation is delete set C.
522.05s - 525.37s |  Okay, so the set C's next generation state
525.37s - 529.27s |  will be set to deactivated.
529.47s - 534.19s |  Next, it continues to process a set D operation.
535.47s - 537.99s |  So, the set D will be added into hash table
537.99s - 543.17s |  with next generation in activated state.
543.17s - 546.61s |  If nothing went wrong, we'll go to the commit phase.
546.61s - 549.13s |  And at this phase, current generation
549.13s - 552.45s |  will be set as the next generation.
552.45s - 554.49s |  So, the set D will be available
554.49s - 557.29s |  in both runtime and query operations.
557.29s - 562.34s |  And also, the set C will be free.
562.34s - 566.38s |  If something went wrong, we will enter the abort phase.
566.38s - 569.66s |  And at this phase, all operation that's done
569.66s - 573.34s |  in the prepare phase will be reverted in a reverse order.
573.34s - 576.54s |  So, the add set D will be undue,
576.54s - 582.68s |  and also the delete set C will be undue.
582.68s - 584.88s |  So, okay, back to the check.
584.88s - 587.12s |  So, what's the problem here?
587.12s - 589.60s |  As we can see, during the NFT set element
589.60s - 592.96s |  catch all deactivate, it used NFT is active
592.96s - 596.08s |  to check if the element is still alive.
596.08s - 600.00s |  However, NFT is active checks current generation,
600.00s - 603.28s |  but the function actually runs in the prepare state,
603.28s - 606.20s |  which will only modify the next generation state
606.20s - 608.60s |  of an object.
608.60s - 612.72s |  So, the check here will always pass.
612.72s - 615.52s |  And by request multiple delete operations,
615.52s - 620.64s |  we can free the element multiple times in the commit phase.
620.64s - 622.60s |  So, the batch request to trigger double free
622.60s - 623.80s |  will look like this.
623.80s - 626.32s |  Since the lifetime check will always pass
626.32s - 629.36s |  if the element exists during the prepare phase,
629.36s - 632.04s |  two delete set element of operation
632.04s - 634.44s |  will both execute successfully.
634.44s - 637.40s |  And so, when you get to the commit phase,
637.40s - 639.68s |  the send element will be free twice,
639.68s - 644.56s |  and we get a set catch all element double free.
644.56s - 649.72s |  Great. So, I found my first bug for current CTF.
649.72s - 654.56s |  But the next day, I found the maintainer
654.56s - 656.28s |  also find the same bug
656.28s - 659.00s |  and send a patch to kernel mailing list.
659.00s - 661.00s |  I mean, why?
661.00s - 664.00s |  So, okay, this means my zero days
664.00s - 666.12s |  officially turns into a one day.
666.12s - 669.36s |  And according to kernel CTF rules,
669.36s - 673.44s |  my bounty will decrease by 20,000 US dollar.
673.44s - 676.00s |  Ouch.
676.00s - 678.64s |  Also, one day vulnerability means
678.64s - 681.44s |  that everyone will know how to trigger the vulnerability,
681.44s - 684.48s |  and they may also come up with some exploit.
684.48s - 686.56s |  So, I definitely need to exploit it
686.56s - 692.44s |  before next kernel CTF slot opens.
692.44s - 694.76s |  So, after we get the double free primitive,
694.76s - 699.88s |  we reuse the exploit method for CVE-2023-4004
699.88s - 702.28s |  to overlap the empty table data
702.28s - 706.16s |  and empty object to leak address and control IP.
706.16s - 707.76s |  Due to a time limit of this talk,
707.76s - 709.52s |  I won't expand the detail of it,
709.52s - 713.68s |  but kudos to LonelyOnCom who discovered this technique
713.68s - 717.68s |  and shared it on kernel CTF repository.
717.68s - 720.68s |  So, now we develop the exploits
720.68s - 725.17s |  and wait for the slot to reopen.
725.17s - 728.61s |  Fast forward to two hours before the competition starts.
728.61s - 731.45s |  I nervously check if there's any kernel updates
731.45s - 734.29s |  related to NF tables.
734.29s - 736.81s |  And guess what?
736.81s - 739.01s |  The patch previously posted on the mailing list
739.05s - 741.33s |  has been merged into the kernel.
741.33s - 744.37s |  And it added one line on it.
744.37s - 748.09s |  It says, reported by LonelyOnCom,
748.09s - 750.73s |  which this means is the patch may actually come
750.73s - 753.37s |  from a zero-day kernel CTF submission.
753.37s - 755.73s |  And since the reporter is LonelyOnCom,
755.73s - 760.29s |  whose exploit I just used, this is indeed the case.
760.29s - 763.41s |  So, just before I have the chance to exploit
763.41s - 768.13s |  an official instance, I found it's a collision.
768.13s - 772.57s |  So, yes, I still found the instance and report the flag.
772.57s - 775.57s |  But too bad for me, the first bug
775.57s - 780.05s |  do collide with an old kernel CTF submission.
780.05s - 785.32s |  That's how I look after the competition.
785.32s - 788.08s |  Okay, so I take some break.
788.08s - 794.55s |  And after three weeks later, I found the second bug.
794.55s - 798.39s |  During the research, I came across an interesting commit.
798.39s - 803.55s |  NF table validate family when identifying table by handle.
803.55s - 806.51s |  The commit is added after the mitigation instance kernel
806.51s - 809.67s |  version, so it can be used upon the mitigation instance
809.67s - 811.63s |  as a one-day.
811.63s - 816.75s |  It added a check on NFT table lookup by handle function,
816.75s - 819.59s |  which is used for searching a register table
819.59s - 821.99s |  by the provided handle.
821.99s - 824.51s |  Without the commit, during the table lookup,
824.51s - 826.71s |  we can provide a family different from the one
826.71s - 828.59s |  which table belongs to.
828.59s - 834.47s |  But what can we do with this inconsistency?
834.47s - 837.83s |  Let's trace where the family parameter comes from
837.83s - 839.75s |  and where it goes.
839.75s - 842.51s |  NF table lookup by handle is only called
842.51s - 844.91s |  from NF table delete table.
844.91s - 848.35s |  And this function is used to delete user-specified table
848.35s - 850.11s |  from NF tables.
850.11s - 856.51s |  The family variable is directly get from user-provided input.
856.55s - 858.99s |  And it's saved into context family
858.99s - 864.28s |  and passed to NFT flush table function.
864.28s - 867.48s |  In NFT flush table, context argument
867.48s - 870.88s |  is passed into every function handling object deletion.
870.88s - 874.60s |  For me, trace which function use the context family
874.60s - 875.92s |  is quite annoying.
875.92s - 880.68s |  So instead of just tracing into each function,
880.68s - 885.72s |  I take the shortcut to trace where it is used.
885.76s - 888.00s |  So I just grab the whole code base
888.00s - 892.72s |  to find where the context family is used.
892.72s - 896.32s |  Grabbing context family gives us a lot of result,
896.32s - 899.28s |  most of which comes from the NF table expressions
899.28s - 904.04s |  like count limit, net, T proxy, and et cetera.
904.04s - 908.68s |  However, most of them cannot be used for the exploit.
908.68s - 912.36s |  But I finally found one that can be exploited.
912.36s - 917.26s |  It is in the NFT log expression.
917.26s - 920.78s |  The trigger path I found starts from the NFT log destroy.
920.78s - 923.46s |  This function takes context family as an argument
923.46s - 927.14s |  and call to NF log output.
927.14s - 929.86s |  Now it's becoming the PF argument
929.86s - 934.50s |  and used as an index to de-reference the loggers array.
934.50s - 937.10s |  The value range of our input context family
937.10s - 941.06s |  and also the PF here is 0 to 255.
941.06s - 945.54s |  But if we check the size of loggers array,
945.54s - 949.46s |  we can find it only have 13 elements in the first dimension.
949.46s - 954.54s |  So we get the global array out of bound access here.
954.54s - 958.65s |  But what can we do with it?
958.65s - 962.21s |  After taking the logger out, it de-reference the me field
962.21s - 966.09s |  and feed it into the module put.
966.09s - 968.17s |  The module put will again de-reference
968.17s - 972.29s |  and decrease the refcon value in the module object.
972.29s - 974.05s |  With this vulnerability,
974.05s - 978.13s |  a logger can be any pointer store adjacent to a loggers array.
978.13s - 980.81s |  So we may cause some type of confusion here
980.81s - 983.53s |  and decrease some variable by one.
983.53s - 987.73s |  However, we are dealing with an awkward situation here.
987.73s - 990.09s |  In order to utilize this bug,
990.09s - 992.81s |  we have to find a pointer store on global
992.81s - 996.05s |  and can be de-referenced twice.
996.05s - 998.69s |  The first is to take our logger me
998.69s - 1004.23s |  and the second is to take our module refcon.
1004.23s - 1007.15s |  To sum up what we have by now,
1007.15s - 1012.79s |  first, the maximum value of pfValue we can control is 255.
1012.79s - 1017.39s |  And we can trigger an out of bound refcon decrement
1017.39s - 1021.27s |  by using a pfValue larger than 12,
1021.27s - 1024.39s |  which will execute like the code here.
1024.39s - 1026.99s |  To exploit this, the first thing we need to do
1026.99s - 1032.59s |  is to find a global pointer placed near the loggers array.
1032.59s - 1035.03s |  So what do we have on the global?
1035.03s - 1037.51s |  By inspect the marrying GDB,
1037.51s - 1040.95s |  we quickly find there's an NFCT expect hash
1040.95s - 1044.23s |  placed near the loggers array.
1044.23s - 1045.95s |  And it's a hash table.
1045.95s - 1049.15s |  So in this case, it seems to be a good target
1049.15s - 1053.55s |  since the pointer point to an array of pointers.
1053.55s - 1058.66s |  Therefore, it can be de-referenced twice.
1058.66s - 1062.86s |  So let's check how the code normally works.
1062.86s - 1067.26s |  First, it gets the me field at offset 1x
1067.26s - 1073.46s |  and then get the refcon at offset 340x.
1073.46s - 1076.42s |  Now let's check the out of bound access case.
1076.42s - 1079.22s |  First, we set pf to 19 to get a hash table
1079.22s - 1082.50s |  pointer NFCT expect hash.
1082.50s - 1085.94s |  The first de-reference we got is the index tree entry
1085.94s - 1088.10s |  of the hash table.
1088.10s - 1090.58s |  The entry points to the HNO field
1090.58s - 1093.70s |  of an NFCT expect object.
1093.70s - 1095.38s |  So after one more de-reference, it
1095.38s - 1100.38s |  will access offset 340x down below the expect object.
1100.38s - 1104.50s |  But since the expect object only have a d at size,
1104.50s - 1107.26s |  it will access out of bounds.
1107.26s - 1111.22s |  Fortunately, all NFCT expect object
1111.22s - 1114.98s |  allocated in a dedicated Cayman cache.
1114.98s - 1117.02s |  So the memory space below will also
1117.02s - 1121.42s |  contain NFCT expect objects.
1121.42s - 1123.82s |  So it will access to an RCU field
1123.82s - 1127.54s |  and decrease the next pointer in the structure.
1127.54s - 1132.71s |  But what does this next pointer useful?
1132.71s - 1135.75s |  As we know, after RCU objects free,
1135.75s - 1138.71s |  Linux kernel will not instantly free the object.
1138.71s - 1142.19s |  Instead, it will hold the object for a grace period
1142.19s - 1145.55s |  to wait for all other RCU readers to finish.
1145.55s - 1148.27s |  So the next pointer in the RCU hash structure
1148.27s - 1150.43s |  is used for temporary story name
1150.43s - 1156.25s |  and waiting for the RCU thread to process to free them.
1156.25s - 1158.89s |  So after the contract expect object are free,
1158.89s - 1164.69s |  they will be like this, chained by the next pointer.
1164.69s - 1168.09s |  And since we can decrease the next pointer in RCU head,
1168.09s - 1169.93s |  we can make it point to another field
1169.93s - 1173.33s |  inside the contract expect object.
1173.33s - 1175.73s |  And to exploit it, we choose to land it
1175.73s - 1177.61s |  on a user-controllable field, which
1177.61s - 1181.59s |  contains a union of IPv4 and IPv6 address.
1181.59s - 1185.03s |  So when RCU callback triggered, it
1185.03s - 1187.27s |  will read the fake function pointer we set,
1187.27s - 1190.76s |  and we can control the IP.
1190.76s - 1193.68s |  However, to actually exploit the vulnerability,
1193.68s - 1195.92s |  we came across some roadblocks.
1195.92s - 1199.52s |  First, we need the index tree of the hash table
1199.52s - 1201.28s |  point to some expect object.
1201.28s - 1204.88s |  So we can use it to do the out-of-bound write.
1204.88s - 1211.12s |  But unfortunately, the hash value is unpredictable.
1211.12s - 1214.76s |  Next, the out-of-bound written expect object
1214.76s - 1217.44s |  need to be in the grace period, so we
1217.44s - 1220.80s |  can decrease its next pointer.
1220.80s - 1223.64s |  But the next period is a small time window
1223.64s - 1225.80s |  and hard to control when the kernel actually
1225.80s - 1228.64s |  frees the object.
1228.64s - 1231.52s |  Well, so long story short, we eventually
1231.52s - 1236.60s |  found a way to exploit it by brute force and race.
1236.60s - 1239.72s |  To exploit this, first, we use a tight loop
1239.72s - 1242.70s |  to create a lot of expect objects.
1242.70s - 1245.34s |  And at some point, the expect count
1245.34s - 1249.74s |  exceeded the value max expected and start
1249.74s - 1254.24s |  to evict the oldest expect object.
1254.24s - 1259.16s |  So the oldest expect object will be placed into the RCU free list.
1259.16s - 1262.32s |  And so on, and so on.
1262.32s - 1266.12s |  And at some point, RCU thread gets scheduled by the kernel
1266.12s - 1271.70s |  and starting to process objects.
1271.70s - 1273.62s |  At this point, we add another thread
1273.62s - 1277.26s |  to trigger out-of-bound decrement in a batch request.
1277.94s - 1283.09s |  And keep this system running for a bit.
1283.09s - 1287.97s |  So we hope for a race scenario like this to emerge.
1287.97s - 1290.33s |  First, the expect object allocated
1290.33s - 1293.05s |  into index tree of the hash table.
1293.05s - 1296.13s |  And then the expect object you want to out-of-bound write
1296.13s - 1299.45s |  is freed into the RCU free list.
1299.45s - 1304.61s |  Then it continues to free another expect object.
1304.61s - 1307.89s |  After that, we trigger the vulnerability 28 times
1307.89s - 1310.93s |  in a batch request to make the next pointer point
1310.93s - 1314.09s |  to a user-controllable value.
1314.09s - 1317.89s |  And finally, the RCU starts to process these three objects
1317.89s - 1322.28s |  and runs our set callback function.
1322.28s - 1323.12s |  Great.
1323.12s - 1325.12s |  Let's try it on local build.
1325.12s - 1332.78s |  And we successfully control the IP to 414441.
1332.78s - 1337.00s |  So what about remote?
1337.00s - 1337.92s |  Let's try.
1337.92s - 1340.80s |  And we got an error message while trying
1340.80s - 1343.56s |  to add the log expression.
1343.56s - 1345.24s |  Since I have no idea what happens
1345.24s - 1347.48s |  on the remote environment, I decided
1347.48s - 1352.92s |  to download the official busy image to debug it locally.
1352.92s - 1357.68s |  So I open the GDB and set some breakpoint on the functions.
1357.68s - 1361.36s |  But when I set the break on n of t log destroy function,
1361.36s - 1364.76s |  GDB complained that it was not defined.
1364.76s - 1368.82s |  What's wrong here?
1368.82s - 1372.26s |  I later found that it's because the kernel config I used
1372.26s - 1376.94s |  is different from the one used in the official environment.
1376.94s - 1380.46s |  So the log expression I used to exploit the vulnerability
1380.46s - 1384.02s |  not even compiled into the official image.
1384.02s - 1387.70s |  But I'm using the official build script
1387.70s - 1389.78s |  to view the local test image.
1389.78s - 1394.59s |  Why else there can be any difference?
1394.59s - 1395.91s |  I mean, why?
1399.91s - 1404.35s |  Eventually, I found it is caused by the official build script.
1404.35s - 1408.19s |  The mitigation instance is built by CoS kernel config
1408.19s - 1412.67s |  plus the Google custom security patch.
1412.67s - 1415.07s |  And the official mitigation instance
1415.07s - 1419.75s |  used CoS config 6.1.55.
1419.75s - 1424.79s |  The NF table expression are enabled in CoS config
1424.79s - 1428.59s |  after the version 6.1.64.
1428.59s - 1432.47s |  However, the build script will always fetch the latest CoS
1432.47s - 1435.43s |  config, so the mitigation kernel I built
1435.43s - 1438.35s |  has the NF table expression enabled.
1438.35s - 1443.90s |  But in the official image, they're actually disabled.
1443.90s - 1447.78s |  So well, I think next time I should definitely
1447.78s - 1454.25s |  check out the config first.
1454.25s - 1460.85s |  OK, so after my second failure, I revisit some old patch
1460.85s - 1465.50s |  and I found a new bug.
1465.50s - 1468.54s |  The patch I was looking at is the patch
1468.54s - 1470.78s |  which introduced the first bug.
1470.78s - 1475.46s |  The title says, NF table adapt set backend
1475.46s - 1478.26s |  to use GC transaction API.
1478.26s - 1481.62s |  So what does the GC mean?
1481.62s - 1485.18s |  Here it means the set garbage collection.
1485.18s - 1487.46s |  There are some set implementation support user
1487.46s - 1491.94s |  to set a timeout on any set element.
1491.94s - 1497.30s |  And there are two types of GC to clean the expired element.
1497.30s - 1500.94s |  The first one, the synchronous one,
1500.94s - 1503.22s |  will be called in the commit phase.
1503.22s - 1506.74s |  And a synchronous one will be triggered periodically
1506.74s - 1508.74s |  in the background.
1508.74s - 1513.50s |  But here, we only focus on the synchronous one.
1513.50s - 1516.58s |  So back to the patch.
1516.58s - 1520.74s |  In the commit message, it says, use GC transaction API
1520.74s - 1524.86s |  to replace the old and buggy GC API and the busy mark
1524.86s - 1526.54s |  approach.
1526.54s - 1529.30s |  So what is the difference between the new and the old
1529.30s - 1530.58s |  one?
1530.58s - 1535.70s |  Let's take a look of the old GC design.
1535.70s - 1538.14s |  In the old design, the synchronous GC
1538.14s - 1542.66s |  will run loglessly and can be triggered at any time.
1542.66s - 1544.38s |  So in order to protect GC threads
1544.38s - 1547.98s |  from raising with the landing API, both of them
1547.98s - 1551.38s |  will check against busy mark set on the element
1551.38s - 1556.66s |  before delete it to prevent double free.
1556.70s - 1559.66s |  But sometimes, the developer might
1559.66s - 1561.90s |  forget to check against it.
1561.90s - 1564.50s |  In this case, there's a new function
1564.50s - 1569.30s |  called nftMapDeactivate forgot to check a busy mark.
1569.30s - 1573.94s |  And this leads to a double free vulnerability.
1573.94s - 1576.58s |  So in order to avoid this kind of issues,
1576.58s - 1582.86s |  nftable developer decided to replace it with a new design.
1582.86s - 1587.14s |  In the new GC design, it introduced the GC sequence.
1587.14s - 1589.70s |  The GC sequence is a per net variable.
1589.70s - 1594.66s |  And it will be increased before and after any modification
1594.66s - 1596.54s |  to the control plan.
1596.54s - 1599.02s |  So it will increase at the beginning
1599.02s - 1604.94s |  and at the end of both the commit and the abort phase.
1604.94s - 1608.34s |  During a GC procedure, GC thread will record and check
1608.34s - 1610.34s |  it to prevent raise.
1610.34s - 1613.58s |  Even though the GC thread still run loglessly,
1613.58s - 1621.34s |  during the check, it will now acquire the commit mutex lock.
1621.34s - 1625.26s |  The new GC design look like this.
1625.26s - 1628.02s |  During batch request handling, GC sequence
1628.02s - 1631.38s |  will increase both before and after the commit and abort
1631.38s - 1632.50s |  phase.
1632.50s - 1637.14s |  And in the GC thread, it will first run loglessly
1637.14s - 1639.42s |  to store the GC sequence and collect
1639.42s - 1641.58s |  all the expired elements.
1641.58s - 1644.22s |  After that, it will acquire the lock
1644.22s - 1648.46s |  and check if the current GC sequence match the safe one.
1648.46s - 1653.74s |  If they are the same, then the elements will be released.
1653.74s - 1656.62s |  For this normal case, since GC thread does not
1656.62s - 1659.38s |  raise with the batch request, the GC sequence
1659.38s - 1661.34s |  will be the same.
1661.34s - 1668.35s |  And the element can be freed by the GC thread.
1668.35s - 1672.39s |  Next, we check if it still works in the race case.
1672.39s - 1674.59s |  If during the prepare phase, GC thread
1674.59s - 1680.71s |  trigger and store the global GC sequence into transfer object,
1680.71s - 1683.23s |  it will continue to run loglessly
1683.23s - 1693.04s |  to find all the expired element and wait for the mutex lock.
1693.04s - 1694.68s |  At the same time, the main thread
1694.68s - 1696.60s |  step into the commit phase.
1696.60s - 1699.16s |  And some of the elements in the set
1699.16s - 1702.68s |  can be released in this phase.
1702.68s - 1706.16s |  The GC thread will need to wait until the main thread release
1706.16s - 1710.72s |  the mutex lock so it can finally check the safe GC sequence
1710.72s - 1713.88s |  and delete the element.
1713.88s - 1716.48s |  But in this case, since GC sequence
1716.48s - 1719.88s |  is recorded in the prepare phase and we check it
1719.88s - 1723.20s |  after the batch request is done, the GC sequence
1723.20s - 1725.04s |  will be different.
1725.04s - 1729.44s |  So the element deletion will be skipped to avoid double free.
1729.44s - 1732.28s |  Next, let's check the case if GC sequence is
1732.28s - 1736.32s |  recorded in the commit phase.
1736.32s - 1738.88s |  In this case, since GC sequence increase
1738.88s - 1745.56s |  after the commit phase, and the GC thread
1745.56s - 1754.56s |  can only check GC sequence after the commit mutex are locked,
1754.56s - 1758.62s |  this race will also be detected.
1758.62s - 1764.50s |  So to sum up, the new GC design require both the batch request
1764.50s - 1767.78s |  and the GC sequence check to hold the lock.
1767.78s - 1770.98s |  This ensures that even there's a race while collecting
1770.98s - 1774.14s |  the expired element, the check will always
1774.14s - 1780.74s |  happen after the GC sequence has increased.
1780.74s - 1784.50s |  But does the batch request always lock
1784.50s - 1786.66s |  means there's no race?
1786.66s - 1791.98s |  Actually, if you look closely, we will find,
1791.98s - 1798.44s |  well, it's not always locked.
1798.44s - 1800.56s |  To find where it released the lock,
1800.56s - 1804.16s |  let's first talk about the module autoload.
1804.16s - 1805.88s |  During the prepare phase, if there
1805.88s - 1808.00s |  is some expression type in the request
1808.00s - 1811.88s |  is not found during the type resolution,
1811.88s - 1817.00s |  it will instantly abort and try to load the correspondent type
1817.00s - 1821.32s |  module during the end of the abort phase.
1821.32s - 1828.25s |  After that, it will retry the whole batch request again.
1828.25s - 1831.41s |  However, maybe there is some efficiency consideration
1831.41s - 1835.37s |  or to just only prevent that lock before the function
1835.37s - 1839.21s |  try to load the request module.
1839.21s - 1842.69s |  The mutex lock will be released.
1842.69s - 1847.50s |  But could it cause any problem?
1847.50s - 1848.90s |  Back to the diagram.
1848.90s - 1852.10s |  Now there's a little time window in the abort phase
1852.10s - 1854.94s |  that it will not hold the mutex lock.
1854.94s - 1858.58s |  So there's a chance during the abort phase
1858.58s - 1862.38s |  the GC thread kicks in, records the expired elements,
1862.38s - 1864.90s |  and do a GC sequence check.
1864.90s - 1868.78s |  In this case, however, the check will be bypassed.
1868.78s - 1876.94s |  And the GC thread can delete arbitrary expired elements.
1876.94s - 1880.30s |  OK, so now the question is, how do we actually
1880.30s - 1883.62s |  exploit the race?
1883.62s - 1887.38s |  Our exploit plan is, first, we add a set element
1887.38s - 1889.26s |  with a short timeout.
1889.26s - 1891.18s |  And then we trigger module autoload
1891.18s - 1893.70s |  to force it to revert the commit and free
1893.70s - 1895.94s |  the element we just added.
1895.94s - 1904.34s |  And at the same time, we trigger GC to collect that element.
1904.34s - 1907.98s |  Next, module autoload will release the mutex.
1907.98s - 1911.18s |  And we hope that the GC thread will grab it and bypass
1911.18s - 1913.06s |  the check.
1913.06s - 1919.60s |  So as a result, the element we just added will be double free.
1919.60s - 1922.44s |  But in order to trigger double free,
1922.44s - 1926.60s |  there are two time windows we need to enlarge.
1926.60s - 1929.64s |  The first time window is from the abort phase start
1929.64s - 1931.92s |  to the set element released.
1931.92s - 1934.92s |  So the GC thread can recall the expired element
1934.92s - 1939.84s |  before they get a link by the revert procedure.
1939.84s - 1944.44s |  The second time window is from the load module to its end.
1944.44s - 1952.00s |  So the GC thread can grab the lock and bypass the check.
1952.00s - 1954.28s |  For the first one, since the abort phase
1954.28s - 1957.22s |  will revert the operations in a reverse order,
1957.22s - 1960.54s |  we add a bunch of requests after the add set element
1960.54s - 1965.44s |  request to delay the unlink operation.
1965.44s - 1968.84s |  For the second one, we just request multiple unknown type
1968.84s - 1974.58s |  to make module autoload runs longer.
1974.58s - 1978.44s |  And here's the demo.
1978.44s - 1984.99s |  OK, so first we boot our machine and check
1984.99s - 1986.95s |  that URD is a normal user.
1986.95s - 1989.19s |  And then we run the exploit.
1989.19s - 1992.35s |  So here, we will first measure the execution time
1992.35s - 1996.03s |  to set up timing parameter for the race.
1996.03s - 1998.27s |  And after that, we set up the environment
1998.27s - 2002.80s |  to enlarge the race time windows.
2002.80s - 2009.20s |  Then we trigger double free to leak address and control IP.
2009.20s - 2017.77s |  And we got the root shell.
2017.77s - 2021.35s |  Thank you, guys.
2021.35s - 2022.39s |  Great.
2022.39s - 2028.15s |  So now I write the exploit and submit it to Kernel CTF.
2028.15s - 2031.35s |  And this is my Kernel CTF submission.
2031.35s - 2035.99s |  I exploited the LTS instance on Kernel version 6.1.81.
2036.03s - 2040.79s |  But as you can see, it is marked with a dupe in that line.
2040.79s - 2042.95s |  It is because for every LTS slot,
2042.95s - 2047.03s |  only the first zero day or one day exploit will be accepted.
2047.03s - 2049.91s |  And there's someone who also submit
2049.91s - 2056.63s |  a LTS exploit, which beats me by nine seconds.
2056.63s - 2058.91s |  That's too bad.
2058.91s - 2064.17s |  So we have to wait for the slot to reopen.
2064.17s - 2067.73s |  So yes, just wait for about two weeks,
2067.73s - 2070.45s |  and the slot will reopen.
2070.45s - 2073.41s |  Right?
2073.41s - 2075.89s |  Wait until it starts to reopen?
2075.89s - 2077.13s |  Yep.
2077.13s - 2081.89s |  But do you know the nftable will be disabled from April 1st?
2081.89s - 2086.35s |  What?
2086.35s - 2087.95s |  Actually, there's an announcement
2087.95s - 2089.51s |  on Kernel CTF Discord.
2089.51s - 2091.95s |  It says they are going to disable nftables,
2091.95s - 2095.95s |  along with the switch to Kernel version 6.6.
2095.95s - 2099.15s |  It starts from March 15th, but actually, they
2099.15s - 2100.39s |  delay the switch.
2100.39s - 2103.51s |  So it's from April 1st.
2103.51s - 2107.63s |  OK, so what's the date today?
2107.63s - 2109.23s |  It's actually March 22nd.
2112.27s - 2115.15s |  The Kernel CTF slot opens about every two weeks
2115.15s - 2117.07s |  on average, which means I won't be
2117.07s - 2122.35s |  able to exploit it before they disable the nftables.
2122.35s - 2125.55s |  So on my third Kernel CTF attempt,
2125.63s - 2131.32s |  I still could not get a bounty, which is pretty sad.
2131.32s - 2135.40s |  To sum up my journey, first, I have hope.
2135.40s - 2139.52s |  And then I found bug one, a lifetime issue.
2139.52s - 2142.80s |  And it collides with another submission.
2142.80s - 2146.92s |  Then I found the bug two, a how-to-use-one-day.
2146.92s - 2151.88s |  But I used the wrong config, so it's unusable.
2151.88s - 2156.64s |  Then I found the bug three, a locking issue.
2156.66s - 2161.04s |  But I lose the submission race, and nftable will be disabled.
2161.04s - 2163.36s |  So it's killed.
2163.36s - 2165.80s |  However, there's a final plot twist.
2170.84s - 2175.16s |  A few days after, I found there are some changes
2175.16s - 2178.28s |  in the Kernel CTF spreadsheet.
2178.28s - 2181.48s |  The person who previously got ahead of me
2181.48s - 2186.12s |  is disqualified due to a bug collision.
2186.52s - 2189.72s |  So this time, I finally take over the slot
2189.72s - 2194.82s |  and report it to Google.
2194.82s - 2197.30s |  And this month, I got my Kernel CTF bounty
2197.30s - 2201.46s |  from Google, which is my first Google VIP bounty.
2201.46s - 2210.84s |  I'm so proud of myself.
2210.84s - 2211.88s |  Thanks.
2211.88s - 2216.40s |  So well, I think this concludes my Kernel CTF journey.
2216.40s - 2225.53s |  And thank you, guys.
2225.53s - 2226.91s |  Is there any questions?
2226.91s - 2235.80s |  Yeah.
2235.80s - 2237.72s |  If there's any questions, you can come up here
2237.72s - 2238.60s |  to this microphone.
2238.60s - 2257.95s |  Thank you.
2257.95s - 2259.79s |  Thank you.