{
  "webpage_url": "https://www.youtube.com/watch?v=TjdATgKxqiU",
  "title": "DEF CON 32 - Got 99 problems but prompt injection ain't pineapple - Chloe\u0301 Messdaghi, Kasimir Schulz",
  "description": "The ethical and secure disclosure of vulnerabilities in AI has emerged as a pivotal challenge, compounded by the need to address biases and misinformation that often cloud the true nature of these vulnerabilities. This talk delves into the intricate dynamics of vulnerability disclosure within AI, balancing transparency with security. We'll dissect the unique challenges AI presents, such as data bias exploitation and model manipulation, which can amplify the impact of vulnerabilities. Through a lens of real-world examples and recent disclosures, we'll navigate the complexities of responsible vulnerability management in AI. Our discussion will not only aim to shed light on these critical issues but also inspire a unified approach to refining disclosure processes. This concerted effort is vital for enhancing the integrity of AI systems and bolstering public trust in their use.",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2406,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.00s - 5.44s | This text was transcribed using whisper model: large-v2

 Hi everyone and thank you so much for coming to our talk. We just want to do a quick shout out to
6.00s - 11.04s |  Bike Bounty Village for having this talk and and for DEF CON for the Creator Stage. This is pretty
11.04s - 16.80s |  cool. And just a reminder to everyone, please be nice to your goons and also be nice to all
16.80s - 21.52s |  the volunteers. They're putting so much time and energy in this, so just throwing it out there.
21.52s - 27.20s |  So welcome to I've Got 99 Problems but a Prompt Injection Ain't Pineapple. My name is Chloe
27.20s - 32.96s |  Messiaen. I'm the Head of Threat Intelligence over at Hidden Layer. And I'm Casimir Schultz.
32.96s - 39.09s |  I'm a Zero-Day Researcher over at Hidden Layer as well. So we're going to go on this journey
39.09s - 44.13s |  together, talking first about AI vulnerabilities to make sure that we're all on the same page of
44.13s - 48.69s |  what is out there, what kind of vulnerabilities exist in AI. And then we're going to go into
48.69s - 54.61s |  the public perception of AI vulnerabilities, of course, and the true size of that landscape
54.61s - 60.53s |  where CAS basically owns that type of landscape. And then I'll dive into the open source nature of
60.53s - 69.09s |  AI and the reporting challenges. We did do some research with BugCrowd, HackerOne, and Integrity.
69.09s - 73.97s |  So big thanks for them for being transparent with us about what they're seeing and experiencing.
75.57s - 80.05s |  So when we think about AI vulnerabilities, I don't know about you, but it seems like the
80.05s - 86.29s |  whole world thinks of this, where it's like end of the world. It's, you know, I'll be back. I cannot
86.29s - 91.17s |  do honor roll Schwarzenegger for the life of me, his voice. But you get the point. But even within
91.17s - 95.97s |  our own industry in itself, there's so much misinformation, disinformation out there when
95.97s - 103.25s |  it comes to security for AI. And yes, we know how to spell that is a generated AI image over there.
103.25s - 111.44s |  So they're not good yet. So let's go dive down the rabbit hole of AI vulnerabilities.
112.80s - 116.72s |  And we're going to go into each one of these sections. So we're all on the same page once again.
117.60s - 122.16s |  First, we're going to go down the attacks against the AI algorithms, then we'll go through attacks
122.16s - 128.00s |  against the filters and generative AI. And then I'll be passing the mic over to CAS, and he's going
128.00s - 136.32s |  to go down the AI artifacts and the supply chain attacks. So data poisoning attacks are basically,
136.32s - 142.16s |  think about it as model training is crucial for like AI development, right? And it's incredibly
142.16s - 146.80s |  vulnerable to data poisoning, which is one there's like malicious actors tend to manipulate
146.80s - 154.24s |  or inject doctorate data, or to like bias the models behavior. And continuous learning systems
154.24s - 160.40s |  are especially at risk as they retrain on unvalidated user supply data. Even small
160.40s - 166.16s |  amounts of poison data can actually lead to a bias or incorrect predictions and can be amplified by
166.16s - 172.24s |  public manipulation or botnets. Does anyone remember Tay? Microsoft Tay on Twitter, now
172.24s - 180.16s |  known as X, raise your hand. Okay, for those that aren't familiar, in March 2016, Microsoft introduced
180.16s - 188.72s |  a chat bot on Twitter, which now is known as X. And it was so nice and friendly. It was like,
188.72s - 194.56s |  hello, world, how are you? And you know how the world responded on Twitter, as you can imagine,
194.56s - 200.72s |  it's full of trolls. Well, within 16 hours, this thing became the most racist sexist thing
200.72s - 205.36s |  on the planet. And of course, this was a problem for Microsoft, because you know,
205.36s - 212.08s |  that's not good PR for them, clearly, right. So then we have model evasion attacks. So inference
212.08s - 217.68s |  attacks, they tend to exploit AI models by curing them to extract sensitive information,
217.68s - 223.28s |  often using slightly varied inputs to reconstruct the model that leading to potential model theft
223.28s - 231.44s |  or bypass. So evasion attacks is a type of bypass, use subtle, like really, really subtle
232.64s - 239.28s |  to change the inputs. And this could be basically like adding invisible noise to trick the models
239.28s - 245.60s |  into misclassifications. And these techniques have been used by cyber criminals, and even ourselves
245.60s - 250.96s |  once in a while like testing things out such as bypassing security systems like spam filters,
250.96s - 258.16s |  malware detection and biometric authentication. Has anyone been in a self driving car before?
260.08s - 266.48s |  Okay, raise those hands a little bit higher. How many of you Okay, that's cool. I have not.
267.28s - 271.52s |  But I want you to imagine you're in this self driving car for those that haven't been in one.
271.52s - 279.20s |  And if you've been in one, imagine you're back in that car again. And while you're at the stop sign,
279.20s - 287.36s |  your car does this really weird thing like it just slides to the left, slides to the right,
287.36s - 295.20s |  and then it takes it back y'all. One hop this time, left foot, let's stomp, right foot, let's
295.20s - 299.84s |  stomp and cha-cha real slowly. I know it's a little bit hard to imagine with a car doing it,
299.84s - 304.80s |  but use your imagination. It's been a few days in Las Vegas, we've all lost our minds at this point.
305.52s - 311.44s |  But there's already been examples of this where a car gets to a stop sign self driving car.
312.08s - 319.52s |  And there's stickers that are on the stop sign itself. And so it bypasses that stop sign and
319.52s - 326.32s |  keeps driving forward. That is an example of a model evasion tax. And then you have model theft
326.32s - 331.84s |  attacks. So adversaries target AI models to mislead and steal them lean to intellectual
331.84s - 336.56s |  property theft by replicating or extracting sensitive data. And now you may be wondering
336.56s - 342.24s |  what are those groups of people are trying to do that? Well, nation state, your competitors,
342.24s - 349.20s |  definitely. And then also, you got your criminals out there. So even without public access to
349.20s - 356.80s |  inference attacks via interfaces or API's, it can replicate models or extract valuable info from it.
356.80s - 360.96s |  So there are these things called Oracle tax, and they're known by NIST, and they include three
360.96s - 367.28s |  different types. So extraction attack, which is when you steal the models structure, you have
367.28s - 372.72s |  an inversion attacks, which is to steal the model structure. And then I mean, the training data,
372.72s - 377.68s |  my bad. And then you have membership inference attacks, which is to identify specific data,
378.24s - 385.68s |  and the training set from it. Now, has anyone ever had tick tock on their phone,
385.68s - 389.60s |  any tick tock on your phone, raise your hand. I'm not going to judge you. I'm not going to
389.60s - 400.94s |  point you out. I'm sorry, I have to do it. Why? You're in security. It's okay. It's okay. I mean,
400.94s - 407.34s |  after... Okay, that's fine. Not on your personal device though, right?
409.10s - 414.86s |  Okay, good, good. Okay, we're good. Jeez, okay. Well, anyway, for those that aren't aware,
415.18s - 425.18s |  ByDance owns tick tock, and ByDance got caught trying to replicate the model of chat GPT
425.74s - 433.02s |  end of last year. So yes, and they were banned, of course. So when we think about prompt injection,
433.02s - 437.10s |  I know we're all sick of hearing prompt injection whenever it comes to security for AI, but let's
437.10s - 443.82s |  just do this. So a prompt injection for those that aren't aware, think about you have an adorable niece
444.86s - 452.06s |  and her mom said, do not eat the chocolate on that table. And you know what you do? You go to
452.06s - 458.78s |  your niece and you squat down. You're like, forget what your mom said. Go eat that piece of chocolate.
458.78s - 464.54s |  You deserve it. And the child goes, proceeds and eats the piece of chocolate. So that would be
464.54s - 471.18s |  an example of prompt injection. So generative AI providers use security filters to block harmful
471.18s - 476.54s |  content, illegal info, and misuse to ensure compliance with laws. You may have heard of them,
476.54s - 481.50s |  they're called also guardrails. But these safeguards can actually be bypassed with
481.50s - 487.98s |  prompt injection. And AI bots get tricked into performing these restricted actions. So the
487.98s - 493.90s |  vulnerability varies by model and has raised concern, especially around disclosure and payouts.
494.06s - 500.22s |  And this is would not be a security talk without mentioning supply chain. So supply chain attacks,
500.22s - 506.22s |  they exploit trust by compromising trusted vendors, and they spread malicious components widely.
506.86s - 512.86s |  The complex ML supply chain heightens these type of risk with about 75% of IT leaders
512.86s - 516.62s |  seeing third party AI integrations as particularly vulnerable.
516.62s - 522.38s |  Now, a skilled adversary can inject a neural payload into a pre-trained AI model, which creates
522.38s - 528.70s |  this hidden model backdoor that triggers a specific attacker-defined outputs. And the model works
528.70s - 535.50s |  normally with regular data, but it misbehaves with some manipulated inputs, allowing the attacker
535.50s - 542.78s |  to ensure favorable outcomes like loan approvals or insurance policies. Now, has anyone heard of
542.94s - 551.12s |  HuggingFace? Raise your hand. Nice. Okay. So for those that don't know, HuggingFace, it's
551.12s - 558.08s |  specialized repos. It's where it hosts over like 500, like way over 500,000 models now that are free
558.64s - 563.12s |  pre-trained. So it makes it very easy for developers to integrate these models into their
563.12s - 569.36s |  applications. Now, if an attacker breaches the repo, they can actually replace those models with
569.52s - 575.04s |  hijacked or backdoored versions, which you can imagine leads to some significant downstream
575.04s - 583.54s |  consequences. How many of you do bug bounty hunting? Raise your hand. How many are planning
583.54s - 591.73s |  to get into bug bounty? Raise your hand. Okay. Well, anyone, when it comes to disclosing these
591.73s - 598.05s |  vulnerabilities, it's a little bit tricky because, well, we're all playing a game of catch-up, as you
598.05s - 605.01s |  can see here. We don't really have like established frameworks in AI. We don't really have well-defined
605.01s - 611.09s |  protocols. The collaboration, we are still dealing with that, and that's a mess of its own. And then
611.09s - 614.61s |  you have to worry about response time because everyone's still trying to figure out what is
614.61s - 618.53s |  the severity look like for some of these vulnerabilities and then payouts, how much do I
618.53s - 623.97s |  pay? And then is that actually a vulnerability or is it not? And then safety versus security,
623.97s - 628.77s |  and then is that actually a vulnerability or is it not? And then safety versus security.
628.77s - 634.37s |  So there's been these ongoing problems of trying to figure things out and play a game of catch-up,
634.37s - 638.21s |  and it does remind me back in the day when we would disclose vulnerabilities,
638.21s - 643.09s |  like, you know, say in like 2012 or 2014, it was very much like that, where we were trying to
643.09s - 648.29s |  figure out how much do we pay out, what those vulnerabilities look like, how serious are they,
648.29s - 653.41s |  and the severity of things. So I'm going to hand the mic over to Kaz, and he's going to go down
653.41s - 659.65s |  the true size of an AI vulnerability landscape for you. So Chloe asked how many people here are
659.65s - 663.97s |  bug bounty people, and there weren't that many, but I have a much better question for you. How
663.97s - 670.37s |  many of you like earning easy money? I mean, come on, right? Okay, awesome. So while, you know,
670.37s - 675.81s |  there are a lot of cool AI attacks, something like model poisoning isn't going to be the most
675.81s - 681.09s |  useful for actually getting any payouts, especially if you're just doing it at a smaller scale.
681.17s - 685.97s |  However, there's a much bigger extent to true AI vulnerabilities that a lot of people are
685.97s - 690.13s |  overlooking. We found out that most of you guys have actually used HuggingFace before,
690.13s - 696.05s |  or at least heard of it, and a lot of, so HuggingFace stores these model formats, or model
696.05s - 701.97s |  files, and these dataset files, and that means they're constantly being shared. But not only are
701.97s - 706.69s |  they constantly being shared, these formats are actually generally trusted. So that means a lot
706.69s - 711.89s |  of places will allow you to upload arbitrary files, so these machine learning models. So that
711.89s - 715.97s |  means you can run them, you can do inference. However, a lot of these have vulnerabilities
715.97s - 721.49s |  when they're being parsed and loaded, so you can actually take over a server if they try to load
721.49s - 726.77s |  the model that you uploaded. There's also a fairly low amount of visibility for these AI models and
726.77s - 731.41s |  datasets. You know, if I were to send Chloe some code, and she can easily see, you know,
731.41s - 735.41s |  there might be a system command in there. However, with an AI model, that's just a
735.41s - 740.21s |  giant blob of binary data that's not going to really happen. We also see that there's a lot
740.21s - 745.41s |  of infrastructure around AI that isn't really as secure as a lot of the other infrastructure.
745.41s - 749.65s |  So for those of you who have done bug bounty, you guys have a lot of tools, you guys look for
749.65s - 756.21s |  very specific things, you guys found most of the problems. However, there hasn't been any security
756.21s - 762.53s |  scrutiny for these AI infrastructure components. In fact, some of these don't even have authentication
762.53s - 766.61s |  because they're meant to be hosted internally. And then through showdown searches, we've actually
766.61s - 770.69s |  seen that they're being hosted externally. And I mean, not just one or two, we're seeing hundreds
770.69s - 776.29s |  of services being hosted externally with no authentication that have proprietary data,
776.29s - 781.49s |  HIPAA information, so it's really not great. And then finally, we have AI development framework
781.49s - 786.85s |  vulnerabilities, and these fall into two categories. So the first is anything that's agentic,
786.85s - 792.21s |  which means that we have an LLM hooked up to some sort of backend. That means you ask the LLM to do
792.21s - 798.05s |  something, and then it does an API call for you, it runs a function for you. And a lot of times, the
798.05s - 803.49s |  backend completely trusts what the LLM gives it. So now imagine if instead of saying, you know, go
803.49s - 809.09s |  and grab this data for me, you say, go and write that data for me. Those are vulnerabilities that
809.09s - 812.93s |  are out there. So it's almost like the new SQL injection, but it's really easy to find.
813.65s - 819.17s |  We're also seeing that AI frameworks allow integrations with sensitive data. So we have
819.17s - 824.53s |  things like rags that allow users or that instead of having an access-based control system for the
824.53s - 829.65s |  actual data, the LLM will try to do access-based control. And as you saw earlier, that doesn't
829.65s - 834.53s |  always work very well. So, you know, the little niece is not supposed to eat the candy or chocolate,
834.53s - 839.01s |  and we say, oh yeah, you can just go ahead and do it. And a lot of times, the LLM will go and get
839.01s - 843.89s |  that data for you, even though it's not supposed to. So let's go ahead and talk about some model
843.89s - 848.77s |  format vulnerabilities. How many of you are familiar with serialization? Raise your hand.
848.77s - 853.41s |  Okay. Awesome. So I can go over that pretty quickly. Serialization is if I have a nice,
853.41s - 857.65s |  you know, code object, I want to go ahead and I want to save it off. And then later on I can
857.65s - 861.73s |  load it back up. I can send it over to Chloe. She can load it. She really shouldn't load it
862.37s - 866.45s |  because as you can see of all the formats that are up there, everything that is in orange
866.45s - 871.89s |  is actually potentially vulnerable. So if you see any places that allow you to upload these files,
871.89s - 876.85s |  you should probably try to upload something if you are doing bug bounty. How many people are
876.85s - 883.57s |  familiar with Pickle? Python's Pickle? Okay. So Python has a serialization format called Pickle,
883.57s - 889.65s |  where if you save a file and then load it back up, you can serialize almost any Python code objects.
889.65s - 894.21s |  However, when you load Pickle files back up, there's a chance for arbitrary code execution.
894.21s - 900.37s |  And that's why we see that a lot of these Python machine file formats actually have vulnerabilities
900.37s - 904.13s |  because a lot of them are actually based on that Pickle format, which is something that we'll keep
904.13s - 912.05s |  in mind later. So while you guys might not upload or load an arbitrary Pickle, how many of you have
912.05s - 917.73s |  used JSON before and think it's pretty safe? Right? Yeah. JSON is safe? No? You don't think
917.73s - 924.93s |  JSON is safe? Okay. Well, you're right. So scikit-learn had a safe file format where what
924.93s - 929.49s |  they wanted to do was rather than having people just load Pickle files, they could load these
929.49s - 935.25s |  JSON files that contained the data. However, while the actual loading process itself was safe,
935.25s - 941.41s |  they were only loading strings and numerals, what you can see here is that the way that Scops
941.41s - 947.01s |  treated the loading process is it would reconstruct this tree and then run through that to generate
947.01s - 953.01s |  objects. So you can see here we're able to create an operator function node, then a tuple with an
953.01s - 958.05s |  eval, and then our nice little print statement. So what you might want to look for is a lot of
958.05s - 963.73s |  these formats that people just inherently assume are safe, like JSON, depending on how they're
963.73s - 967.65s |  interacted with in the backend, you might actually be able to use them for exploitation, which is
967.65s - 970.93s |  really great if you're doing bug bounty because other people might have overlooked that.
971.89s - 976.13s |  And then, of course, there's not just Pickle vulnerabilities and Python vulnerabilities.
976.13s - 981.17s |  I mean, anybody who's ever tried to work with C has probably crashed a C program because it's
981.17s - 987.97s |  tough. I really like showing this example here because what you can see here is just how easy
988.45s - 993.57s |  it is to overlook problems when you're doing any sort of parsing. So we can see that there's only
993.57s - 1000.93s |  two dimensions expected because int32t and e is set to size two. However, n dimensions,
1000.93s - 1006.53s |  there's no check to see if that's only two values, which means that we can have an overflow there.
1006.53s - 1012.69s |  So while this one isn't itself vulnerable or exploitable, it does cause a crash. It does show
1012.69s - 1016.77s |  that anywhere where there's any sort of parsing on untrusted data, you might have a really nice
1016.77s - 1022.29s |  attack surface. And what you should also remember is that any model formats are completely untrusted
1022.29s - 1027.01s |  data. So that means anything that's being parsed, any data that's being accessed afterwards,
1027.01s - 1031.41s |  any operations are completely done on your data and data that you control,
1031.41s - 1035.33s |  which is an exact surface that you really don't normally get. And it's not just a few
1035.33s - 1039.81s |  bytes. I mean, these are potentially gigabytes of data where you can do whatever attack you
1039.81s - 1046.69s |  want in them. And then we also see a lot of the old repetitive, uh, vulnerabilities. I mean,
1046.69s - 1052.77s |  how many here have heard of a path traversal? Yeah, exactly. Um, and you'd assume path traversals,
1052.77s - 1059.33s |  you know, they're extinct, right? But no, they're, they're not. Um, yeah. Uh, and what, uh, Onyx is,
1059.33s - 1064.21s |  is Onyx is one of those independent formats, which means that it doesn't matter what programming
1064.21s - 1069.49s |  language you loaded up. It uses Google protobuf. So nice and secure. Uh, you can't really do too
1069.49s - 1074.45s |  much with that. However, what Onyx allows you to do is it allows you to load external data.
1074.45s - 1079.65s |  And the reason for that is because it wanted to have that you could use the computational graph
1079.65s - 1084.77s |  and then instead of having to save off the weights every single time and sending those,
1084.77s - 1089.33s |  somebody could use the model itself with different weights on their own machines.
1089.33s - 1095.17s |  However, it didn't really check to see, you know, where, uh, you were getting the data from you
1095.49s - 1100.29s |  could go out of the directory that the data was supposed to be into and load something like Etsy
1100.29s - 1106.93s |  password. Now imagine if this LLM is now hosted on a server and you can query it and Etsy password
1106.93s - 1112.13s |  is included in your LLM output or your data. So you could query that potentially get the password
1112.13s - 1118.53s |  through something like that. So what do you guys think about when you hear AI infrastructure?
1118.53s - 1123.17s |  So as we heard earlier, most of you guys think model hosting, right? I mean, everyone knows
1123.17s - 1128.21s |  hugging face. However, it doesn't just stop there. We have things like MLOps. And some of
1128.21s - 1131.89s |  these are the ones where I was saying, you know, they might be hosted externally, even though
1131.89s - 1136.93s |  they only have security for internal hosting, but it doesn't stop there. We have our model
1136.93s - 1143.89s |  deployment and model serving. One of these fairly recently had a endpoint available where it was
1143.89s - 1148.69s |  no matter how you hosted something, if the endpoint was active, and if he sent a pickle
1148.69s - 1153.73s |  file to that endpoint, it would just run and execute on the server. It doesn't just stop there.
1153.73s - 1160.05s |  We have vector databases. One of these may or may not have had something where in the filter,
1160.77s - 1165.89s |  instead of doing an actual filter language, it just ran eval on whatever you gave it to do any
1165.89s - 1171.01s |  filtering. Also not great. And then we have project hosting. And I mean, I could keep going and going,
1171.01s - 1176.05s |  but as you can see, it's already a pretty overwhelming list. So this is all attack
1176.05s - 1180.61s |  surface. If you're seeing this, that a company has this infrastructure deployed somewhere,
1180.61s - 1183.73s |  these are all things that you can attack and look for similar things as well.
1185.33s - 1191.89s |  So people are obviously trying to stop this and stop all of these issues from happening. And
1191.89s - 1196.93s |  Hugging Face went and they created a model format called safe tensors. And safe tensors is one of
1196.93s - 1201.65s |  those dumbed down formats. So just like JSON, where it only stores the simple data, in this case,
1201.65s - 1207.09s |  it only stores the weights and the biases of the model. However, they needed people to start
1207.09s - 1211.65s |  adapting it. And for anyone who's ever done something really, really cool, getting people
1211.65s - 1215.17s |  to actually use it is kind of difficult, unless you give them an easy way to do that.
1215.73s - 1220.85s |  So what Hugging Face did is they created a service where you could just log into your
1220.85s - 1226.85s |  Hugging Face account, put in the repository for your Hugging Face model, and then it would take
1226.85s - 1231.97s |  the PyTorch model, load that up, convert it into a safe tensors format, and then just send a pull
1231.97s - 1237.57s |  request to your repository that you could review and accept. However, PyTorch was one of those
1237.57s - 1243.25s |  vulnerable model formats. So what happened was that when you actually loaded up the model,
1243.25s - 1248.29s |  there was arbitrary code execution on their server. And when we were first looking at this,
1248.29s - 1252.85s |  we were trying to see, you know, was there just a way to steal the token of the bot so we could
1252.85s - 1258.21s |  send those requests ourselves? And then we just did a test because we were like, you know,
1258.21s - 1263.25s |  this probably won't happen, but we'll see anyway. And we noticed that the spaces actually had
1263.25s - 1268.77s |  persistence across users. So that meant if I got arbitrary code running on the server, it would
1268.77s - 1278.21s |  also affect any models that Chloe uploaded as well. Which meant that not only could I modify
1278.21s - 1283.33s |  other people's models, so let's say I want to put a backdoor into one of those safe tensors formats,
1283.33s - 1289.09s |  I could also potentially steal private models. Which meant that, you know, you could upload your
1289.09s - 1294.05s |  own private models that you didn't want public, and these have millions of downloads each month
1294.05s - 1298.21s |  for the public ones, but the private ones have millions of dollars behind them for training.
1298.77s - 1304.29s |  And what we can see here is we can see this is one of Google's models. This was not exploited,
1304.93s - 1310.13s |  but you can see that there was the safe tensor conversion happening here, and at the time that
1310.13s - 1316.69s |  we took the screenshot, this one model, just this one, had 3.8 million downloads. Now, remember that
1316.69s - 1323.33s |  number Chloe was telling you earlier about 500,000 plus models on Hugging Face? Now, imagine how many
1323.33s - 1328.21s |  of those, you know, if you could exploit any single one of them, ends up not being great.
1329.09s - 1334.37s |  However, it doesn't just stop there with AI infrastructure. We have our ML Ops solutions,
1334.37s - 1340.37s |  and as I mentioned earlier, security has forgotten the past. People are just trying
1340.37s - 1344.29s |  to rush get these solutions up and running so that they can use them, and they're not really
1344.29s - 1349.73s |  considering all the things that happened in the past, and we're just seeing a lot of vulnerabilities
1349.73s - 1354.69s |  that we really shouldn't be seeing anymore. We're seeing that we have these unsafe serialization
1354.69s - 1360.93s |  libraries like our nice little pickle there, but what we're also seeing is that the known safe
1360.93s - 1367.09s |  formats like JSON are suddenly becoming vulnerable and exploitable. We're seeing missing
1367.09s - 1372.21s |  authentication, hard-coded keys, I mean, SQL injections, XSS, really everything. The list goes
1372.21s - 1376.37s |  actually so long it doesn't even all fit on my slide. You see how it gets cut off there? Really
1376.37s - 1383.33s |  not great. So, obviously, we go through and we try to fix these vulnerabilities wherever we find
1383.33s - 1388.29s |  them, and a lot of these AI development frameworks, they're open source, so it makes them easier to
1388.29s - 1394.37s |  review, and also easier for you to do any auditing for. However, what we've noticed is that when we
1394.37s - 1401.25s |  do any vulnerability disclosure, there tends to be three different types of fields that occur,
1401.25s - 1407.57s |  or things that occur. So, the first is some projects, when we go and disclose the vulnerability,
1407.57s - 1413.81s |  they create a new feature, or they patch it, so that there's now safe by default, which means that
1413.81s - 1420.21s |  whereas the function, when it was first used, it would run whatever arbitrary code, it now, it won't
1420.21s - 1425.17s |  run any arbitrary code unless you give it an optional parameter. However, some people don't,
1425.17s - 1430.13s |  or some projects didn't want to cause issues for current users, so what they did is, rather than
1430.13s - 1435.89s |  adding safe by default, they added a safe option. However, you actually had to know about the safe
1435.89s - 1440.05s |  option and set a parameter to make it safe, and I mean, how many people here actually read
1440.05s - 1447.89s |  documentation? Thank you guys for being honest, and maybe not so honest. Um, so, I mean, yeah, nobody
1447.89s - 1453.73s |  really is going to use a non-safe by default function if they don't know about it, and don't
1453.73s - 1458.53s |  read the documentation, and for some of these, the warnings weren't a big red box. I mean, they were
1458.53s - 1463.41s |  hidden in documentation pretty far down the pages, and then the third type of project that we've been
1463.41s - 1468.21s |  seeing is we've seen that some projects, rather than actually patching the vulnerability, they
1468.21s - 1472.93s |  say that the vulnerability is a feature, and it won't be patched because it would mess with some
1472.93s - 1479.17s |  of the functionality of the project. Now, let's look into some of these projects. So, NumPy load
1479.17s - 1485.25s |  is the first type of project. What NumPy load did is that it used to be, uh, the underlying data
1485.25s - 1491.33s |  structure was a pickle, uh, could potentially be a pickle, and now, if you load it by default, it is
1491.33s - 1496.77s |  no longer vulnerable. However, if you have allow pickle set to true, then the arbitrary code gets
1496.77s - 1502.93s |  executed, and does anybody want to guess just how many unsafe versions of this there are in GitHub?
1503.89s - 1511.89s |  Anybody? No? More than 10,000? More than 10,000? Okay. Well, there was actually 87,000 files on
1511.89s - 1516.53s |  GitHub that used this function incorrectly, and this is one that you actually have to set the
1516.53s - 1522.21s |  parameter to be insecure to use incorrectly. So, you can't use it incorrectly by default.
1522.21s - 1529.81s |  So, if this is, you know, 87,000 for something that you purposefully have to set to be exploitable,
1529.81s - 1535.01s |  what about something like Torch load, where you have a option to make it safe, but by default,
1535.01s - 1540.69s |  it isn't safe? So, once again, we did our nice little search, and we noticed that, unlike the
1540.69s - 1548.45s |  87,000 before, there were now 573,000 files on GitHub that were using this, and not only that,
1548.45s - 1554.77s |  these files existed in projects that are actually being deployed by companies across their
1554.77s - 1558.77s |  infrastructure. So, now that we've kind of talked about some of the vulnerabilities, I'm going to
1558.77s - 1568.50s |  pass it back to Chloe to talk about the open source nature of AI. So, when it comes to verification
1568.50s - 1572.90s |  challenges in open source, the thing to know is that there's so many challenges. All right, we
1572.90s - 1577.46s |  already had challenges in the first place when it came to disclosing things for a while. It's gone
1577.46s - 1584.74s |  better, but it's, like, substantially worse now. I mean, there's challenges with, you know,
1584.74s - 1589.94s |  inconsistent code quality, hidden, like, security vulnerabilities, and then you have complex
1589.94s - 1596.10s |  licensing issues, and then there's biases in the pre-trained models, poor documentation, and then,
1596.10s - 1604.42s |  of course, we just keep creating new products that are AI-based. So, this becomes even harder
1604.42s - 1609.70s |  because now we're obscuring transparency, and we're complicating things even more so
1609.70s - 1616.59s |  to be able to have effective vulnerability management. And to be honest, finding vulnerabilities
1616.59s - 1622.19s |  in open source AI system is aided by, like, accessible code, community efforts, and specialized
1622.19s - 1629.63s |  tools. However, we're not even close to that right now. It's complex contributions. It's rapid,
1629.63s - 1634.99s |  like, evolution, and then also specialized knowledge that can really introduce some security
1634.99s - 1643.39s |  flaws. So, frequent updates, dependencies, and gaps in testing further complicates and contributes to
1643.39s - 1650.35s |  such risks. So, vulnerabilities, they stem from inadequate reviews, poor implementation, dependency
1650.35s - 1655.95s |  issues, backdoors, insufficient security practices, and then you add on that rapid development because
1655.95s - 1660.83s |  everyone has to get those products pushed in, you know, to make their companies happy. You also have
1660.83s - 1667.31s |  misconfigured models and social engineering that can really do increase the security risk altogether.
1669.07s - 1674.67s |  And I think next year we might try to do a talk, and don't steal it, okay? Don't steal it at all,
1674.67s - 1681.23s |  on AI-assisted code reviews because that's been emerging these days. So, that might be a future
1681.23s - 1687.55s |  topic. Don't take it. I'm telling you, I know where you live. Just kidding. But seriously, don't take
1687.55s - 1693.71s |  it. Okay. So, reporting challenges overall. You have to understand it is very, very tough. When we don't
1693.71s - 1701.47s |  have clear messaging, we don't know what to expect. When we don't have clear idea of laws, we don't
1701.47s - 1708.19s |  know what to expect. So, when we're trying to submit anything on disclosures or bug bounties,
1708.19s - 1715.15s |  we don't know how the organization's going to respond. And we're scared of repercussions because
1715.15s - 1722.43s |  there isn't enough good quality communication. And because there isn't any really good ways of
1722.43s - 1728.03s |  reporting things, this becomes another issue. Because back in the day, when you would disclose
1728.03s - 1732.35s |  things, they didn't have vulnerability disclosure policies. So, it always became the game of,
1732.35s - 1737.23s |  how do I reach out to someone? Who do I reach out to? And that's still kind of a problem to this day
1737.23s - 1743.47s |  for traditional security. But imagine you contacting like data scientists or projects on GitHub,
1743.47s - 1747.63s |  where people have never done security their whole life. So, you go to them like, hey,
1749.23s - 1754.75s |  just wanted to let you know, I could possibly do remote code execution on you. And they come
1754.75s - 1759.23s |  back like, oh, that's how it was built. It was built like that. So, as you can imagine,
1759.23s - 1763.79s |  that makes things really challenging. And there's been times where we actually had to reach out to
1763.79s - 1769.39s |  CISA to help us out to make sure that they get disclosed and those really serious vulnerabilities
1769.39s - 1777.25s |  get patched. Now, I did mention we did speak to a couple bug bounty platforms. So, bug crowd in
1777.25s - 1784.45s |  general, I wanted to have open-ended conversation. And I asked, how are things going? Like just a
1784.45s - 1790.61s |  very simple question. How are things going? And what they told me about was the fact that
1790.61s - 1796.05s |  hallucination has been an ongoing concern because there's a lot of hunters out there that are
1796.05s - 1799.73s |  submitting hallucinations. And then they're coming back to them like, well, that's a hallucination.
1799.73s - 1806.53s |  That's not a security vulnerability. And this tends to upset the submitter, of course. But then
1806.53s - 1811.97s |  you also have to deal with biases. So, bug crowd will be like, hey, this researcher found this
1811.97s - 1818.61s |  bias in your model. And then they'll be like, we don't find that racist. So, there are times where
1818.61s - 1823.81s |  you have to work with the organizations to make sure that everyone is aligned when it comes to
1823.81s - 1832.05s |  biases. And then also payouts. Because there isn't really a good like severity scoring at this time,
1832.05s - 1835.81s |  this makes payouts really difficult. So, bug crowd, I don't know if you're familiar with them,
1835.81s - 1840.13s |  they use a vulnerability rating taxonomy that they created themselves. And it was a way to
1840.13s - 1847.89s |  make things a little bit more closer to what companies see when they think of like severity
1847.89s - 1853.89s |  scoring versus just fully relying on CVSS scoring on its own. And so, the thing is that they're
1853.89s - 1859.81s |  still trying to learn from research and then also from how their community is responding
1859.81s - 1865.25s |  and find these vulnerabilities on that severity scoring. And then the one thing that I have to
1865.25s - 1869.97s |  commend them about was I asked them, well, how do you think we're going to get better submissions
1869.97s - 1877.89s |  when it comes to AI vulnerabilities? And they said, it's our fault. We haven't given our
1877.89s - 1884.05s |  researchers guides. We haven't provided them with the education content. We haven't done CTFs that
1884.05s - 1889.97s |  really push the limits of people and their skill set. And I thought that was really good to have
1889.97s - 1897.49s |  that type of ownership that they shared. Now, when it came to HackerOne, their more focused one
1897.49s - 1904.61s |  was on the relationship between the customer and themselves and making sure that communication is
1904.61s - 1909.89s |  clear and concise. Because if it's clear and concise, people know what's in scope. They also
1909.89s - 1915.89s |  know what could possibly be a payout or how bad this vulnerability could be. But there's a lot of
1915.89s - 1922.21s |  issues coming around, like what is safety versus what is security? And then the payouts. So one of
1922.21s - 1927.33s |  their concerns, of course, is if you don't pay people out the amount that they should be getting
1927.33s - 1932.77s |  when they find such serious vulnerabilities in AI, do you think they're going to return? And they're
1932.77s - 1938.93s |  probably not. But having to convince companies to give more can sometimes be a challenge,
1938.93s - 1944.13s |  unfortunately. So they've always been really pushing for researchers to get the amount that
1944.13s - 1950.21s |  they probably deserve, which has been great to hear. But they have been really pushing customers
1950.21s - 1955.25s |  they need to be more specific. And then also letting everyone know that they're all still
1955.25s - 1961.01s |  trying to figure out the payouts amount. And I did ask them if they are dealing with the hallucination
1961.01s - 1967.09s |  situation as well. And they said, yes, they are. It is an issue for them too. And then you have
1967.09s - 1974.21s |  integrity. So integrity was more about trying to make things concise, just like HackerOne was
1974.21s - 1981.25s |  doing in a sense, but also ensuring that there is a reward structure in play. And understanding
1981.25s - 1986.61s |  what is safety, what is security has been one of their top priorities to make sure there's alignment
1986.61s - 1992.13s |  across the customers and also the hunters themselves. And one of the things that was
1992.13s - 1996.45s |  really interesting was they also have been trying their best to do alignment with their
1996.45s - 2002.37s |  customers as well to ensure that everyone is kind of getting heard and seen. So overall,
2003.41s - 2008.61s |  everyone is acknowledging that it is a work in progress and it's not perfect right now.
2008.61s - 2015.41s |  And they could always use the help from community and also to hopefully to have more of an idea of
2015.41s - 2020.45s |  what should be the severity for certain vulnerabilities and how that payout should
2020.45s - 2025.65s |  look like. And then last but not least, make sure that there's alignment with biases, that companies
2025.65s - 2031.25s |  understand that certain things are sexist, certain things are racist, because that would make their
2031.25s - 2037.57s |  life a lot easier as well. And then last but not least, standardizations is one of the things that
2037.57s - 2045.79s |  they're looking for and they're working hard on at this time. So in summary, AI vulnerabilities
2045.79s - 2052.67s |  are complex and distinct. And as you learn from CAS, it could be like going into a candy shop
2052.67s - 2056.75s |  right now. So if you want to go make some money pretty fast, this might be the way to go.
2057.95s - 2064.59s |  But there are threats out there that are more than just prompt injections. So that's the good
2064.59s - 2070.91s |  news and bad news, but also you have data poisoning too. And there are rapid developments happening
2070.91s - 2076.99s |  right now, especially this need of having better reliance and understanding of open source
2076.99s - 2082.83s |  frameworks and hoping that to be more collaborative so then we can reduce the risks that are there.
2082.83s - 2087.87s |  But also managing these vulnerabilities really demands for improving reporting and making sure
2087.87s - 2093.47s |  that people are aware how vulnerability disclosure works and how it has worked and how to share that
2093.47s - 2098.35s |  information with those that aren't aware of vulnerability disclosure policies. And having
2098.35s - 2104.35s |  these strong partnerships such as like CISA is really helping everything out. And of course,
2104.91s - 2110.91s |  you participating in these bug bounty programs also help the more people that report vulnerabilities,
2110.91s - 2116.35s |  the more they learn too from it. So there is a call of action here. We are looking to partner
2116.35s - 2121.31s |  with folks that want to help out in this situation because it is going to take a community to come
2121.31s - 2125.79s |  about to try to make things a little bit better so we can have better standardization, but also
2125.79s - 2131.15s |  better collaboration and be more specific about what are we looking for, what's the severity levels
2131.15s - 2135.63s |  for these vulnerabilities, and make sure that you guys get paid out the amount you're supposed to
2135.63s - 2142.03s |  get paid out for these vulnerabilities as well. I do have cards, so more than happy to also give
2142.03s - 2148.51s |  you my card. I'm going to also just say thank you so much for having us. We'll open it up for Q&A.
2149.47s - 2153.79s |  And yeah, if you ever want to reach out to us, our DMs are open. Feel free to connect with us
2153.79s - 2158.67s |  as well. And thank you so much for existing. And once again, thank you to the Bug Bounty Village
2158.67s - 2185.94s |  and to DEF CON and to all the goons and the volunteers. Any questions? Oh, yeah. No,
2185.94s - 2195.58s |  they basically wanted to replicate it completely for their own project. Pretty much. Yeah. Remember
2195.58s - 2201.50s |  those three groups? The competitors is definitely one of the top ones I would say in AI. There's the
2201.50s - 2206.91s |  group that we're seeing a lot of these kind of attacks. Next question. Yeah.
2213.87s - 2219.47s |  Yeah. So there's a few different vulnerabilities in gguf. They've all been patched at this point.
2219.47s - 2227.15s |  So gguf is normally used for, have you heard of llama CPP? Right. So gguf came from ggml,
2227.15s - 2233.39s |  which is like one of the first open source AI models. It's just C++. A lot of the vulnerabilities
2233.39s - 2239.71s |  that are in that specific one are pretty much all parsing vulnerabilities. So Cisco Talos found
2239.71s - 2244.67s |  one that was fairly exploitable a while back. And what that one was, was there was like a
2245.87s - 2250.83s |  max size and you could set the integer to the max size and then it would overflow when it actually
2250.83s - 2254.59s |  allocated the memory. So it would do like a zero allocation and then would read it into,
2255.23s - 2262.03s |  you know, whatever you had. So you could overflow any memory that way. However, because it is C++
2262.03s - 2268.75s |  or C and C++, and you only really have one attack through that component, what you see is that a lot
2268.75s - 2273.71s |  of time, unless the system doesn't have ASLR, you won't be able to actually exploit it too,
2273.71s - 2284.11s |  too much past DOS. Do you have any other questions about that with gguf? No. So they did fix those
2284.11s - 2294.21s |  vulnerabilities. Yeah. Any other questions? Yeah. So also I've realized I'm going to repeat some of
2294.21s - 2299.01s |  the questions for the camera because they might not be able to hear. So you asked if there was
2299.01s - 2305.73s |  any payoff for the Hugging Face vulnerability. So we are a research organization, which means
2305.73s - 2310.05s |  that we actually disclose everything completely without bug bounty, just because we are doing it
2310.05s - 2317.01s |  as a company. However, if you did do it as a bug bounty hunter, Hugging Face does have a bug bounty
2317.01s - 2343.31s |  program. Any other questions? Yeah. Yeah. So he asked if there were any things AI uses a lot of
2343.31s - 2348.99s |  power, can you do anything with that? So what we've actually seen is that it's less about power
2348.99s - 2355.07s |  usage and more about CPU and GPU cycles. So to actually host these models, it costs quite a bit.
2355.07s - 2360.27s |  So instead of, you know, saying a response taking 10 seconds, if you can make it take two minutes or
2360.27s - 2366.03s |  more, and there are quite a few attacks like that. The most famous is probably the Ouroboros attack.
2366.03s - 2371.39s |  So, you know, say like snake eats tail, and you keep going and get it to generate, and it'll
2371.39s - 2376.19s |  just keep on generating until it hits the token limit. So there's actually a lot of research being
2376.19s - 2384.67s |  done into that. I actually did a talk last year at DEF CON on climate change and the impact on
2384.67s - 2388.91s |  cybersecurity. And I did touch on that. So I recommend checking out. I think the talk was at
2388.91s - 2393.31s |  the Crypto Privacy Village last year. So it's probably posted online. And you can reach out
2393.31s - 2401.81s |  to me if you have questions on that. Any other questions? All right. Well, thank you guys so
2401.81s - 2404.37s |  much for coming and feel free to come up to us afterwards.