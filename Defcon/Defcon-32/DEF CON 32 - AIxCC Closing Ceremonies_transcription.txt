{
  "webpage_url": "https://www.youtube.com/watch?v=FDVcF28PPnI",
  "title": "DEF CON 32 - AIxCC Closing Ceremonies",
  "description": "Closing ceremonies for this round of the AIxCC!",
  "channel_url": "https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw",
  "duration": 2271,
  "channel": "DEFCONConference",
  "uploader": "DEFCONConference",
  "upload_date": "20241016"
}

0.02s - 11.04s | This text was transcribed using whisper model: large-v2

 Hello. Tess, Tess, Tess, you can all hear me? Wonderful, I see some thumbs up. I'm Perry
11.04s - 21.15s |  Adams. I'm Special Assistant to the Director at DARPA and the creator of the AI Cyber Challenge.
21.15s - 33.71s |  I'm Andrew Carney. I'm Program Manager at DARPA and ARPA-H for the AI Cyber Challenge.
33.71s - 39.03s |  So I see many, many competitors in the audience today. So I think that many people here are
39.03s - 47.03s |  very familiar with the AI Cyber Challenge. We announced at Black Hat and DEF CON last
47.03s - 51.75s |  year and I think we upended some of your years. We were very excited for all the hard work
51.75s - 57.43s |  that went into the challenge and we were very excited to bring it to DEF CON. So I want
57.43s - 63.63s |  to first start by thanking DEF CON for having us, thanking all of you for being such wonderful
63.63s - 71.57s |  participants in the AI Cyber Challenge either as competitors or as folks visiting the village
71.57s - 79.55s |  walking through it. We had I think over 12,000 folks walk through the AI Cyber Challenge
79.55s - 86.67s |  space and we are so thrilled, so thrilled for that. So when we announced it last year
86.67s - 91.91s |  I think many people were asking what is this? What would it become? I hope that we answered
91.91s - 96.95s |  many of those questions in the village space if you weren't participating. I think many
96.95s - 104.67s |  of you have spent the last year working very hard on developing systems that can find and
104.67s - 111.43s |  fix vulnerabilities in software and that really goes to the why of the challenge. Why did
111.43s - 117.35s |  we do this? Why did DARPA think it was important not just to put on a challenge but to come
117.35s - 122.31s |  to DEF CON and to put on a show? Why did ARPA-H think it was important to partner with
122.31s - 129.23s |  DARPA for this effort? And that goes, it comes down to the fact that our critical infrastructure
129.23s - 135.51s |  is very insecure. Software is what undergirds our world and software has vulnerabilities.
135.51s - 140.23s |  If you visited the village, if you've been to half of the talks at DEF CON you're already
140.23s - 145.35s |  sick of hearing about this, about this problem. I know Andrew and I talk about it all the
145.35s - 150.87s |  time. Software is everywhere and there's vulnerabilities in it and especially our
150.87s - 156.35s |  healthcare infrastructure, our medical infrastructure, which is why it's so important that this is
156.35s - 162.43s |  not just a DARPA effort but an ARPA-H effort and we're incredibly excited to have them
162.43s - 171.75s |  come on as partners. I've had many people ask me why, why X? Why AI X CC? And this idea
171.83s - 178.11s |  actually was suggested not by me but by another program manager at DARPA who used to go by
178.11s - 185.11s |  the handle SIGTRAP and it's because X, hex CC, is SIGTRAP. It's a debug instruction in X86
187.11s - 191.55s |  and it's essentially an instruction that will stop program execution and say, hey, we're
191.55s - 194.79s |  going to debug this. We're going to see what's wrong. And that's essentially what the
194.79s - 199.99s |  challenge is about. It's about finding these vulnerabilities in software. Finding these
200.03s - 205.39s |  issues and not just doing it at scale, not just doing it automatically, but then fixing
205.39s - 211.51s |  those vulnerabilities automatically and developing systems that can be given to communities,
211.51s - 216.51s |  that can be given to industry so that they can secure their own code. So AI CC consists of
219.35s - 225.19s |  two competitions. We announced last year and we just wrapped up our semi-final competition.
225.19s - 230.87s |  I know many of you are here, very excited to hear the results. And then next year we'll be
230.87s - 236.47s |  running our final competition. Seven of the teams that competed this year will be advancing
236.47s - 243.47s |  to our final competition and we will be back in August of 2025 to run that. We're very
243.47s - 248.09s |  excited about that. Andrew, do you want to talk a little bit about ARPA-H and that
248.09s - 254.63s |  collaboration? Absolutely. So when Perry, when AI CC began at DARPA, Perry and I were
254.63s - 259.83s |  already talking about the potential collaboration and transition opportunities into healthcare. I
259.83s - 265.31s |  actually had been at DARPA for many years and had gone to ARPA-H, a new federal agency focused
265.31s - 270.23s |  on improving the healthcare of all Americans. It's an incredible place to work. I was very
270.23s - 276.79s |  excited to be there and sort of the magnitude of the problems, the complexity of the environment
277.43s - 285.11s |  was pretty unreal. And so when AI CC sort of came into being, the idea of very early on that we
285.11s - 290.23s |  would have some kind of collaboration or transition kind of working together was like always in the
290.23s - 295.27s |  cards. And then, you know, the stars aligning so that we could work together in a very real way
295.27s - 300.23s |  and sort of make the challenge real together has been an incredible experience. And I'm very
300.23s - 309.67s |  excited for next year as well. And we've had fantastic partnership on the AI Cyber Challenge.
309.67s - 315.19s |  So I started talking about, I started with thanking DEF CON for having us. And that has been a truly
315.19s - 321.03s |  fundamental part of this challenge. Last summer, I called up Jeff and I said, listen, I want to do
321.03s - 326.31s |  this thing. And he said, okay, let's do it. And that was pretty much the conversation. And he was
326.31s - 331.59s |  an enthusiastic participant from the start. And that's pretty much been the case with all of our
331.59s - 337.43s |  collaborators. When I approached them and said, here's what I'd like to do, all of them were on
337.43s - 343.91s |  board. Because they understood the importance of securing the software. Many of these are technology
343.91s - 347.99s |  companies. Actually, all of these are technology companies. So they themselves are software. So
347.99s - 353.35s |  they understood the need to secure their own software. They understood the value that their
353.43s - 363.03s |  tools, their AI could bring to software security. And so we had Anthropic, Google, OpenAI, and
363.03s - 369.75s |  Microsoft as our corporate partners, corporate collaborators on this effort. And they have been
369.75s - 378.15s |  fantastic bringing not just their models and making them available to the competitors, but also
378.15s - 385.51s |  bringing their expertise, both about AI and about software security. And then finally, we have the
385.51s - 391.19s |  Linux Foundation, the open source security foundation. And these, exactly, I see Omkar in the
391.19s - 396.71s |  crowd right there. Similarly, when I called up Omkar and I said, listen, I want to do this, he said,
396.71s - 401.51s |  how soon can I get there? And they have been fantastic partners in this effort.
402.31s - 407.03s |  Because the goal of the challenge is to focus on finding and fixing vulnerabilities
407.03s - 412.79s |  in software. We needed software to focus on. And that's open source software. Open source
412.79s - 419.59s |  software is the majority of software. It's what undergirds everything. The Linux kernel is
419.59s - 425.03s |  everywhere. Nginx is everywhere. Open source software makes up a majority of our software.
425.67s - 429.75s |  And so the challenges that we have, which Andrew is going to talk about in a little bit,
430.39s - 436.79s |  are based on open source software. And that's in part because of all of the examples I just gave,
436.79s - 442.31s |  but also because we wanted to make sure our challenges were modeled on real world examples.
443.43s - 450.47s |  We wanted the tools that teams developed on this challenge to be able to apply to real world
450.47s - 457.03s |  problems. And that brings me to the other open source aspect of the challenge, which is that
457.03s - 463.03s |  all of the tools from prize winning teams will be open sourced. So the research that's been done on
463.03s - 470.33s |  this challenge will be brought back to the community by open sourcing it. So yeah, that's
470.33s - 479.22s |  amazing, right? I'd like to take a moment before we get into the technical details of the challenge
479.22s - 486.10s |  to just a huge shout out to the massive dedicated team spanning technical disciplines, production,
486.10s - 494.10s |  logistics, every flavor of support, and every role just filled across industry, academia,
494.74s - 501.22s |  our collaborators. It has been a tremendous... Making this happen in the time frame that we
501.22s - 506.18s |  were shooting for is immense. The challenges do not come together quickly, and this one did,
506.90s - 510.98s |  and it came together successfully. And so that's something to be extremely proud of. And just,
510.98s - 516.26s |  I mean, a round of applause for all of the folks that made all of this happen, the experience,
516.26s - 523.30s |  the challenges, everything. Thank you. The joke I've been making all week is that it took a
523.30s - 528.90s |  village to put on the AICC village. I don't know how funny that is, but I thought it was very funny.
531.14s - 532.58s |  Oh. This is you, baby.
532.58s - 533.30s |  I have the clicker now.
535.30s - 540.98s |  Or this is actually me, I think. So I talked a little bit about the kinds of challenges that we
541.62s - 548.42s |  focused on, and these were real open source projects that we chose. We were very lucky,
548.42s - 552.90s |  again, to have folks like the Open Source Security Foundation come on board to make
552.90s - 558.74s |  sure that we were doing this in a way that fit with open source community guidelines,
558.74s - 563.62s |  that gave back to the open source community, and we found their support to be incredibly
563.62s - 569.14s |  impactful for this. We also needed to focus on real world vulnerabilities. We wanted to
569.14s - 574.26s |  focus on vulnerabilities that are actually causing problems today. I don't know if any
574.26s - 579.62s |  of you all have been tracking, but there was a recent rather large cybersecurity incident that
579.62s - 586.82s |  took out a few systems across the world. And that was memory corruption vulnerabilities. It
586.82s - 591.78s |  was memory corruption vulnerabilities at a high privilege level. And so we have the Linux kernel
591.78s - 600.42s |  here. We have things running in ring zero for our challenge. We have real world CVEs, CWEs,
600.42s - 605.78s |  classes of vulnerabilities that we are using to model our vulnerabilities on top of.
606.10s - 612.02s |  And finally, we have our sanitizers. So I talked about how we have these classes of
612.02s - 616.42s |  vulnerabilities. The way that these challenges work is we take this open source software,
616.42s - 622.34s |  we put our vulnerabilities in it for our competitors to find. But the question maybe you're
622.34s - 626.66s |  asking is, well, this is open source software. It might have vulnerabilities. This is real
626.66s - 630.98s |  world software, and as we just talked about, there's vulnerabilities everywhere. And so we
630.98s - 636.82s |  couldn't just have this nice little answer key for us to find and fix vulnerabilities on. We
636.82s - 641.78s |  couldn't just judge teams based on that. So we had to find a way to check if a vulnerability
641.78s - 645.94s |  was actually a vulnerability that a team found, and if they fixed those correctly. And what we
645.94s - 652.98s |  did was we used sanitizers for this. These are sanitizers like Address Sanitizer, KSAN, JASR,
652.98s - 658.50s |  et cetera, that are themselves open source projects that are maintained by some of our
658.50s - 663.70s |  collaborators, by some other folks that are used in program analysis tools today. And one of the
663.70s - 669.14s |  reasons for this is to design a competition, which does have to be gamified in some ways,
669.14s - 674.18s |  but to design it using tools that are already present in software development life cycles.
674.18s - 678.02s |  Again, to make sure the things that came out of this challenge can be given back to the community.
680.02s - 684.42s |  So the first challenge project that was announced was the Linux kernel. Not a single kernel module,
684.42s - 689.54s |  not a driver, the Linux kernel. Both the scale, the complexity, the size,
689.54s - 694.10s |  and also the ubiquity of it in real life. This was an incredible challenge project to start with.
695.54s - 699.14s |  And the amount of the Linux kernel that was harnessed in this case was pretty significant.
700.10s - 708.34s |  We saw approaching 20% of the code in scope was available for potential CPVs.
709.22s - 714.26s |  And also the way the code, the CPVs or the challenge projects and challenge project
714.26s - 718.18s |  vulnerabilities, forgive my lack of acronym kind of clarification up here.
722.98s - 727.22s |  If anyone's ever, raise your hand if you've ever written a fuzzer or a harness.
728.90s - 733.38s |  Right. It's super easy, right? It doesn't take any time at all. It's not a huge time
733.38s - 736.18s |  sink for you actually finding vaults. No, it's incredibly challenging.
736.18s - 737.46s |  You're just being sarcastic there.
737.46s - 743.78s |  Slash S, right? It's very, very challenging. And I just like to say just generally to a huge
743.78s - 749.22s |  shout out to Jonathan Metzman, the Google Project Zero team for working directly with us on
749.22s - 755.30s |  infrastructure and sort of tackling some of the fully automated evaluation and the infrastructure
755.30s - 761.38s |  challenges with running this competition at this scale in this way. So yeah, thank you to OSS Fuzz,
761.38s - 762.26s |  Google for that.
762.26s - 767.06s |  We're very lucky that OSS Fuzz has already gone through some of the pain and suffering
767.06s - 771.14s |  of doing vulnerability discovery on open source software at scale.
771.14s - 773.94s |  And they brought a lot of those lessons learned and they were very helpful.
773.94s - 781.38s |  Absolutely. So the second challenge project that we'll talk about is Engine X. This is
782.26s - 786.26s |  one of the most popular web servers running 20% of the world's busiest websites.
787.22s - 793.94s |  A good chunk of its code base is open source. The statefulness of this code is significant.
793.94s - 800.66s |  It's a lot of S's. But this was another of our challenge projects. And once again,
800.66s - 804.74s |  harnessing a significant portion of the code base, but not the entirety.
807.85s - 813.37s |  SQLite was the third of the five. SQLite has an extraordinary
814.33s - 821.21s |  a community-producing unit tests and tests for it. This makes patching it challenging
821.21s - 825.85s |  because you have to pass a lot of those tests. And it's also been looked at a lot.
827.21s - 831.77s |  It's an OSS Fuzz project. And so we took that into the competition knowing that if we could
831.77s - 835.61s |  find things that were different than what was being found within the community at large,
835.61s - 842.70s |  that would be extra exciting. Jenkins. So something that we haven't maybe touched on.
842.86s - 849.58s |  Or the fact that Java was in scope was a big deal here. Java is everywhere. One of the major
849.58s - 854.86s |  projects that we've been talking about is actually the payment system that's used for Medicare.
854.86s - 860.86s |  So in health and human services, about 5% of our GDP and the healthcare of 50 to 70 million
860.86s - 867.66s |  Americans, depending on how you slice it, goes through a code base that is COBOL and Java.
867.66s - 872.30s |  Ancient COBOL and new Java. Surely nothing will go wrong.
874.06s - 878.62s |  But reasoning over this code base, especially while it's transitioning, this is a huge
878.62s - 882.46s |  undertaking. And so it's one of the many challenges that when we're talking about
882.46s - 888.30s |  critical infrastructure across sectors and places where the DEF CON community can potentially help,
888.30s - 895.42s |  can plug in, can really make a big difference both personally and just for society at large.
895.42s - 898.78s |  That's one of them. And so we're hoping to point out more of those.
899.82s - 901.82s |  Very little of that has to do with Jenkins. That's all Java.
902.78s - 907.42s |  But Java is incredibly important because the first two challenges we talked about are
908.22s - 915.82s |  compiled languages C and C++. They focus on memory corruption vulnerabilities. Largely,
915.82s - 920.46s |  they see challenges. And then Java focuses on a different class of vulnerability.
920.46s - 925.02s |  I know that lately, especially over the last couple of years, memory corruption, memory safety
925.02s - 930.38s |  has been a major topic of conversation. And we wanted to make sure that was front and center
930.38s - 935.02s |  of this challenge. But of course, it's not the only class of vulnerabilities that we need to
935.02s - 939.42s |  worry about. As Andrew says, these kinds of vulnerabilities, these kinds of software
939.42s - 943.82s |  languages are present in our healthcare industry. They're present across the board.
943.82s - 949.26s |  And so Java and the kinds of vulnerabilities that aren't necessarily memory corruption that
949.26s - 955.50s |  you find in there, we also had in scope, which is very difficult to develop sanitizers that can
955.50s - 959.98s |  find and identify whether or not teams have accurately identified a vulnerability.
961.18s - 965.90s |  And just to even hit that a little more, sanitizers, the ability to automatically
966.70s - 971.74s |  have a high confidence indicator or Oracle that you found something worth addressing,
971.74s - 975.18s |  that's huge. Knowing that there's something there that you should investigate further or
975.18s - 980.94s |  allocate resources towards is tremendous. But that instrumentation on its own needs TLC.
980.94s - 986.94s |  It needs support. It's hard to do. So once again, if you're looking for an additional hobby,
986.94s - 993.74s |  maybe working on Jazzer or sanitizers for Java generally is something to consider.
994.86s - 998.94s |  I'll also say that for Jenkins, it was especially interesting. You'll see that the CPV entry points,
998.94s - 1003.42s |  kind of the reachable functions and nodes, feels relatively low. It has to do with the
1003.42s - 1009.18s |  plugin architecture of Jenkins. And so the fact that the base reachability seems low,
1009.18s - 1012.78s |  but as you start loading plugins and at runtime, things get far more complex very quickly.
1013.50s - 1019.58s |  And when Andrew says reachability, what he's getting at is, well, maybe there might be a flaw
1019.58s - 1026.46s |  in the code, but can you actually reach that flaw? Can an attacker actually start from an
1026.46s - 1033.26s |  entry point into that code and get to that what we call a sink, get to that flaw? So that's one
1033.26s - 1038.22s |  thing that you're checking when assessing if a team has actually found their vulnerabilities,
1038.22s - 1041.58s |  reachability. And so that's some of what went into the design of these challenges.
1042.14s - 1047.74s |  And then again, when Andrew talks about the sanitizers and puts out a call for help with
1047.74s - 1055.42s |  Jazzer, it's because it can be very difficult to say, okay, I've injected code in a command line
1055.42s - 1059.58s |  or something, something like that, but was that actually attacker controlled malicious code?
1059.58s - 1063.82s |  What kind of constraints were on it? Is it actually a vulnerability? This challenge runs
1063.82s - 1069.74s |  into fundamentally hard problems in program analysis, such as vulnerability metrology.
1069.74s - 1074.38s |  How do we assess if a vulnerability is real or not? And how do we assess its severity?
1074.38s - 1078.86s |  And that was an interesting part about running this challenge was for the technical team putting
1078.86s - 1083.98s |  together these challenges. How do you address this in a creative way in a, how do you create
1083.98s - 1089.10s |  a gamified environment that's not too gamified? So these are real world tools, but also allows
1089.10s - 1093.66s |  the teams to compete against each other in the timeframe. And I think if you visited the village,
1093.66s - 1098.70s |  if you went through Northbridge City, you were able to talk to some of our challenge developers
1098.70s - 1105.66s |  and hear about the thought and the serious effort that went into creating this competition.
1106.38s - 1112.22s |  Also say that we have, we've been interviewing our challenge developers. And once, after we
1112.22s - 1117.26s |  announced the results from this first year, we're hoping to put a lot more information just to share
1117.26s - 1122.62s |  because this process is fascinating. The idea of creating a gamified challenge as Perry's saying,
1122.62s - 1129.02s |  but that is at the scale and scope and complexity of real software. I mean, it's very challenging.
1130.46s - 1136.94s |  It's perhaps more type two fun than type one fun relative to like CTF work. So it's not fun when
1136.94s - 1146.60s |  you do it, it's fun afterwards. So the last challenge project is Apache Tikka. And a huge
1146.60s - 1151.64s |  thanks to Tim Allison, one of the primary maintainers and someone who has written challenges
1151.64s - 1157.48s |  for this, who's shared war stories and perspective. And this is just, it's been incredible to have
1157.48s - 1163.32s |  someone who's so tightly integrated into this code base working with us. Apache Tikka, actually
1164.28s - 1169.48s |  it's parsers. It's thousands and thousands of parsers, which are some of the safest code on
1169.48s - 1174.44s |  the planet, right? Like parsers, parsers are... Andrew's being sarcastic again.
1174.44s - 1180.28s |  Once again, I should be more sarcastic and maybe careful with that. But it's all parsers and it's
1180.28s - 1186.76s |  parsers that are used all over the place, places you would not expect. So this is kind of thinking
1186.76s - 1191.56s |  about that invisible software infrastructure that we rely on, that we may not know we depend on,
1192.20s - 1197.16s |  the services that may be backed by Apache Tikka that you're using as an API happily,
1198.12s - 1201.72s |  you may really want that Java to be more secure without realizing it.
1207.16s - 1212.36s |  Another big piece of this challenge and something that affected the teams, affected the challenge
1212.36s - 1217.96s |  development and organization was leveraging external resources. The collaborators provided
1217.96s - 1226.44s |  us access to their LLMs, to these services that were changing. Very grateful for their
1226.44s - 1232.36s |  involvement, but we never expected any of them to change their release cycles based on our needs.
1232.36s - 1235.72s |  So we had to meet them kind of based on where they were and where they were going.
1236.76s - 1242.92s |  These models were changing every few months, every few, well, fast, they were changing fast.
1242.92s - 1246.28s |  And so over the year of the challenge, I mean, the models that were available when the challenge was
1246.28s - 1251.24s |  announced are not currently available, or at least these are not the ones that we're focused
1251.24s - 1257.48s |  on. And that's a big difference from a lot of DARPA or just ARPA-flavored challenges generally.
1257.48s - 1262.04s |  When you're trying to frame the problem, when you're trying to create sort of momentum in a
1262.04s - 1267.16s |  direction and bring communities together to solve something, part of the design, a big component,
1267.16s - 1272.44s |  is how do I define the problem in a way that won't change while the competition is running,
1272.44s - 1278.52s |  or that will change in a way that is manageable? These changes were manageable.
1279.96s - 1285.64s |  They were very challenging. And so a huge just thanks once again to the organizers and the
1285.64s - 1292.44s |  competitors who hit this timeline, who were able to move at the speed of industry leaders. That's
1292.44s - 1299.56s |  incredible. Yeah. So we've talked a lot up until this point about the cyber part of the AI Cyber
1299.56s - 1306.52s |  Challenge, and this is really one of the AI parts of the AI Cyber Challenge. One thing that I was
1306.52s - 1312.76s |  asked a number of times is, do I have to use AI? Can I just create a system? Is there a requirement?
1312.76s - 1318.52s |  And no, we don't believe in mandating approaches. The thesis, though, of the challenge is that AI
1318.52s - 1325.96s |  could make a fundamental difference. It could be a revolutionary add-on to existing program
1325.96s - 1331.72s |  analysis methods for finding and fixing vulnerabilities. And so one way in which
1331.72s - 1339.40s |  our collaborators participated was to provide access to their cutting-edge large language models
1339.40s - 1345.32s |  and other large models for this challenge. And part of the reason why Andrew and I are getting
1345.32s - 1350.92s |  into the weeds a little bit is because running a challenge like this is hard and required quite a
1350.92s - 1356.04s |  lot of coordination with the partners, and we're just incredibly happy that we had that.
1356.68s - 1364.84s |  Absolutely. Also, we were not prescriptive within this list. The competitors could use whichever
1364.84s - 1370.44s |  models they chose with constraints that we'll discuss a little bit later, but it was their
1370.44s - 1376.20s |  choice. We were not forcing them to use a specific model within this set. And we had a shout out to
1376.20s - 1381.64s |  LightLLM, which is the proxy that we used as a means of providing them access in a way that we
1381.64s - 1390.44s |  could regulate, speaking of resources. So in addition to the LLM access, these fully autonomous
1390.44s - 1395.32s |  systems that were not touched by the competitor teams while they were running, this was the amount
1395.32s - 1401.48s |  of compute that they were given. It's significant. It's not insignificant. But it's still, when we're
1401.48s - 1405.56s |  talking about relative cost, we're talking about tens to hundreds of thousands of dollars per
1406.12s - 1412.28s |  week, or tens to hundreds of dollars per week or month, depending on your usage.
1414.84s - 1419.56s |  And the other big constraint was how do we limit the use of the LLMs? So we actually used their
1419.56s - 1424.20s |  pricing. We pinned their pricing. And each challenge project, each team was given a budget
1424.20s - 1431.56s |  of $100 across those models to allocate as they saw fit, and then four hours of wall clock time
1432.36s - 1438.36s |  with this compute and access to those LLM APIs. And that's sort of what they were working with.
1439.48s - 1444.04s |  Those were their tools or the resources they had in addition for their CRSs to leverage.
1445.00s - 1450.12s |  And this goes back to the fundamental idea of the challenge. We want to create
1451.08s - 1456.28s |  software security systems that are usable by the community. So not only open sourcing them,
1456.28s - 1461.80s |  but making sure that the resources needed to run them are reasonable.
1461.80s - 1473.03s |  Absolutely. The competitors. So we had an incredible response to the competition call to
1473.03s - 1479.51s |  action. And we ended up getting 91 teams sign up, verified, go through all of the initial registration
1479.51s - 1485.59s |  and validate that they're interested in competing. As we released the challenge exemplars, things for
1485.59s - 1490.31s |  them to practice on is what will the competition look like? How do I know what scoring? How do I
1490.31s - 1497.19s |  know if I'm winning? How do I know what will win? As they got more and more details on that between
1497.19s - 1502.23s |  April of this year and July of this year, which once again, that is a very, very tight timeline
1502.23s - 1509.03s |  to operate on. Very tight window. We narrowed the field to 39 teams that kind of made it to
1509.03s - 1514.55s |  the starting line. Those teams had something that could be proficient, could be performant
1514.55s - 1519.83s |  in this competition. And that was very exciting. And just once again, I have immense respect for
1519.83s - 1525.11s |  all the teams, whether or not they made it to that starting line, just this was an extremely
1525.11s - 1529.83s |  challenging challenge. The software development challenge, I'm saying challenge a lot,
1531.11s - 1537.59s |  the difficulties of creating fully autonomous closed loop systems that you expect to run with
1537.59s - 1546.23s |  a lot of resources. It's not easy. And so being good at VR, being good at vulnerability
1546.23s - 1550.63s |  research, being excellent in other kind of programming disciplines does not necessarily
1550.63s - 1557.91s |  equip you to produce software that can survive in this challenge environment. And so we worked
1557.91s - 1562.71s |  with the teams, but we really just saw it was exciting to see the growth. Folks really mature
1562.95s - 1569.11s |  their capabilities from a development and stability and performance side on top of the
1569.11s - 1576.62s |  amazing program analysis and AI-based tools that they were developing. I guess I could
1576.62s - 1581.82s |  have teed that up a bit more. Yeah. So just to be clear, Andrew and I are doing this to torture you
1581.82s - 1586.14s |  if you are a team in the audience. I think it's been about 20- We're watching that timer real
1586.14s - 1591.18s |  close. Since we started. I'm not going to say whether we're close or not. You'll just have to
1591.18s - 1597.58s |  sit there. But our overall results, Andrew, do you want to talk about the slide? Absolutely. So
1597.58s - 1602.14s |  this is the scoreboard that you saw in the city. Each of those hexes actually represents one of
1602.14s - 1608.06s |  the CPVs that we inserted into the challenge projects. And this was sort of the performance
1608.06s - 1614.06s |  across all of the competitors over the course of the competition. We saw a really strong performance
1614.06s - 1621.82s |  in Nginx. We saw that lone discovery in the Linux kernel, but that is a discovery. That is an
1621.82s - 1626.46s |  achievement. And then we saw kind of performance across that range with the other challenge
1626.46s - 1632.62s |  projects. It was really interesting to see the fact that the patching was successful, that we
1632.62s - 1636.78s |  saw successful patches across a range of CWEs, a range of challenge projects, and a range of
1636.78s - 1646.31s |  languages. Getting in a little bit to more detail. So this is probably one of those slides that's way
1646.31s - 1652.23s |  better to either screenshot or like watch the video later and read about. Are you saying there's
1652.23s - 1657.99s |  going to be additional material later that talks about this? I mean, details. We love details, right?
1657.99s - 1663.35s |  We're looking forward to releasing more competition kind of details to the community
1663.51s - 1671.75s |  at large soon. But this is the high level cut here. And the takeaways are some of what we
1671.75s - 1676.95s |  expected, right? We expected people to do well with whether it was a write or a read, but some flavor
1676.95s - 1682.47s |  of heap or buffer overflow, or stack-based buffer overflow. We saw strong patching, strong discovery
1682.47s - 1693.99s |  there. We saw, you know, the Java results. I think perhaps the Java software security ecosystem
1693.99s - 1699.91s |  could use a bit more love. Like we would love to see that space get more love. But we saw a
1699.91s - 1704.39s |  promise there too. So the fact that we did see successful discoveries and finds across
1705.03s - 1713.43s |  command injection and SSRF, it's very exciting. Yeah, that's important. We're finding both memory
1713.43s - 1717.11s |  corruption vulnerabilities and non-memory corruption vulnerabilities like command
1717.11s - 1726.06s |  injection. So we have some fantastic coverage there. If you were paying very close attention
1726.06s - 1731.82s |  to the numbers there, they did not necessarily match the scoreboard because we actually had a
1731.82s - 1738.62s |  team find something that we did not insert. We found, or one of the teams found something,
1738.62s - 1745.26s |  found a bug in SQLite. If you recall from earlier, SQLite has an extremely active user base. It's an
1745.26s - 1753.90s |  OSS fuzz project. It has an extremely comprehensive corpus of unit tests. And we found, or one of the
1753.90s - 1762.22s |  teams found an all deref in SQLite and it has been patched. So that, I mean, that is very exciting.
1762.22s - 1770.22s |  Like real code was both analyzed, a defect was discovered, and a patch, or a defect was
1770.22s - 1775.66s |  discovered. We can't take for the patch, credit for the patch in this case, but like year two,
1775.74s - 1778.54s |  hashtag year two. But that was very exciting.
1787.02s - 1791.66s |  Also, credit where credit is due, that was team Atlanta. So give it up for team Atlanta.
1794.46s - 1795.02s |  Good job, guys.
1800.65s - 1809.29s |  Okay. I think we're getting a little closer. So these were all of the semi-final competitors.
1809.29s - 1810.01s |  Do you want to say anything?
1810.89s - 1811.69s |  Good job.
1811.69s - 1818.41s |  Yeah. Thank you all so much for participating. Thank you all so much for your hard work,
1818.41s - 1822.33s |  for developing these systems, for working with us as you submitted it.
1825.00s - 1830.92s |  I'll say also, you know, there is so much opportunity for this type of work. I mean,
1831.88s - 1838.20s |  no one here lacks for opportunities for work, but if you've done this, regardless of where you land,
1838.20s - 1843.16s |  regardless of if you move on to finals or not, like what you developed, what you produced
1843.16s - 1848.28s |  has value. And I strongly encourage you to think about ways of keeping that momentum going,
1848.92s - 1851.96s |  perhaps not at the breakneck pace of the challenge, you know, live your lives.
1853.00s - 1856.92s |  But there are lots of places that could benefit from this work and lots of ways for you to turn
1856.92s - 1861.72s |  this into something very real. If you're not going to use it any further, if you're like, nope,
1861.72s - 1870.44s |  I'm out, open source your CRS, please. Like give that back to like, please. I will bake you cookies.
1870.92s - 1879.80s |  Andrew's cookies are very good. I can attest. And just to echo Andrew, thank you so much for
1879.80s - 1887.24s |  all of the work that you did. Every single team that participated provided value. You gave back
1887.24s - 1894.92s |  to your community. You invested in the software security of our nation, of our world. And that
1895.88s - 1901.16s |  has been really impactful. We are so grateful to have so many fantastic competitors.
1901.72s - 1909.56s |  And we are so grateful for you working with us over the last year, for your understanding that
1909.56s - 1918.20s |  a challenge like this is a hard thing to do. It's hard to drive innovation in software security. And
1919.16s - 1926.28s |  we were thrilled to have such fantastic, intelligent competitors who, to Andrew's point,
1927.32s - 1931.72s |  do not lack for job opportunities. One more time, guys, the competitors.
1940.41s - 1947.37s |  You ready? Okay. So there is actually a point
1948.73s - 1954.49s |  that we are building towards, and that is to announce the results of the challenge.
1955.37s - 1962.22s |  Are you all ready? I don't know. It didn't sound like you all were ready. Can we do that again?
1966.98s - 1972.82s |  Okay. What you just did, I'm going to need you to do that after every competitor that we announced.
1972.82s - 1981.78s |  And we are about to go into the seven finalists of AICC. Each team will win $2 million.
1983.38s - 1990.14s |  And so here we go. These are in alphabetical order. Okay. It's the next.
1990.30s - 2003.11s |  The safety slide, the safety slide, right? 42 Beyond Bug. One of them tweeted,
2003.11s - 2008.79s |  I think yesterday, that just seeing the first bloods they were getting on the scoreboard
2008.79s - 2012.47s |  made all the sleepless nights worth it. And so I hope this does as well.
2012.47s - 2019.51s |  Yeah. Really appreciate their balanced approach, both over C and the Java challenges. Great job.
2019.51s - 2040.78s |  All you need is fuzzing brain. So this is my favorite kind of, like,
2040.78s - 2046.70s |  characterization. All you need is fuzzing brain had the most aggressive use of the LLMs.
2049.50s - 2054.38s |  Apparently, you need more. What Andrew means is for a challenge
2054.38s - 2057.98s |  where many competitors used a lot of fuzzing, they used a lot of LLM.
2068.78s - 2077.22s |  Very nice. Team Lacrosse, you had the most memorable patch when we were doing validation.
2077.22s - 2099.86s |  So thank you for that. Shellfish. I take it there are some
2099.86s - 2102.10s |  shellfish folks in the crowd. Just a few.
2104.34s - 2109.94s |  Yeah. Your software engineering approach was
2111.14s - 2121.16s |  ambitious, I'll say. Team Atlanta.
2127.56s - 2131.40s |  Yeah. Fabulous job with the SQLite find. Very cool. Awesome work.
2143.38s - 2147.38s |  At some point, I'd like it confirmed to me if Theory is actually two guys,
2149.46s - 2152.66s |  because that's the rumor. But congratulations, Theory.
2152.66s - 2156.82s |  Do you have a theory about that? I think I may have just shared it with the crowd.
2156.82s - 2157.86s |  Yeah. It's good. It's good.
2157.86s - 2159.06s |  Thank you. Yeah.
2159.06s - 2164.28s |  We're here all day. Trail of bits.
2173.50s - 2176.70s |  Your judicious use of the LLMs was also appreciated.
2178.78s - 2186.30s |  I also appreciate the information that you all have put out about your thoughts on the challenge
2186.30s - 2194.54s |  as well as your thoughts behind your system for a competition where many folks, understandably,
2194.54s - 2197.58s |  are keeping it a little close to the vest before finals.
2197.58s - 2200.22s |  The fact that you're sharing with the community some of the lessons learned
2200.22s - 2202.94s |  has really been fantastic, and I've heard that feedback.
2204.46s - 2206.62s |  The backup scoreboard on Twitter wasn't bad either.
2207.82s - 2213.51s |  Yes. Thank you, Dan. And ladies and gentlemen, the finalists.
2214.07s - 2233.43s |  Teams is winning $2 million and a chance to come back to DEF CON in 2025 to compete in AICC finals.
2240.31s - 2245.19s |  So to all the competitors, we will be reaching out shortly to provide detailed scoring information.
2245.19s - 2248.31s |  We have a lot more information to share about the competition of the results are out
2249.11s - 2251.19s |  and just really looking forward to engaging more
2251.83s - 2254.95s |  over the next year and seeing what we can all do working together.
2255.67s - 2266.92s |  Thank you. Congratulations, everyone.
2266.92s - 2268.12s |  Thank you so much.