**Introduction**
- **Alfredo Ortega**: Security researcher, bug hunter, and founder of Neuroengine.ai.

**AI-Powered Bug Hunting**
- **Crash-Benchmark**: Standardized evaluation framework for AI-driven static analysis tools.
- **AK1**: Simple bug-hunting AI agent with a model-agnostic design.
- AI-bughunting challenges: Early models lacked sophistication and privacy concerns limited the use of commercial AI models.

**Previous Work**
- Recent advancements in Large Language Models (LLMs) have led to their application in various fields.
- **Black Hat Presentation**: Generic view of tasks, including vulnerability detection and protection.
- **Google Project Zero**: Focused on exploiting vulnerabilities, concluded that LLMs have very basic exploitation capabilities.

**Crash Bench**
- An Infosec event where different AI models are evaluated on their bug-finding capabilities.
- Various examples, including real vulnerabilities, are given to the AI to identify bugs and generate reports.
- **Model**: Each AI company releases its own models, some open-source and others closed-source.
- **Local Open-Source AI**: Preferred for bug hunting to maintain privacy and avoid sharing code with third parties.

**AI Model Comparison**
- **Lambda3**: One of the most recent and best-performing models.
- **Lambda2**: Previous state-of-the-art model with limited understanding of code.
- **GPT-4**: Powerful model but not the best at bug finding, possibly due to strong alignment.
- **Mistral**: Another high-scoring model in the benchmark.
- **AI Model Alignment**: Models with strong alignment may deny providing answers related to bugs or exploits due to ethical constraints.

**Real-World Bug Example**
- **Integral Overflow Bug**: AI model successfully identified and reported the bug, replicating the human report.

**Demonstration**
- Live demo using a complex code example to showcase bug-finding capabilities.
- **QBD4**: Fuzziness allows it to find bugs even in challenging cases.
- **Cloud**: One of the best models, successfully identified a heap overflow vulnerability in the kernel.
- **OpenVC**: Developers fixed the vulnerability identified by the AI model.

**AI Limitations and Challenges**
- **False Positives**: AI may report non-exploitable vulnerabilities, requiring manual verification.
- **Exploit Development**: AI models currently struggle with this aspect, often requiring human intervention.
- **Business Logic Vulnerabilities**: LLMs may not understand business logic, limiting their effectiveness in certain domains.

**Automatic Patching**
- **Auto-Patching Software**: Extends auto-hacking by not only discovering but also patching vulnerabilities automatically.
- **Closed-Loop Confirmation**: Ensures that the generated patch doesn't break existing functionality.
- **JattyBD**: Tool used for automatic patching, generating multiple patches and tests for a library.
- **Obfuscation and Randomization**: Techniques applied to create a unique, harder-to-exploit version of the code.

**Key Takeaways**
- LLMs can assist in reviewing and discovering vulnerabilities, but human intervention is still necessary.
- AI-powered bug hunting will significantly reduce the number of bugs found and exploited.
- Custom versions of software with added security checks and obfuscation can be created, making exploitation challenging.
- Exploit development is not yet fully automated and may require additional training and information.

**Conclusion**
- AI-powered bug hunting and automatic patching show promising results, but further development is needed.
- The impact on the job market is uncertain, but adapting to new technologies is essential.