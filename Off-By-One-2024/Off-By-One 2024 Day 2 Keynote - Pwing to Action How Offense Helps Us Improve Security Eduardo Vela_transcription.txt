{
  "webpage_url": "https://www.youtube.com/watch?v=508ng3J1FgI",
  "title": "Off-By-One 2024 Day 2: Keynote - Pwing to Action How Offense Helps Us Improve Security: Eduardo Vela",
  "description": "Abstract\n\nIt\u2019s too easy to find problems in software today. Not enough people understand how attacks really work, and this is bad for defense. Both attackers and defenders need this knowledge, but we are always busy fixing today\u2019s problems.\n\nThere\u2019s not always enough time for making things stronger long-term. In this talk, I share how my teams have used offense to make defense better. We use weaknesses and exploits, study how they work, and then use this data to help organizations decide how to move forward.\n\nWe\u2019ll discuss using exploits to prioritize patches, motivate process changes, and ultimately create systems that are more resilient. Sometimes, we offer money to break things in new ways \u2013 and show us where to focus, sometimes, we turn on the heat on a broken system so it transforms into something better.\n\nThis is not always about fancy new insights, ideas or attacks. It\u2019s just about working together to make systems safer for everyone.\n\nSpeaker\nEduardo breaks things for a living, mostly the digital kind. Leads a team of Google\u2019s finest bug squashers, manages disclosure drama and spends an unhealthy amount of time poking at software and hardware in the name of science teaching them new tricks (that they really shouldn\u2019t be able to do)\u2026\n\nAims to make the digital world a more or less terrifying place, one exploit at a time. Eduardo often wonders why anyone trusts him with this much responsibility.\n\nHe\u2019s pretty sure this is due to a combination of luck, caffeine, and the fact that he\u2019s surrounded by people far smarter than himself. But hey, he\u2019s not going to question it too much.",
  "channel_url": "https://www.youtube.com/channel/UCmrsIbKdxEMBEefD8v8RigQ",
  "duration": 2044,
  "channel": "Off-By-One Comms Team",
  "uploader": "Off-By-One Comms Team",
  "upload_date": "20240831"
}

This text was transcribed using whisper model: large-v2

 OK, so thank you for the introduction.  My name is Eduardo, as I was introduced last.  And, oops, what is this?  Hopefully this works.  Oh, there we go.  Perfect.  That's me.  I work at Google in the Information Security Engineering team.  And mostly what we do is things related to making Google products safer.  And recently I spent a little bit more time, as he said, on CPU and Firefox.  And the things that I want to talk about today are related to the ways that we have been approaching,  the way that we did things internally for Google products to the outside world as well.  So our dependencies and open source and hardware dependencies.  Technically, what I do at Google is things related to vulnerability stuff,  mostly related to vulnerabilities found manually by people.  I don't do it.  We have a large team, but I manage one part of the team.  So, yeah, I was trying to make an agenda, but it was very complicated.  So I decided to summarize it as something else, which is simply a goal,  which is trying to write down how keeping exploit information secret makes offense cheaper,  defense more expensive, and how we, as part of developing more secure products,  are trying to reverse this trend.  And this is one of the reasons why we are here today  and why we like to engage with the security community, offensive and defensive.  I want to start by explaining more or less the information asymmetry example  with a very classical example you might know already.  These are four cars.  These four cars are all, from the outside, they look the same,  but one of them has bugs.  One of them has problems.  I hear that in Singapore it's very expensive to buy cars, actually,  but imagine this from a world where people buy very cheap cars.  From the perspective of the seller of the first car,  they want to sell it for $50,000, let's say.  I don't know if they are a good car or not, but let's assume it is.  The second car, the person wants to sell it for a little bit more, $55,000.  The reason is they both think that's how much their car is worth.  The third person thinks it's a little bit cheaper.  They want to get rid of it faster.  So they sell it for $49,000.  The last person, the one that knows they have problems in their car,  they can sell it for less, a lot less,  because they know the car has problems.  But the buyer will not know the car has problems until a lot later.  So they also can sell it for more expensive, because why not?  People will be like, I want the best car, and they will just...  You know there's these people that just go sort by price  and then get the most expensive.  This is bad, right?  This is bad for consumers, this is bad for users,  this is bad for everyone.  This is mostly depending on how much people are willing to pay for things.  People are willing to pay depending on the information that they have.  The average price for a car of this size is $52,500.  I just added it all and divided by 4.  In the scenario that I gave you,  let's imagine that the average price of a car is $52,500.  There is one car that is buggy out of five cars.  But then another person with a car with problems decides,  oh, that's actually a pretty good deal,  I also want to sell my car,  and they will add one more car to the mix.  So now there are two cars out of six.  And that will lower the price to $50,000.  Now the problem is that  now it is not worth it for some of the people without good cars  to sell their cars anymore.  So they might remove themselves from the market,  which leads to two out of three cars having bugs.  This is just generally an explanation,  it's called a market problem,  and it's a general scenario of how information asymmetry  from the buyer and the seller can result in market collapse  and no incentives to improve.  So, generally speaking, how could we solve a problem like this?  Any ideas?  AI.  Artificial intelligence?  Yes, maybe.  Maybe transparency as well.  So, good.  The way that I want to express this is that  the way that we react to security problems nowadays  is mostly as if they were a fire, or an incident,  or something that requires immediate attention,  or immediate response.  But that has the consequence of  after you are finished with the incident,  then you have maybe a slightly bad reputation,  a slightly worse product, slower, whatnot.  And that is technically not the best way to react to problems,  it's a very expensive way to react to problems,  and it is generally not good for your business,  or for your users.  A more generally better way to deal with security problems  is better to think about it as a medical, medicine problem,  which is like, rather than eating, you know,  free third-party dependencies from Node.js,  Polyfill.io, and all of these very tasty, high-calorie treats.  You might want to do better,  maybe things that are not as tasty,  like memory-safe languages,  or maybe do things that have a clear dependency graph,  and things like that, like verifiable legal systems.  And that, generally speaking, is difficult,  like it makes it, it's less appealing,  but it's complicated work.  Same for behavioral changes,  like maybe instead of launching things  without testing in production,  low coverage of testing, and so on,  maybe you have to make lateral changes,  like code reviews, or unit tests,  or continuous integration, and so on.  And all of these type of changes,  from one thing to another, are expensive.  Are expensive for developing features.  It increases development time,  which are things that developers have to worry about.  Developers have to worry about many, many components  of the things that they are working on.  Like, for shipping features in time,  it could be development time,  it could be the maintenance of the features that they have,  it could be the latency of the applications that they run,  bugs that are functional bugs,  that are introduced by accident,  maybe performance,  they want to make sure that the applications that they run  look and feel fast, and are fast for the users,  as well as capacity,  maybe there's not a lot of RAM in the cell phones,  maybe there's not a lot of storage on the database  to be able to run this.  And generally speaking,  all of that is what developers are thinking about  all the time.  So when you say,  oh, we want to add some additional features,  like lifestyle changes,  or dependency changes,  then you are essentially adding time  to their development time,  you are making them take more code,  you are adding latency,  you are adding more bugs to work on,  you are adding additional performance issues,  and you are requesting more capacity.  So all of that is difficult for you to be able to sell,  unless you have a good way to measure it.  So going to something a little bit more technical,  or back to something that maybe is easier to grasp.  So can someone spot the bug in this?  Where is it?  Here.  Do you have it?  Yes.  In this, which line do you think has the bug?  It's a very simple bug,  but this is just an example.  Any ideas?  The what?  Okay, I will not stand there.  Fascinating, okay.  The what?  On which line, yes?  Yes, exactly.  So this filter here is the very simple use of the free.  The way it works, very quickly speaking,  is that you have a very simple structure,  the structure has an allocation,  somewhere in memory,  in this case I decided memory 4000,  it sets the first field to 666,  then it frees the memory,  so it is given back to the allocator,  then it goes back,  another structure is allocated,  it reduces the last time structure of this allocation implementation,  it sets it to zero,  and then something is reducing the memory address from before,  that is supposed to be zero,  and it modifies the value,  and then instead of being zero,  as you expect,  and here what happens is that you have two memory addresses  that are pointing to the same thing.  Now, this is a very common, simple type of bug.  There is a simple fix to all of the use of the free bugs  that you can implement today in your application,  which is you go to the libc function,  you find libc free,  and then you return immediately.  This is 100% effective,  and it works the following way,  you have the same as before,  you free it,  and when you allocate again,  you get a new address,  and that's it, no more use of the free.  The problem with this is that  if you run this twice,  you start using more and more memory,  until you have so much memory,  that you run out of memory.  This is not great,  but it fixes the use of the free.  So the question, generally speaking,  will be here,  is it worth it to fix use of the free?  Now, it depends.  This actually was an extremely useful mitigation  for CTF in DevCon,  where you're allowed to do binary debugging,  binary patching,  and they just modified one,  like the allow function,  with, I think, A2,  well, they just changed the pointer,  so that it was essentially allowed.  And for them, it was worth it, right?  Like, they get flags for opt-in,  and the machine was,  any time that there was out of office,  it would restart.  So, out of memory, it would restart.  So then it was actually pretty good,  for their use case.  But, if you want to run this  on your own organization,  it probably would not work out.  So the general question is,  how to calculate when is it worth it,  and when is it not worth it.  So, earlier on,  I was mentioning the example of  responding to issues as an incident,  as an emergency.  One way that seems to work,  or at least historically has worked for us,  is to look at the costs that  responding to an incident costs.  So, for example,  lost sales or partnerships,  or features that you couldn't launch.  And then you just ask,  or you let the people ask themselves,  how would you spend to respond to an incident?  Like, you look at actual data,  like what actually happened in the past.  And you will have three,  generally speaking, levels.  Like, how much is the hypothetical maximum  you would want to spend,  the amount that you actually spent,  and if you had implemented  certain security obligations,  how much you would have spent otherwise.  And let's say with these three categories,  you have over time,  you brought it over time,  how you expect them to change over time.  So, you can expect that  you're going to have,  at the rate of introduction of bugs,  you're going to have more bugs like this.  Or you can expect that,  on the other hand,  that if you implement certain mitigations,  maybe things will get better over time.  And you can use that  as a mechanism to approximate.  Here's an example of the Chrome security team,  or the V8 security team,  how they are looking at  comparing two types of mitigations.  One is disabling the JIT,  the just-in-time compilation sample,  just-in-time compilation of JavaScript code in V8,  versus the V8 samples,  which, if I have time,  I will go into later.  Generally speaking,  the two examples here  are looking at previous data.  They are looking at  inter-world exploits,  V8 CDN,  which is a setup  in which we pay people to write exploits  for vulnerabilities in V8.  You actually can participate.  You don't have to find the bugs.  You can just pick a bug from the bug tracker,  and then implement an exploit for it,  and you get paid some money for it.  The other one is Pwn2Own,  which is a bit more hard to participate,  but you can participate as well.  There is one part of it  that is interesting,  which is the in-the-wild one.  As you can see now,  thanks to V8 CDN and Pwn2Own,  there is actually more data  than there would be  if we only relied on in-the-wild exploits.  If we only relied on in-the-wild exploits,  then we will have way less information,  and we might reach the wrong conclusions.  As you can see,  there is some sort of a trend  on JIT compilation being actually  very, very effective  in general,  but not so effective  compared to in-the-wild exploits,  versus sandbox bypass,  which is generally speaking,  effective as compared to the other.  The more data that the V8 security team has,  the easier it is for them to defend  or invest in different areas,  especially in mitigations  that have some amount of development costs  or maintenance costs,  like the V8 sandbox test.  I want to shift a little bit  into the in-the-wild area  that I'm talking about here.  The one way to think about it  is that because of  the way that technology  is affecting our lives right now,  it is very common for everyone to use it,  including good guys, bad guys,  all types of guys and girls,  and the law enforcement  and intelligence community  are struggling to keep up with this.  The way that they are...  10 years ago,  James Comey, the FBI director,  was calling for backdoors for encryption,  and at the end of the day,  he made a very compelling argument  about why law enforcement  needs this information.  However, one of the things  that happened at the end  is that the...  One of the things that happened  after all is that we have the...  government, intelligence community,  become another participant in the market  for CODX.  However, it's not just law enforcement,  it's also military use.  And it's not just the military use  of one country,  it's the military use of multiple countries.  This is just like continents,  but you can imagine.  And it's not just law enforcement  from one country,  it's law enforcement from every country.  And it's not just law enforcement,  it's also investigation.  It's also not just that,  it's also bad guys,  folks like cybercriminals and so on.  Generally speaking,  you can think of every time  that someone is trying to reach,  let's say,  the treasure,  every time someone is trying  to split the pot or whatever,  they will try to take multiple paths.  And the more paths that are taken,  the less likely it is  that they will take the same amount of area.  Simply because from a perspective of  you have to select to go right or left,  you have to select,  you might assume that something is hard,  but actually it's not hard.  And depending on your previous experiences  and your skills in certain areas,  you might choose different areas.  Leading to multiple types of  reaching the same end result.  Which, at the end,  looks something like this.  As you can see here,  the top two folks took the same path  at the beginning.  But then almost everyone else  went the different way.  And then when a defender is trying to  figure this out,  they have a better knowledge of the system,  so they will take the most straight path.  And the most straight path  might not always be the most effective.  One will probably be the most effective one  to stop all the other actual adversaries.  For this case,  let's say that defense sets up  a couple areas,  a couple mitigations on the way,  but they actually are not that effective  against real adversaries.  Generally speaking,  you can think about this as a form of cost.  So, in the same scenario that  offense needs a certain amount of information,  defense needs to know as much information  as offense to be able to have a fair fight.  However, it becomes more expensive  because there is more demand  and there is more competition.  And it also becomes harder and more expensive  for defense to afford this information  simply because they have less supply.  And the reason they have less supply  is because it's harder.  So, it just becomes more expensive.  So, the general concept is  how to increase offense  without increasing defense costs.  And we can also use artificial intelligence,  but in this case,  we can say it's transparency.  And generally speaking,  the idea is that  by publicly talking about ways  to solve these very difficult technical problems,  we can attract offense to use those problems,  those mechanisms.  And they don't have to,  but they can.  Because from their perspective,  they can use what is going to work.  But then, this will also make it easier  for us to make it worth it  to invest in certain mitigations.  However, as soon as you start investing  in these mitigations,  then people will start choosing  the other path.  They will be like,  okay, I'm not going to use the path  that you know is going to be detected.  Then you use the other one.  But then it just becomes a money problem.  Then you have to decide  do you use the very cheap path  that comes for free,  or do you choose the more expensive path  that you have to figure out yourself.  And this also becomes a problem  in the case that  if there is a lot more information public,  then the vulnerability clusters,  the exploitation techniques and primitives  are also likely to be...  They are not infinite.  They are limited.  So the more people  that are publicly talking about this,  the more likely it is  that you're going to have duplicates  and you're going to have collisions,  and it's more likely that  things are going to eventually get shut down.  So then from the perspective  of talking to research,  you have going from one defender,  if you are talking with the public,  then you have many defenders  that can balance the situation.  As a form of comparison,  you can see what we did for the current CTF.  This is the number of public kernel exploits  according to multiple sources.  I duplicated them and added them together.  And you can see in 2016  when DirtyCow came out,  there was a big interest spike.  And then since then,  it kind of went down.  Then we started launching KernelCTF,  and then that started going nowhere.  So now we have more kernel exploits public  than we ever had before.  Another thing that we're trying to do with this area  is to go from the current status,  where we have quite a lot of very expensive,  it is very expensive to get this information  because it has much competition,  to a situation like hopefully in the future,  let's say 10 years from now,  there is a lot more researchers,  a lot more people that are able to do this  just because it is a lot easier.  The barrier to entry is not so high.  And also, hopefully,  we are doing a good job with mitigation.  It's also a lot harder to do reliable exploits.  And it's also harder for offense  to profit from this scenario.  So in the ideal long-term scenario,  the ideal scenario would be  that it goes from the current scenario  where it's very profitable to do offense  to something where it's not so profitable to do offense.  Of course, it's never going to cross like this.  This is an ideal scenario.  So the idea of all of this presentation  was to try to break down  why keeping information secret  made offense cheaper,  different from offensive and reversing the trend.  And the premise is that  with transparency,  buyers can differentiate more secret products,  which means that they can demand better  from their offenders.  It also allows defenders to quantify their mitigations,  and by the way,  increasing the cost of offense  and lowering the cost for defense.  And hopefully this whole mess  that I presented at the beginning  kind of makes a little bit more sense.  And I don't know how much time I actually have.  So...  I...  I don't know how much time I have.  LAUGHTER  APPLAUSE  Thank you, Eduardo.  I think we have some time left for Q&A.  Anyone have any questions?  Hey, so speaking about  making it more expensive for attackers  and cheaper for defenders,  when are you rolling out MiraclePointer in Chrome?  Where is MiraclePointer?  I didn't mention MiraclePointer.  That's so sad.  I thought I did.  I ended up choosing something else.  I think it's already there,  but I'm not sure if it's enabling  all platforms.  As far as I know,  it's enabled in some platforms,  but not in all of them.  Or maybe it's enabled in all of them.  I'm not sure.  Is it not enabled in all of them?  It's enabled in some of them, for sure.  But there's a chance that it is in  some obscure, architecturalized problem.  Hi, thank you for your talk.  The part about the market forces  disrupting the industry, more or less.  From your experience,  from Google's context,  does increasing bug bounty  actually help disrupt the market  for the offensive security side,  or is it counterproductive  because the prices are high?  From your experience,  should Google increase bug bounties  for our CEs, et cetera?  I think it depends on what you do  with the information.  Once you have a bug bounty program,  when you increase the prices,  you most likely are going to receive  more reports.  But then, if you don't do anything  with the reports that you receive,  then, well, then nothing happens.  There's going to be the same amount of bugs.  So, maybe this is a good way to think about it.  So, if you have renewable bugs,  so if the bugs that you fix  are slower than the rate  that you're introducing them,  then it's irrelevant how much you pay.  But if you are removing bugs  from the code base  faster than they are being introduced,  then it is more likely  that you're actually making an impact.  So, I would imagine that  if you are increasing prices  without doing anything with information,  then it is not going to disrupt anything.  But if you are,  even with a small amount of information,  if you're doing something about it,  then, yeah, you will eventually make an effect.  But you usually need a lot of information  to be able to make good investment decisions.  And the investment decisions  usually are very expensive.  So, that's why more information...  Usually, that's why...  So, for KernelCDF,  we increased the rewards 10 times  from 10,000 to 100,000.  And the reason was  because we needed a lot more information  than we wanted to have.  Hi, thanks for your sharing.  Yeah, I think I briefly saw your slides.  I think it wasn't covered in the main sharing part.  Like, in your appendix,  there was a slide on rewriting Binder  from C to Rust.  Actually, for my school project,  I had a project converting C code to Rust.  I think it was pretty difficult.  I just wanted to know your thoughts  about moving forward  with the conversion to Rust, right?  With everything supposedly memory-safe.  So, will you fully eliminate  this class of memory bugs?  What are your thoughts on it?  Yeah, so, one of the advantages of Rust  is that it definitely...  And not just Rust.  Rust, and Golang, and even Java.  Like, they remove certain type of problems.  And there is...  One of the things that I wanted to mention  in this slide that I didn't...  It's a map of slides.  Is that you can eliminate problems.  And by eliminating problems,  or class of problems,  you are...  This is the most effective way  of improving security.  Just by eliminating the problem.  And yeah, Rust eliminates that problem.  And it is, however,  the most expensive thing to do.  So, re-implementing code  is highly difficult.  And even from the...  I don't think I have the code here, but...  Well, I have a part of the code.  But if you find the patch series  that Alice posted on the Linux kernel  maybe this...  She actually goes into talking about  the trade-offs of rewriting code.  And actually, the first sentence  of the commits of the patch series,  of the intro letter,  was that this is usually the last thing  that you want to do.  But it is worth it for certain scenarios.  Especially in something as critical as Binder,  that is accessible even from sandbox processes.  That was very important for them to tackle.  Also, it also had other reasons.  The code was very hard to maintain  because it was very complex.  And there was a lot of technical...  So, yeah.  It makes sense for them to do it.  Okay.  Hi.  So, I tell you, I ask...  So, I really like the fact that you presented  the trade-off analysis  on how increasing certain variables  would decrease the cost of defaults.  But is there any practical studies  or statistical studies done in Google  that really shows that there's  a statistical significance in these areas  that you guys have explored?  Or is it just based on observations?  Yeah.  I mean, I think it's interesting.  To be able to find enough data,  we would probably need  a couple hundred years worth of information.  So, maybe in a couple hundred years.  But the number of vulnerabilities  and exploits that we have  and information that we have  is coming at such a slow rate  that it's mostly something  based on principles.  But we will only be able to  essentially measure it with error bars  and degrees of freedom and so on.  Once we have enough data  that the amount of error rate  is easy to measure.  But that would take a really long time.  I think it's probably possible  for certain types of bugs  that were super common  were well studied.  But whether to say...  There's also problems about  what you can measure  and what you cannot measure.  It would be hard right now  to be able to make that.  But it's something we looked into.  Or at least I looked into.  But it's not immediately obvious.  At least for certain parts of it  you can actually do.  So, for this part,  the rate of introduction of bugs  versus...  At least for the bugs  that people know how to find,  you can see how that goes to zero  if you make certain changes.  I mean, it becomes obvious.  If you're protecting the use of the free bugs  and then you're switching to binder,  you will have no more use of the free bugs.  And then that, of course,  it goes from something to zero.  It's very different.  But, yeah.  To be able to do a general analysis  that says that in general,  for every possible case,  this will work.  They will have statistical significance.  That would take a very long time  to be able to have that.  The market is good though.  Alright.  Great talk. Thank you.  I was curious if you had any thoughts  on increasing the access  or the transparency  in having all these end days  effectively visible.  It's obviously a trade-off  because there's going to be  cyber criminals and other people  who now have access to these  and then time to patch.  Very apparently with Android  and also many other platforms,  it's pretty long.  Under a number of months,  if not years,  for weird edge network devices  and stuff like that.  I was curious if you had any thoughts  on what the trade-off of  this increasing transparency  versus people having a lot more access  to easily applicable end-day chains.  I think the problem here is twofold.  One of them is increasing the time to deploy.  Sorry, increasing the time to deploy.  The time to deploy would be  once the bug is public,  how much does it take  for it to reach the user?  That increases or decreases  the shelf life of the vulnerability.  You are correct that  if an exploit is public...  By the way,  all of the kernel CTF submissions...  There is a Linux kernel exploit  for pretty much any kernel  always public in our world.  Yeah, there is definitely a...  Hello.  Oops.  Hello.  Can you hear me?  I can't hear you.  I suspect this is the next talk.  Anyway, I'll try to quickly finish this.  The time to deploy is...  Okay.  Can you hear me?  Yes, I can hear you.  Okay.  All right.  Time to deploy.  Yes.  Generally speaking,  if the number of bugs was...  If the supply of bugs was very limited,  then I would agree that  disclosing the few bugs that exist  and an exploit for them  would have a very equal effect  on the market.  However,  with the assumption that  the bugs are easy to find  and are very diverse,  then the trade-off would be  whether to have more information  to be able to accelerate progress  or to restrict it to be afraid  of it being used very well.  This is different for cases  when the software is open source,  like the Linux kernel,  where actually the community  needs to know this information  to be able to make timely changes  versus when the code is not open source,  and then the only people  that can equally make a difference  with technical details are SQL.  In those cases,  it's still useful to have transparency  from the perspective of metrics  and exploitation paths  that might allow the user  to understand how to protect themselves  despite whether there's action or inaction.  But yeah,  that is the trade-off.  The trade-off is mostly about  what information do we make public  and when,  and what considers unreasonable risk.  Unreasonable risk being risk  that is unexpected  versus risk that users are  understood and expect  that will come from them.  Generally speaking,  the only way that we can reduce  the patch cap,  the patch cap being the time to deploy,  is if we put pressure  as consumers on the manufacturers  that give us this code  to create the mechanisms to update  and to do it quickly.  They will never do that  if information is skipped away from the users.  I guess the aspect is  the more information we can provide  to provide pressure to the market  to deliver these results is better.  And this, of course,  creates short-term versus long-term.  Okay.  I think I need to stop  because someone is talking.  All right.  Thank you very much.  Thank you.