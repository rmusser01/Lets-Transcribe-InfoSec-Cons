{
  "webpage_url": "https://www.youtube.com/watch?v=2reOuSSDavM",
  "title": "Off-By-One 2024 Day 2 - The Forgotten Treasure In Classic Targets : Hua Hangyu & Wu JunTao",
  "description": "Abstract\n\nAs is well known, there are numerous mature fuzz testing tools on the market, including iconic ones like AFLplusplus for general objects, syzkaller for Linux kernel.\n\nHowever, despite the continuous and round-the-clock efforts of these powerful fuzzers to test these targets, several longstanding vulnerabilities have exposed in recent years, posing significant risks for privilege escalation.\n\nThis suggests that many secrets still lie deep within the code, beyond the reach of fuzzing techniques.\n\nTherefore, by investigating the pain points of fuzzers and conducting manual code audits targeted at high-value objectives such as the Linux kernel and mobile RCE-involved decoders, we have successfully uncovered dozens of high-value vulnerabilities.\n\nThese vulnerabilities, which are not easily detected by existing fuzzers, may enable privilege escalation to obtain root access. Through analysis of these vulnerabilities, we have devised methods to enhance fuzzers and discovered multiple new 0days.\n\nSpeakers\nHangyu Hua (@HBh25Y) is a security researcher at numen cyber and the co-founder of Shuffle Team, mainly focusing on low-level security such as boot chains, kernels, etc.\n\nJuntao Wu (@Dawuge3) is a security researcher and the founder of Shuffle Team, mainly focusing on mobile/web3 vulnerability discovery and exploitation. He was inducted into the Samsung Mobile Security Hall of Fame 2021/2022/2023.",
  "channel_url": "https://www.youtube.com/channel/UCmrsIbKdxEMBEefD8v8RigQ",
  "duration": 2293,
  "channel": "Off-By-One Comms Team",
  "uploader": "Off-By-One Comms Team",
  "upload_date": "20240901"
}

This text was transcribed using whisper model: large-v2

 So, good afternoon everyone, welcome to our talk.  Thank you everyone for sticking to the last, second topic.  And this is about some benefit research in classical techniques.  So, a little about us.  My name is Junhao Wu.  You can just call me Dawu He.  I'm a security researcher at CERTIC.  And I have founded the CERTICO team, which likes to explore new things.  I have been selected into sensing and hardware owner of mobile security hardware green screen clients.  At the same time, I have submitted some benefits to Google and other beneficiaries.  Now I'm mainly focused on mobile and web 3.  And this is my partner, Tang Yihua.  And he is a security researcher at Yunnan Cyber.  And he is also the founder of the Shuffle Team.  He mainly focuses on the low-level security research.  So here is the agenda.  The content mainly revolves around the forgotten trash,  focusing on the vulnerabilities that are often overlooked by security agents  and can not easily be detected by tools.  And I have divided the content into three parts.  So let's take a first look at why forgotten trashes appear.  Here is a little background.  So we once had a doubt.  Is this one of the reasons why the exploit chain continues to appear?  If you are familiar with the news about web 3 security manipulatives,  you should know that if there are problems in the project,  it is very easy to cause direct economic losses.  Therefore, the audit habit of web 3 project  is to find all the vulnerabilities as much as possible.  However, the differences between the web 2 project and the web 3 project  is that its size is very large.  And it is almost impossible to guarantee that all vulnerabilities are discovered.  Therefore, some have been tested by tools, of course,  that have been manually audited many times are often ignored.  This is the first vulnerability since a while that I want to talk about.  It is very special because many security researchers have noticed and retweeted it.  The reason for this appears in 3 types that have been tested and audited many times.  And because there are many projects using 3 types,  so it has a very wild impact.  The vulnerability exists in the lowest-biased PNG,  and which could process the PNG images embedded in fonts.  This function first will obtain the image width and height from the head  as 32-byte integrals,  and then it truncates the opt-in values to 16 bytes  and stores them in the T-squared measure structure.  Then it will use the truncated values to calculate the bit-byte size.  Then it will block the back-end stores of that size.  You can see it finally passed the PNG structure and the back-end store to the PNG function.  So normally this isn't a vulnerability that is difficult to find,  but why didn't the father find it at that time?  From the fuzzing test results,  you can see that the black box fuzz tools  and the fuzzers corresponding to the OASIS fuzz platform  were used to test this code representation.  But the bugged file isn't being fuzzed at all.  So it is a very clear gap.  And the next, let's turn our attention back to the recent vulnerability.  This vulnerability appears in the relevant parsing code of WebP.  It occurred in September last year.  It is also the vulnerability in the wild,  affecting the security of Android, Chrome, macOS, and other platforms.  The cause of this vulnerability is the problem of the auto-bounce writing  in the invention of the Kafka encoding algorithm.  In the processing flow,  the decoder first reads the prefixed encoded data from the image data stream  and builds a completed Huffman encoding table based on this data.  Then the decoder decodes the compressed data in the data stream  according to the table and restores the original image.  According to the standard Huffman coding algorithm,  when constructing the Huffman coding table,  the first level table will be used.  This table is used to carry Huffman code with a length less than 10 bytes.  The default byte is 8 bytes.  If the length exceeds 10 bytes in coding,  the decoder will allocate a secondary table to carry this part of the long encoding.  The problem is that the Huffman coding table saved in the decoder's default picture is reasonable.  So it calculates the maximum memory length in advance based on this assumption.  The core premise here is that the encoded data must confer  the specific of parametric Huffman coding.  That is, the Huffman tree should be a complete binary tree  with each left node corresponding to a prefixed code.  There are no unused left nodes.  However, since the Huffman coding table data comes from the on-chess itself  and may be irreparably congested by attack,  the decoder passes a completed binary tree  without checking the valid date of this data  and may allocate too much of the secondary table  and will cause the total memory usage to exceed the pre-allocated size  and a deep backflow will occur.  Regarding the key point of this availability,  we can actually focus on two main lines.  The first is the counter area and the second is the root table.  Here is the counter area which stores the numbers of the users of each line of the code.  Specifically, the code line is allocated space in the top-level function with Huffman codes  and obtains the corresponding specific data in Huffman code lines.  This root table is actually a Huffman table  and its boundaries are determined at the beginning.  The most important thing here is the table size  can have user, control, color, cache size variables  to affect the selection of the fixed values.  The Huffman tables are logged in using the number of each tree groups  times the table size times the size of Huffman tables as the final size,  most of which cannot be changed by the adapter.  The patch here is actually the in-app version.  In fact, the repair method adopted by Google  is to add some new structures for preparation and verification.  But the repair on iOS is closer to the original version.  It can be seen that the most direct fix here is to check  whether the access element has crossed the boundary.  If so, it will fail on the original panel.  However, this fix may not be particularly perfect  so they later adopted a more complex fix patch.  And since WebP is also a project in Google OSS fast platform,  let's take a look at why this variability was discovered by researchers.  We can see that the function file where the problem occurs  has actually been relatively fully tested by photo coverage.  Each function has been 100% covered.  So where does the problem lie?  Here we need to reflect why hadn't this bug been found earlier?  Why had the library not been found enough?  Had it not been found right?  In fact, the deep-seated reasons why this variability  hadn't been discovered are as follows.  First, its travel conditions are very complex.  It needs to overflow data of sufficient length to track a crash.  But in the actual environment,  the space allotted by the malloc is usually larger  than the actual parameter size passed in.  Secondly, the tracking of this variability  also requires the cooperation of the first-level table  and the specificity of the encoding data.  That's caused a lot of bugs during the blockchain version.  And this is the last variability.  This variability is located in the TCP-ULP model.  There are some protocols in the Linux kernel that support this model.  And these protocols that support this model need to  implant TCP-ULP-AUX structure,  which contains a clone function point.  When the TCP three-way handshake protocol is complete,  that is, when the TCP listener states  change to TCP established,  SK clone lock will be called to generate a new SK.  When the protocol is used, such as subflow,  and implements the clone function,  the clone function will be called to process the context state.  If there is no original SK,  it will copy its own context to the new SK.  For example, the TLS protocol doesn't implant this clone.  This will result in two different SKs  pointing to the same context,  which is a typical double-free problem.  The interesting point of this vulnerability  is that the TLS protocol only allows  the TCP established step to be set  after the three-way handshake is successful.  So we need to use the same software to repeatedly  perform the three-way handshake to check this vulnerability,  which obviously violates the general password coverage standard.  Next, let's turn our attention to the second part,  review the targets.  We can see that since Google offered a bounty for Linux,  a large number of exploitable vulnerabilities in Linux core has appeared.  And Hongjiong also has Ubuntu LTE project every year.  So Linux seems to be a great target for BAU.  As we all know, the most common solution for bug hunting is farming.  So we want to fast Linux.  First of all, the tech service.  A year or two ago, EPF was very famous  and had many vulnerabilities.  But due to Ubuntu's setting,  the user couldn't access this module.  You can see the screenshot on my computer here.  Next is IOUMI.  Last year's EPF had,  it clearly showed that IOUMI had many vulnerabilities  and most of them are exploitable.  These vulnerabilities could even be used in Android critical before.  But recently, Android disabled unregulated users for accessing IOUMI.  However, many distributions like Ubuntu know its use.  Therefore, it's a good tech service  with repeat code updates.  However, the disadvantages are also perverse.  Too many people have deleted,  which makes the vulnerabilities easy to repeat.  Then there is our superstar, Netfilter.  We can see that a large number of vulnerabilities in Google KCTF come from this module.  We can even see that most of them.  Looking back at the vulnerabilities in recent years,  this module has always been a hotspot for bug hunting.  This fully provides that this module is complete enough  and may contain a lot of treasures.  Moreover, a large number of well-known security researchers  have published their own treasure-hunting articles for this module,  which makes it very easy for us to get stuck.  But the price is that this makes the vulnerabilities you find in Netfilter  will be easy to repeat even higher than the IOURI.  By the way, as a large number of Linux kernel vulnerabilities are discovered,  the price of Ubuntu on Tobio is decreasing year by year.  So, what is the target of funding?  After some research, after some searching,  I finally decided to fund in the network package scheduler.  Its advantages are that it is complete and has no vulnerabilities requirement,  and most importantly, it has less attention.  Its disadvantages are also perverse.  It has poor generalizability and requires a new network space.  So, how to use NetScheduler?  First, we need to create an SD, then call sendMessage with ASConnect link.  There are also a lot of RTMs for us to choose.  You can choose to create a new QDisk, a new class, a new chain, a new filter, and a new action.  RTM New QDisk has many options,  like Schedule FIFO, which means first-in-first-out,  and Schedule HDB, Schedule Multicube.  RTM NetFilters also has many options.  But first, we need to select which CLS you want,  like CLS.UserDistribute, CLS.Follow, CLS.Flowers, and CLS.EM,  like EM.Text, EM.IDT, EM.UserDistribute.  RTM New Action is the same.  You can choose the actions you want.  So, after kind of finding these structures,  it's easy for us to start planning,  like you can use this code.  Unfortunately, after a while, we didn't have any good results.  So, I decided to do a code review directly.  Then I found something interesting.  Let's look at the CLS.Flowers.  We can see that the FLC,  we can see that the FLC,  this general OPT has an LOS infrastructure  that directly uses the LOS parameter,  which looks like a potential variable for the ability.  The LOS of this LOS is 255.  But when we continue to look at it,  our subsequent code actually checks the length of length.  So, this isn't a bug.  My spider science told me  I should go back and take another look.  Let's go back and take a look.  We can see that after getting OPT from the LOS,  OPT is assigned a value immediately.  And below the arrow is the length.  This length will be 0 when we first call this function.  And R1, R2, R3 are also 0.  So, it looks like 0 is 1.  The arrow contacts the OPT struct and the data.  Length points to the end of the data filled in the arrow.  It's not difficult to concentrate a suitable length.  And then OPT overrides length to 0.  Now we can bypass the check for length  and fill the subsequent data into the structure  to complete the autofound right in the structure.  The limitation is that we can only overwrite over 128 bytes.  If we look for this structure in the source code,  we can see that the structure where our arrow is located  is actually imbibed in two different structures.  One of which happens to have a chain that can be overwritten later.  Which means that we can modify a head point  which can easily convert this vulnerability.  The other contains a function point,  which means we can modify and delete a function point.  Since the CLS flower provides us with the function of passing the data,  we pass the bank to the user space.  It may depend on the OPT of each handler  and the total length.  If we continue to fill data in this arrow after checking the structure overflow,  since our length has been modified,  we will start writing the content of the data again.  After layout, we can modify the length of part of the original OPT.  At the same time, build a fake OPT struct at the end of the arrow.  Since the kernel will be mistaken,  since there will be more data passed in later.  Realizing the autofound right in the structure  we can link the function point mentioned earlier.  That's the exploded Ubuntu.  More details can be found in the link below.  Now let's turn our attention to mobile RCE  and the parsing logic of HEAP.  HEAP files are special cases of ISO-based media file formats.  Latest conditions and timed media and metadata streams forms a check,  whereas static media and metadata is stored as items.  Consequently, HEAP has the following basic designers.  First is, still images are stored as items.  Second, image sequences are stored as checks.  So a file may contain both image items and image sequences,  checks along with other media.  The picture on the right illustrates how a file player processes a coded image  and the derived image includes a file.  The file player decodes a coded image into a reconstructed image.  Similarly, the file player applies the operation of the derived image  to the indicated one or more important image  to obtain the respective code image.  The flowchart here simply shows the parsing process of simple on-sensing.  Since the HEAP I mentioned before  can actually be regarded as a combined type of file.  This is mainly divided into two parts.  The first part is about the HEAP file HEAP parsing,  and the other part is about the parsing of the file content inside HEAP.  Since the file content is highly inclusive,  its content will lead to parsing of some other file formats.  At that time, tools like AFL++ hadn't yet introduced filing tests  that could be directly run on mobile phones.  So I initially wondered why I can't test these function libraries through the QMU.  But this idea was quickly rejected because  the gen-line interface elements of these function libraries  are relatively complex Java objects,  which may not be well suitable for the simulation testing QMU.  So I considered whether I could use Vreda to write a fuzzer  for completely code coverage calculation.  So here it is, a not-too-bad fuzzer.  Its main improvement is that it doesn't both mutiline functions through JS,  but directly use the function interface brought by the Vreda core.  In fact, the Vreda core itself is the underlying hooking framework  directed through the QMU object,  so we can directly use its underlying interface.  The function shown in the pictures here,  the onMessage() will transmit the code block information  checked by the underlying stacker back to our Honey's program,  and thereby realizing the collection of block coverage  and some other functions will support some crash or excess signups.  So the final effect is that I can run my fuzzer directly on my phone.  The result of fuzzing test is a total of 16 manipulations were discovered within mouse,  including 5 critical and 1 high.  The CVE-2021-39804 is actually a bit interesting.  This manipulator itself is not very dangerous,  but it does bring me some inspiration and encouragement,  which proves platforms like OASIS Fuzz cannot achieve 100% success,  even if faced with some fuzzing modules that have been tested many times.  Different fuzzing methods will still be able to discover different security manipulations.  So let's compare some existing fuzzing test flow methods,  which are basically connected from the last year, Zerocon 2023.  So MJ, he wrote a great series of blogs,  he had a research about closed-source and binary fuzzing,  and I used to write the 3D fuzzer.  The folks have a topic about Android J9 fuzzing,  and the guys from Zhejiang University also test on something called closed-source libraries.  So fuzzing testing can be a general approach to find bugs in closed-source native libraries.  For mobile testing targets, code coverage should be the most effective fuzzing testing method.  From an object point of view, currently in Android native library,  basically all the function libraries' objects can be fuzzed tested.  For shared libraries, if the parameters are memory, address, or object,  string, file, or descript, we can use QMU to collect the code coverage.  If the parameters are objects of Java library,  we can use JVM or QMU or Frida to collect the code coverage,  such as a survice process.  And there is already a corresponding tool, like Chatsify.  And its implementation is also to collect the code coverage through Frida.  And for native network process and session,  Netella from Google Project Zero did a corresponding buzzing test  on some mainstream information-transmitting software,  including WhatsApp and other applications.  So the method used was also similar to the Frida passive fuzzing test.  So in theory, it can also use Frida to collect the code coverage.  So is there anything else we should pay attention to?  As we mentioned before, the CVE-2020-15999 is missed by fuzzing test operations.  And the CVE-2020-4863, they are missed because of complex conditionals  and deep travel paths.  Another point is that the seed source of fuzzing test always affects some aspects.  Sometimes it is difficult for us to find a seed file that can trigger deep-level code very acutely.  So in addition, fuzzing testing tools themselves usually have various problems.  For example, Frida is very unstable and prone to crash in the actual user's theory times.  And its testing speed is very slow.  And due to the particular of mobile Frida code coverage crashing,  the multi-line process parallel fuzzing test method is also very difficult to implement.  So it's time to review.  And here is the CVE-2020-30699.  In fact, this isn't a very complex type of limiting.  It still happens in the header fuzzing module that appears in the GIF file.  So why hasn't this bug been found by Frida?  So here is the details about CVE-2020-30699.  And the V38 variable will store the file data read from the GIF.  And it will use the size in the subsequent read function.  And V21 points to a memory variable.  And its header stores the value of 1.  And it will pass 1 from the V21 as the memory read area.  So since we can control this total length,  it seems an out-of-bounds override.  But there is a limitation.  The sum of the lengths is actually the same as the elements  accumulated in the MyLogElement object before.  So this will have to be no problem.  Actually, the real problem is here.  There is an integral truncation vulnerability  which can cause MyLogElement to allocate less space than expected.  You can see that in the example,  this vulnerability uses the AND instruction instead of the MOV instruction.  So if our logic is as shown in here,  it will traverse the collection area in the HVCC function.  And then it will traverse the NeoUnit inside each area  and calculate the final space size that needs to be collocated  based on the size of each NeoUnit plus the length of the prefix.  Therefore, due to the previous integral truncation vulnerability,  an out-of-bounds right problem will occur here.  So why is this vulnerability not defined by father?  It is the vulnerability that also requires mutiline connection to match.  This parsing part is a sub-model in the head.  And it requires that the key file has mutiline NEOs.  But in fact, the value of each NEO is limited in the small size.  And finally, the sum of these values must exist  the maximum value of AND 32.  Next, let's turn our attention to the content of this report  and enhance our fathers.  So, why father fail?  The first kind is that the more use of this vulnerability is difficult to construct.  For example, some memory destructive vulnerabilities  may not have induced effects such as crashes  so when they write out-of-bounds,  they may perform it at the internal of some memory unit  and the modified value is reasonable under certain probability.  The second situation is usually more common  in some process files parsing.  Because in some cases,  the character parts of the code are many and complex,  it is difficult to determine all the key process modules in a short time.  Therefore, in most cases,  more attention may be paid to the earlier processing modules.  There is also the third type of vulnerability  which is often the hardest one to find.  It needs to be checked when the mutiline conditions are met at the same time.  In many cases, we may not actually understand the key points  involved in parsing a function.  At the same time it is closed,  so in closed parsing tests,  we usually focus more on the adaption of earlier interface elements  with the improvement of the code coverage.  In attention, there are still some types of vulnerabilities  that are difficult to find with parsing tests.  So we improve our parsing in the following ways.  First, for mutiline layer nest structures,  we pay more attention to the efficiency of the parsing testing  to reach specific blocks or instructions  rather than the code coverage.  This means we may pay more attention to the deeps of the code testing  rather just than the breadth.  Because many existing parsing test tools focus on the breadth,  but some test results or vulnerabilities  only occur in the shadow code.  Frequently crashes in the shadow logic  are not very competitive to test the deep code.  Secondly, we pay more attention to the path testing  of some key interesting locations.  For example, we use mutiline test on the same path,  but we use the different or unique value space.  In general, this is the guided path testing structure  that focuses more on some forgotten vulnerabilities.  And here are some new vulnerabilities tests we discovered.  We can still find some new vulnerabilities on these targets.  However, since some vulnerabilities are still being fixed,  we will mainly talk about the latest canal here.  Hi.  So the bug is in the schedule module.  In the module, we need to delete the code  and make a new variable with a new Qtist.  The length of the tree is determined by the number of CX queens.  We can see that at position 1, QOPT bands are assigned.  At position 2, the node is allocated by Qmax binds minus Qbinds.  At position 3, Qbinds is assigned by Qoptbinds.  At position 4, we put some extra Qtist objects into the removed analog.  And then freeze them at position 5.  Why is the old Qbinds used to allocate the removed size at position 2,  but the new Qbinds is used in the for loop later?  This seems strange.  If maxbinds minus newbinds is greater than twice maxbinds minus oldbinds,  the for loop may write Qoptbinds to a tap point.  That will be freezed soon.  But first, we need to figure out what number CX queens and real number CX queens are.  After I checked the source code of AI and tried it in my old network card,  I found that these two require hardware support.  So this is a useless part.  My spine size tells me these are not that simple.  What if we create a virtual network card?  Boom!  We can now control the real number CX queens and default number CX queens to 128.  Here is an example.  First, we create a network card.  Then we set real number CX queens to 124,  and call maxmultiple twins again,  which will create a 124-valued object.  Then modify real number CX queens to 190,  and call maxloop twins again.  We can write a tap point or loop that will be released immediately.  Vulnerability was fixed by us a few weeks ago.  Here is a demo.  Thank you.  Thank you.  Thank you.