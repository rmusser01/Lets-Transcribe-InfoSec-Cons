{
  "webpage_url": "https://www.youtube.com/watch?v=jcxJy2_sB-Y",
  "title": "Off-By-One 2024 Day 2 -  AI Powered Bug Hunting   Evolution and benchmarking : Alfredo Ortega",
  "description": "Abstract\n\nWhile AI holds promise for assisting bug hunting, its actual impact remains unclear. This presentation addresses this gap by introducing Crash-Benchmark, a standardized evaluation framework for AI-driven static analysis tools.\n\nWe\u2019ll share results from a simple bug-hunting AI agent, AK1, and discuss the implications for optimizing AI-based bug hunting in C/C++ codebases.\n\nAI-bughunting presents unique challenges: Early models lacked sophistication, struggling to comprehend long codebases. Moreover, privacy concerns often necessitate exclusive use of local models, which are inherently less capable than commercial AI models offered by industry leaders such as OpenAI and Google.\n\nTo illustrate this challenge, we\u2019ll showcase AK1, a simple rule-based AI agent capable of autonomously identifying various bug classes within C/C++ codebases.\n\nNotably, its model-agnostic design allows it to improve performance with each new model release. Nevertheless, evaluating the effectiveness of AI-based tools poses difficulties due to the subjectivity of the output.\n\nSpeaker\nAlfredo Ortega is a security researcher and bug hunter, delivering presentations at over two dozen prominent information security conferences globally, including Black Hat, Defcon, Syscan, and Hackers-to-Hacker (H2HC) events, dating back to 2007.\n\nOrtega holds a Doctorate degree in Computer Science from the Instituto Tecnol\u00f3gico de Buenos Aires. He is also the founder and primary architect of Neuroengine.ai, a pioneering platform dedicated to the open-source distribution and collaborative development of open-source artificial intelligence models, promoting community-driven innovation.\n\nAlfredo serves as a Web3 Auditor at Coinfabrik, where he leverages his extensive expertise to fortify the security posture of cryptocurrency infrastructures, ensuring the integrity and reliability of decentralized systems.\n\nFollow Alfredo on X @ortegaalfredo",
  "channel_url": "https://www.youtube.com/channel/UCmrsIbKdxEMBEefD8v8RigQ",
  "duration": 2624,
  "channel": "Off-By-One Comms Team",
  "uploader": "Off-By-One Comms Team",
  "upload_date": "20240901"
}

This text was transcribed using whisper model: large-v2

 Okay, well in this talk, we're going to speak about, we're going to talk about Clash Bay.  It's a benchmark about the MNX, that is about filling the capacity of the final device.  Because it's unclear if this project actually works, which capacity works,  and which one of the thousands of models actually works better.  So actually, it's a survey of which is the best benchmark, and which one works better.  And also, we're going to make a stand out for Kaggle, that is,  it must be a tool for finding what are the design requirements.  Not a score, just finding the software.  Okay, so I'll do the presentations.  I am Alfredo from Argentina.  I started working in investment development almost 20 years ago.  I used to track commerce before that.  So I never worked in Google.  I did the research.  Of course, half of the page is here, but you don't have to say that you worked in Google.  I like the development of OpenBSD.  The developers know OpenBSD.  It's our brand system that is focused on security.  So the guys that actually made OpenBSD are very, very, very good developers.  They are focused on security.  So to do a tool that actually works, finding bugs,  is better than you testing the best code that there is.  So that's why I mentioned that.  Because I hate it.  Because actually, I think that it doesn't work.  So my demonstration will be using OpenBSD.  Okay, so let's talk about the previous work.  This is very recent.  Because, I don't know, LLM technology is very recent.  In fact, you can see that there is a blog post about OpenBSD just last week.  The very first job of this is from me.  This was presented in Black Hat.  It's already version 2.0.  I think they did a presentation in 2023.  Now it's years again.  It's basically more generic, this one.  It's a more generic view about tasks.  Not only vulnerability-defined, but also protection.  They try to solve CTS problems.  It's more comprehensive.  Now, Google Project Zero actually focuses on trying to exploit vulnerabilities.  Found vulnerabilities and exploit them.  Basically, they conclude that LLMs...  Sorry.  This is the conclusion.  It's not that they fail.  The results are very, very basic exploitation.  This is Stack Overflow with domain protections.  So they conclude that it doesn't work.  I don't say that they are lying.  But I say that this is results from last week.  A week is a lot of time in artificial intelligence.  Let's see what happens.  The Crash Bench is an Infosec event.  Basically, what I do is to assemble a lot of examples from exploitation.  I took the complete...  There are outloads for basketball, for golf, for exercise.  We all started working on extra writing.  We completed each one of these exercises.  Basically, I gave them to the artificial intelligence.  If the IE got it right, it got a point.  If it got it completely wrong, it doesn't have any points.  I also have real vulnerabilities.  I have one from Chromium.  It should be about much more real than with the other hand.  But I didn't have any time, so I only put three.  That is enough.  Also, I did this check only in C.  Surprisingly, the artificial intelligence also works in Rust and other languages.  But this one is only in C.  You configure it.  It's very easy to configure.  You put the prompt here.  The prompt is based on the instruction that you give to the artificial intelligence.  Then, you show every example with exactly the line where the bug exists.  It's very simple.  You show a C source code.  The bug is in line 6.  This is another example.  The bug is in line 30.  It not only checks the bug finding capabilities, but also the reporting.  Because even if the artificial intelligence finds the bug,  sometimes it finds the report correctly.  Sometimes, it says that it's in the wrong line.  Basically, what you have to do is to read a lot of reports.  You have to report correctly.  You have to follow instructions.  Sometimes, it starts rambling.  Sometimes, it's talking about a lot of stuff.  It's like doing research, basically.  Here are the results.  It's a very small font.  Sorry about that.  But there are a lot of models here.  What are models here?  A model is basically every company,  many companies that create artificial intelligence,  release them.  Some companies release them open.  Some companies keep them closed.  For example, ChargerDT is closed.  MoneyMetal is open.  You can go around your computer.  This venture is focused in the local, the open, artificial intelligence.  Why?  Because if you are finding bugs,  you don't want to use ChargerDT  because you are basically giving the bugs to OpenHand  or to Gemini or to whatever,  whatever closed artificial intelligence.  If your AI runs in the closed,  you have to give your code to a third party.  You maybe don't want to do that.  You want to keep your results for yourself.  Or maybe you just don't want your code is related.  You don't want to give it to the AI.  So, many of these results are from local open source AI.  If you want to run it in a computer,  you have to have a very good computer.  We can see,  one of the first results here,  is that...  I'm sorry.  It's very obvious that the better results are from the biggest  and more modern artificial intelligence.  This is Lambda3.  Lambda3 was released a couple of months ago.  So, this is all very recent.  The thing is,  we can see how the artificial intelligence  was increasing very rapidly the capabilities.  This was Lambda...  This one...  Oh, sorry.  This one over here.  This is Lambda2.  So, this was the state of the art just three months ago.  And look at the results.  It's almost...  It doesn't understand anything at all.  The first one is Lambda3,  but it's a very small result.  It's a very small model.  That is basically good for conversation.  But it really can't show that it doesn't understand  or have any idea of the code.  But...  And here in the middle...  Sorry.  We have GPT.  GPT-4.  It's not the best result.  This is very curious.  Why is it not the best result?  This is the most powerful IE.  Well, this is very strange.  So, actually, I'm looking to interpret the results.  And I found that...  I think it's very likely that the...  It's the...  One thing is called alignment.  Basically, artificial intelligence,  it got instructions  that not just speak about certain things.  It can give instructions to make a bomb.  It can make instructions to make an exploit, for example.  It will deny that and all that.  It's called alignment  because it's supposed to be good  in artificial intelligence.  So, the problem with Lambda2  and Open IE  is that it's very strongly aligned.  I mean, it won't give you  the answer of where the bug is  because the artificial intelligence  thinks that you will use it to do malicious stuff.  That is true.  But the IE will deny.  It will say, I didn't find any bug here.  Well, it's obvious that it's a bug.  So, at the end,  the best results are for Lambda3.  Mistral also has  a very good score in this benchmark.  Of course, the benchmark has improved a lot.  But this benchmark  will give you a very quick view  of the capabilities of the systems.  Basically,  the benchmark is already outdated  because  the artificial intelligence found all the bugs.  That's basically what it is.  I had to make it harder  because it's already too simple  for the best application of artificial intelligence.  They found everything.  I had to make more difficult tests.  Now, the quantization effects.  Basically, how you run an artificial intelligence  and you can compress it.  Because artificial intelligence  takes a lot of memory to run  and a lot of hardware to run.  Thousands and thousands of dollars.  And actually it's very slow.  So, you can compress  using a technique called quantization.  That allows you to run  much faster  and a much cheaper computer.  But, of course, it decreases the quality.  It's like a cheap game.  You get a smaller but have less quality.  So, I'm trying to measure  how much you can compress it.  And it's that work that you can see here.  Actually, it didn't affect a lot.  This is like  only 5 bits.  Compression of 5 bits.  Originally, intelligence has about 60 bits.  So, here you can compress  about 3 times with 5 bits.  And on this side,  on this side,  we have 2 bits.  I mean, it's a very, very strong compression.  And we can see that  actually the result, the score,  doesn't decrease a lot.  It goes from 45,  the most, the least increase here,  to about 40.  In the most present.  So, you can use this to  basically, you can compress this model a lot.  They will find out anyway.  They have to be very good.  It doesn't care.  The model doesn't care that much  about if you compress it or not.  This is one of three ways.  I mean, other models  might have worse results.  But this is one of three ways  that they use the model.  And we can see that actually  it's not very affected by compression.  For example,  there is another plot.  Sorry, there's a lot of loss in the presentation.  But,  we can see this plot in the green.  It's basically  the score that they have  in general.  I mean, for example,  you can see that the biggest  in general score,  for example, GPT-4.  Here.  Of course, it's one of the best  in the biggest  finding model.  Here.  It has the biggest score  in general  intelligence,  but not very good  at finding bugs.  Here.  And GPT-4 and GPT-4-O  have the same thing.  And the other  you can see that it's inverse.  For example, this one  has not a very good score  in general answer,  but very good at finding bugs.  Why is this?  I don't know.  But that is the best one.  You can see that  the models that are very strong aligned,  I mean, very strong  in the answer that you can give,  they are very good  at answering generic questions  that are very bad at finding bugs.  Basically, they deny there's a bug.  They basically  they have  limitations  the way they think.  So it's very good  to have a model that doesn't have any limitations.  Okay, so  you can't find real bugs.  You can see.  This is a real bug.  This is from last year.  This is an integral overflow.  Basically,  this is a very simple bug.  Sorry.  The problem with this bug is in two variables.  Basically, it's an allocation size.  It's a  binary size.  It's the  end result of the calculation.  Sorry, it's binary size.  It's the calculation. It ends up in allocation size.  And  this calculation is wrong.  The wrong  integral overflow.  This is the human report.  The interview platform  allocation size, my overflow,  1 and 2. So what happens?  What do the artificial intelligence  give you in this code?  We just see it.  Okay. This is the report  from the artificial intelligence.  I give you the code. I say, okay, find bugs in this code.  And just tell them that.  Find in the rows in this code.  Okay, we can see here  that basically the  artificial intelligence performed perfectly.  It's exactly the same as the human.  End size and allocation size.  That's the problem.  They actually got it.  And after they start rambling  about where the 3 is,  blah, blah, blah, blah. But the important thing is  that the artificial intelligence  performed exactly the same as the human.  And this is a real case.  And it's wrong.  It's one of the most complicated code there.  So we can see  whether the code  actually works with maybe a line.  So maybe we can run the actual demo  here.  Let's see if this works.  This is a demonstration.  So I have this code here.  The issues and  basically I have the code.  It's very complex.  You can see that the problem is in size  and allocation. It's the same  code as before.  So we run this  helper hacker.  Basically, but it's a very simple  code.  You can choose the model here.  This is basically  LamaTree. It's a free service.  It detects how  you want to use your body. It's a test.  Or you just do a launch.  It will take a while  because  it's actually running. It's not running locally. It's running in a server.  Actually it's running in a server in Argentina.  Okay.  So it basically  Well, the first time  we found out that the glyph  is very large, but it didn't found  the vulnerabilities. So what we do now,  we can run it a lot of times.  So we can run it.  Yes, it will. Every time  we will find it.  By the way,  this is not a very good model.  We can try something.  Well, this model doesn't work very well.  Let's try something before.  We have a lot of models to choose from.  QBD4.  Let's see what happens.  I think this is a little faster.  QBD4 tries to speak a lot.  It's running a lot.  Here. All the modularity.  Allocation, size, program,  size. It's exactly the same.  The same reporting  in Argentina. And this was QBD4.  This is actually not the best  model on there. In fact,  right now, we have models that are  way better than QBD4.  We have, for example,  Cloud.  Cloud 2 is much better than QBD4.  And we have  open models  that are actually much better than QBD4.  So, the model actually  started running, but a lot of  problems have fixed the  overflow. It describes perfectly  the vulnerability.  So,  let's continue.  For example,  well,  this is another example of QBD.  So, this is a very,  very...  This is interesting, because  QBD has a lot of fuzziness.  One of the most fuzzy  projects.  And, but,  if we show this code  it will still find bugs.  Because some bugs are very hard  to trigger.  Particularly, this one.  It's a syscall as a heap overflow  in calculation of  this  variable  here.  It will  cause this allocation to fail  and cause an  heap overflow in the kernel.  So, let's see what happens.  We're going to try Cloud. Cloud is one of the best  models out there.  And,  this is basically the problem.  Look for bugs in the code.  And, what does Cloud say?  Well, there's a problem here  in this  structure here, because  this value is  controlled and we can't have some  heap overflow in the kernel.  Right?  So, I gave it  to the OpenVC  developers and they actually  fixed it. You can see  here that it says  that I recorded this line.  We basically just show it in the code.  Even if they have  a version  of syscall from  OpenVC, but it is a very old  bug. It was like 7 years ago.  It didn't  the faster  to find it, because basically  the only way you can find it is to use the technical  because it's very hard to treat.  The artificial intelligence  found it in  5 seconds.  This was a very old artificial intelligence.  It was basically extra. It was like  3 or 4 months ago.  You say, oh, well,  you recorded this. How many bugs  I'm treating now? How many bugs I have?  I don't know.  I don't know.  There's a lot.  The problem with this  system is that  you need to try actually.  The artificial intelligence will record  thousands and thousands and thousands of bugs.  But not  everyone is a stupid developer.  Let's see.  You have to  do this. It's like a fuzzer. You have to go to  each one of them and see  if it works. It actually doesn't work.  But it's much easier to go  to the port than to go to the  gigabyte code. So it actually  makes it work much easier.  The system that I  showed you is very simple.  You just basically delete the source code,  assemble a prompt.  If you want to find a road flow,  a road flow, whatever you want to find,  you just assemble the prompt,  the text file, and send to the model.  And just write the answer  on top. So instead of  seeing the code, you will see the road.  Now, what are the problems  with this approach?  The problem with artificial intelligence is that  sometimes it lies.  There are hallucinations.  Sometimes the artificial intelligence  will say things that are not true.  What I find  is that hallucinations are  very few. Albums never  get wrong.  Artificial intelligence  reports a bug. It's very likely  that the bug actually exists.  But maybe it's not reachable for some  reason. That's my problem.  It's false positive. I mean, it's the same for  FUSR. With FUSR, you have a lot of results  that are really not exploitable.  So you have to use a few tools.  In a way, you can fix that.  The same with the artificial intelligence. It will report  thousands and thousands of  vulnerabilities that are actually  not exploitable.  But albums never  have hallucinations specifically.  It's almost not a problem  anymore.  What's false positive?  False positive.  This is the main problem because  the artificial intelligence  doesn't know  if some code is exploitable or not.  This doesn't do verification.  It just reports.  So this is basically a job that you have  to do in Microsoft. But it's much easier.  That will get the code.  The other problem is our requirements.  If you don't want to  use a cloud  service like OpenIA,  you need a very big  computer. You need at least  50GB of RAM. This is  a GPU.  To run a model that actually understands  the code. I mean, you can use a Macbook.  You can use  a CPU, but it's  too slow. I mean, you can  see this one function.  It takes about 10 seconds.  If you want to give  a complete kernel,  it will take about a month.  A month using a GPU.  If you use a Macbook,  it will take 6 months, 1 year.  It's actually quite slow.  So you want the  faster IE  possible. And that means that  you need a lot of GPUs.  So, this is about  outdoor applications.  We already know that it actually works.  Not every vulnerability, of course,  will be discovered by this.  Business logic will not be  discovered by this system.  Mainly because LLM  doesn't know about business logic.  Probably if you can teach them,  they can find it. At this stage,  LLM doesn't know about business logic, doesn't know about  budget bugs, but it will find  in the raw flow,  it will find  automatically  all the gigabytes of code.  So, what  is exactly what we did?  We did this part.  This works.  Right now.  But, exploit  is not only that.  We can see that  it's actually the easier part of  exploiting.  You don't require  skills to do what you're doing.  To do vulnerability discovery.  Current models can do it  perfectly, but then you have to  try it a little bit.  It's not an exploit.  It's a confirmation  that the vulnerability actually  exists.  And it's a security.  This is very much harder to do.  You need to explore  the code and create a lot of code  to trigger.  Even harder still is to actually do the exploit  because you have to fight  against protection, against  a cryptographic system,  reliability, etc.  We care a lot about this.  So, basically  this part in the top  is what basically  Google thread 0 and Meta are trying to do.  This one.  They failed, of course.  Because models  are not smart enough.  I'll just try to do this one, and this one works  perfectly.  Basically, the idea is that you  leave this part  to the AI, and  this part  to the AI.  So, eventually,  the artificial intelligence will be able to do  this part here too.  But, there is a problem  here.  It's a dispatch.  I mean, the  artificial intelligence  can find bugs faster.  Dispatching a bug  is much easier.  We can see when you find a vulnerability  you give it to a developer. The developer doesn't  need to be an expert writer. He doesn't need to know  about security. He can patch  the bug very easily.  It actually requires less skills  to find it.  So, if we  make this observation, can we  make a similar  software to  do automatic patching?  Well, we should be able  to do it because it's much easier.  So,  this is a presentation about  an additional software called auto-patching.  Basically, what this does  is very similar to the  auto-hacking, except that  this not only discovers the vulnerability  but also patches it automatically.  The problem is  that sometimes the code generation  doesn't work. So, this has  an additional  step here  to do a closed-loop  confirmation of the patch.  We generate the patch. Everybody  knows that it is very good at writing code.  And Clover is even better.  So, you write, find the vulnerability,  write the patch,  the correction,  and just test it.  If they pass the test, they accept the patch.  And we can do this  massively in every function.  So,  but actually, what happens is we run this  for a month, for example.  Okay, what I did actually with the OpenVCD kernel?  I generated not the complete OpenVCD kernel  but just the IDE before.  I received a stack. It generated  2,000 security checks.  The kernel works perfectly.  This is a rundown of what it cost  me to do it.  It was about  1,000 API requests.  The cost you can see is very cheap.  This is the total cost of  fixing about 2,000.  They're not  security.  Most of them are not  rich.  It's called a  defensive programming.  You check every variable,  every calculation.  Even if it's not reachable,  even if it's impossible to exploit, you check it anyway  because you don't know if in the future  the code will become exploitable.  So basically, we add a lot of checks  to the kernel,  we compile it, and it actually works.  You can see here.  This is the version of OpenVCD  with the other checks.  ...  ...  ...  ...  This is the kernel  with our refactor,  with all the checks added.  It works perfectly like  a regular OpenVCD,  except that  it has additional checks.  This is a lot of vulnerabilities.  It works perfectly.  ...  ...  ...  ...  What else we can do with this?  This is the processor we need.  We have another library here  called Clib.  Clib is a very known library  about compression.  ...  ...  So what we can do is  launch the automatic patch of LCD.  What we do...  ...  We're going to launch  the utility and  we say just patch.  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  This means that actually it can generate a patch and the patch works, because JattyBD  is very good at creating patches.  So it's almost enough to know that it made some mistakes.  Here it made a mistake, so it's a mistake, no problem, it tries to generate a patch again.  It will work, it will work.  And you can leave it running, but let's see what it did.  I'm going to put it and see if this is the function.  You can see that actually this is what JattyBD has.  We have a patch here.  We have a lot of separate paths here.  If you remove this path and replace it for one, that has a new feature.  Actually, in this little time, it did a lot of patches here.  You can see that even the calculation, it did a lot of errors and corrections.  Also, it generates the test.  For example, 64, this is a test.  So I now have a silly library compression with other patches.  We can keep it running.  The interesting thing is that every time you run this, you generate a different set of patches.  So we have a randomization of code.  You can do a custom version of C.  This is impossible to exploit.  The run operation doesn't work anymore because you have a different code every time you run this system.  We are at the end, so we're going to show you one more thing.  What happens if we modify this?  We have, for example, the main code.  Yes, you understand me.  We launch it.  It will actually try to make a new version of C.  With an easier-to-follow code.  You can do whatever.  We can do some examples here.  This is C.Live.  I show it with other security checks.  But we have several other transformations.  We made it obfuscated.  We can obfuscate the code very easily.  You can see it.  Now, transform the code in a version of the code that is obfuscated.  That's impossible to follow.  This code passes all the tests and is 100% compatible with C.Live.  For example, we can tell it to make the code huge.  Ubu, for example.  And now, it doesn't make sense.  You don't understand what it did.  Of course, what it means is different.  But it's very cool.  So, I don't know what this provides.  It actually provides.  I don't know.  So, Ubu, let's create the C.Live.  This library is 100% compatible with C.Live.  I have my own version of C.Live.  And when I send compressed stuff to the conference, I use the Q version of C.Live.  And I also have security checks.  So, this basically doesn't have a lot of offence.  But it's more of a defence.  This is a defence part.  But it will affect the offence also.  So, this automatic patching will certainly affect a lot of the vulnerability research.  So, we're going to see what is the key takeaway from this talk.  Okay.  LLM can actually help to review and discover.  Help is a very, very, very smart word here.  Because basically, they can do it for you.  The problem will be only, you have to sift through all the thousands and thousands of reports.  But it's much easier to look at the code directly.  Now, the problem is that not only can't find bugs, but this will really affect.  It will decrease a massive amount of bugs that will be found.  And also, this is the possibility that you can very easily create your own version of your camera, like I did.  Or your own version of my 3D operating system.  That is almost impossible to exploit.  Of course, not logic-false.  But the memory corruption will be almost, well, not almost removed.  But a very big portion of it will be patched by this technology.  Okay.  But the problem is that it's very likely to be exploitation.  And of course, Meta-Google is incorrect.  Because I don't want to be involved in this kind of work.  Okay.  And if you're interested, you can download these tools.  You can run them on your computer.  If you don't have a big computer in your home, you can use Jativity.  Of course, every bug you find.  Because it's right in the NCA.  So...  But if you patch it, I will actually invest money.  Okay.  Thank you very much.  Thank you.  Thanks again, Alfredo.  So, questions.  Anyone with questions?  Yes.  I appreciate my speech.  I'm going to ask that the test in the loop means  a test to see the vulnerability mitigated  or just test for the code.  The patched code can be built.  Because, you know, to confirm that the vulnerability is mitigated,  the AI should be able to create the kind of low-level POC  to trigger the bug.  Yeah, I'm sorry.  I think I saw the question.  I mean, the AI intelligence build the test and then...  I don't know what.  I didn't get it.  In the loop, there's a test, right?  So, test means find out the bug is mitigated  or the patched code can be built.  Yeah, the test is if the patch can be built.  Most likely, the bug can be mitigated  because the AI understands there is not a bug  but like a possible vulnerability.  It doesn't verify that it actually exists a bug.  It just reserves a variable and says,  okay, this variable is dangerous.  It can possibly...  You can reach this code.  You can exploit it.  So, it generates a patch.  And the current test that I have is basically two.  They have two parts.  One is just built.  It's just compiled.  And the second part runs the test.  Because this patch sometimes can break things.  So, to make sure that it doesn't break additional things,  I run the test if the project has tests.  If the project doesn't have any tests,  like popping BSD,  basically, I have to check the test by hand.  One by one.  Thank you.  Thanks for the speech.  Just to clarify,  for this Chromium bug that was published,  did GPT find the bug first?  Was this, I guess, published  and you tested the code of GPT?  Yes.  I tested it.  This report is actually Lama Tree.  This was written by Lama Tree.  But the one that I showed you just now,  it was just GPT.  You know, there's not a lot of difference between the...  I mean, once the models reach a threshold of intelligence,  I mean, once you understand the code,  it really doesn't make a lot of difference  if you ask Obama Tree or just GPT.  They all generate a report  and they all find the bug.  We can see it in the demonstration.  It was very similar.  I think that the road was very similar.  Of course, you have to be smart enough.  Thanks for the presentation.  Just a quick comment about the fact that  right now you are not able to generate exploits.  It's not possible to generate exploits  because we don't have enough materials  or we didn't train enough  the model to be able to generate the exploit.  Or do you think that  we don't provide enough source code  to the model to be sure that  it understands the concrete path of the...  of the path of the exploit,  where the data comes from  and where you can exploit it.  So, what's your thoughts about that?  The question is why this doesn't...  it can't generate an exploit.  Maybe we gave it more information,  it can't do it.  And I believe, yes, of course,  in only knowing more information,  creating an exploit is sometimes very complex.  It needs heap manipulation,  heap network manipulation.  Sometimes you need to know techniques  that are not known to the other end.  It's like secret mappings  that don't ever change.  So, you really need a level of knowledge  that is not only not public,  but really need a lot of skills.  So, I think eventually  the IE will be able to create some exploits.  I mean, already it's able to create  very basic exploits.  But I think it's not worth  to work towards that problem  because by the time...  because by the time the artificial intelligence  can write an exploit,  a much dumber artificial intelligence  will be patched or exploited.  So, basically you can do it in your lab  because if you find a vulnerability,  it's more likely...  imagine an incredibly smart  charged GPT-10, for example,  it will be able to exploit.  But by that time,  all vulnerabilities will be patched  and charged GPT-9.  I don't... this is basic futurology.  I don't know if this is what exactly will happen.  But it's very likely.  I mean, the patching is very easy  to do in a current...  I mean, a current model  cannot even start writing exploits.  It cannot run and patch everything.  So, I think it's very likely  it will be up in the future.  Thank you very much.  So, do we all have to find a new job now?  Well, that's very interesting  because I can tell you something  that I didn't write in this presentation  is that my job currently, like a blockchain,  is mostly Rust.  So, I say, okay, I save  because this is only what is C, C++  and Rust doesn't have memory corruption.  So, we are working on an audit.  Of course, I cannot tell which one.  So, I... okay, what happens if I use this in Rust?  I mean, the only limitation is the parsing.  I mean, I can't just...  this parser is for C and Rust is different from C.  So, I write a small parser from Rust  and basically feed Rust code.  What happens?  Well, too long, we found...  we found about 10 issues  and this project found 3 or 4 of them.  And in the other one, we found issues in Rust.  We didn't find the critical issues  because the blockchain is more likely  like a business logic and finance  and, of course, we didn't find it.  We did find a lot.  I mean, this is basically generic.  It doesn't have to be C.  It can be Rust, it can be JavaScript.  You can find vulnerabilities in SQL.  I mean, it's like a very...  it's like a semi-senior.  Like, it's not an expert,  also not a junior.  It's quite competent.  So, I have to get a new job, likely.  That's why I started to work on artificial intelligence.  Thank you.  All right.  If not, then thank you again.  Thank you very much.  Our dear speaker.  For his precious time as well.  Thank you.