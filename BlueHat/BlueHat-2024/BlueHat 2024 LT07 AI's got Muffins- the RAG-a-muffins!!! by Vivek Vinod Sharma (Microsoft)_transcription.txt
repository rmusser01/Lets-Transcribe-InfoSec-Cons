{
  "webpage_url": "https://www.youtube.com/watch?v=UWPlw7VvSA4",
  "title": "BlueHat 2024: LT07: AI's got Muffins- the RAG-a-muffins!!! by Vivek Vinod Sharma (Microsoft)",
  "description": "BlueHat 2024: LT07: AI's got Muffins- the RAG-a-muffins!!! by Vivek Vinod Sharma (Microsoft)\n\nAbstract: Retrieval Augmented Generation [RAG] is heavily used for building GenAI apps. But what happens when RAG gets poisoned! We uncover the importance of RAG and how best to secure it from misuse and abuse in GenAI-LLM application development.",
  "channel_url": "https://www.youtube.com/channel/UCKmzq2lAhDxLy36KtvVWpaQ",
  "duration": 799,
  "channel": "Microsoft Security Response Center (MSRC)",
  "uploader": "Microsoft Security Response Center (MSRC)",
  "upload_date": "20241112"
}

0.00s - 2.16s | This text was transcribed using whisper model: large-v2

 Hey, folks.
2.16s - 5.72s |  I'm Vivek, and I'm here to talk about muffins.
5.72s - 7.60s |  How many of you all are attending just
7.60s - 9.92s |  because of the word muffins in the talk?
9.92s - 11.88s |  Quick show of hands.
11.88s - 12.96s |  OK, all right.
12.96s - 16.52s |  I promise I'll tell you where to find them.
16.52s - 21.36s |  So the talk is going to be focused on rag poisoning.
21.36s - 24.00s |  Folks familiar with rag poisoning or what it is,
24.00s - 25.32s |  we'll set the stage for it.
25.32s - 27.56s |  OK, I see some in the back.
32.14s - 36.58s |  So this is going to be the agenda for our lightning talk.
36.58s - 40.58s |  We'll cover what is rag or what it stands for
40.58s - 44.18s |  and how it can be poison, and why should anyone
44.18s - 45.98s |  care when, especially when they're
45.98s - 48.42s |  doing gen-AI development.
48.42s - 50.90s |  We'll have a few suggestions based
50.90s - 53.98s |  on the lessons learned on what we are seeing within the MSIT
53.98s - 56.62s |  deployment around detections and mitigations,
56.62s - 59.18s |  and in the industry and where the research is headed,
59.18s - 61.50s |  and then we'll conclude.
61.50s - 66.78s |  So for folks not familiar, the acronym RAG
66.78s - 69.90s |  stands for Retrieval Augmented Generation.
69.90s - 73.82s |  So basically what it is, it's using an LLM application
73.82s - 77.70s |  or helping an LLM application get its responses grounded
77.70s - 83.42s |  in more accurate and up-to-date content from external
83.42s - 87.42s |  or from knowledge data sources that developers can
87.42s - 90.02s |  contextualize the LLM application with.
90.02s - 96.90s |  So a good way to think of it is, let's say I take M365 copilot
96.90s - 99.82s |  or I take a regular copilot agent,
99.82s - 101.46s |  and I want it to be contextualized
101.46s - 106.82s |  to the SharePoint site that has a bunch of documents
106.82s - 110.02s |  that my organization or my division has worked on.
110.02s - 113.50s |  And for users who are outside or even within my division,
113.50s - 115.90s |  they want to interact and find out what my division does,
115.90s - 118.18s |  who is working on what work stream,
118.18s - 124.06s |  that's the way I would summarize it at a high level.
124.06s - 126.46s |  There's a strong synergy between LLMs
126.46s - 130.86s |  and how they are supposed to handle the external knowledge
130.86s - 134.14s |  data sources, and that's the key that we are going to focus on.
137.46s - 141.34s |  The next question is, how can we poison RAG or Retrieval
141.34s - 143.74s |  Augmented Generation data sets?
143.74s - 146.90s |  So it involves, in the early example
146.90s - 150.06s |  that I gave you where I have a SharePoint site
150.06s - 152.94s |  and the documents that are part of the SharePoint site,
152.94s - 157.50s |  consider scenarios where insider risk or somebody not
157.50s - 162.74s |  intentionally or unintentionally having inaccurate data
162.74s - 165.62s |  that they want others to trip on.
165.62s - 168.46s |  What I mean by that is, let's say
168.46s - 171.42s |  it's a website about numbers and trying
171.42s - 173.34s |  to report accurate numbers.
173.34s - 175.10s |  It's very easy for someone to make
175.10s - 177.82s |  intentional or unintentional mistakes
177.82s - 182.74s |  and fudge the numbers and have the LLM application that's
182.74s - 186.26s |  using that whole knowledge data source report
186.26s - 188.30s |  inaccurate numbers for people who are interacting
188.30s - 190.02s |  with the LLM application.
190.02s - 194.74s |  So that's what is, in a nutshell, what RAG poisoning
194.74s - 197.02s |  is.
197.02s - 199.86s |  Why should we care about it?
199.86s - 203.82s |  Folks familiar with the OWASP Top 10 for LLM?
203.82s - 205.14s |  Yep.
205.14s - 207.18s |  We talk about training data set poisoning.
210.18s - 213.58s |  I would say that's a click deeper than RAG poisoning
213.58s - 216.74s |  because there you need access to the underlying models.
216.74s - 220.82s |  In the case of generic gen-AI applications,
220.82s - 223.34s |  the model is a black box where you're
223.34s - 227.46s |  interacting with a chatbot as an end user.
227.46s - 230.94s |  And in very few instances of the time
230.94s - 235.18s |  where you're creating this LLM application on top of the,
235.18s - 238.34s |  or like a custom copilot on these data sources,
238.34s - 239.94s |  will you have access to the data sets
239.94s - 241.46s |  that you're going to feed it to?
241.46s - 248.30s |  So that's where, if I have not good data or contentious data,
248.30s - 250.42s |  that can result in an LLM application
250.42s - 255.06s |  generate incorrect or biased or even harmful outputs.
255.06s - 257.66s |  And we spoke about the tight linkage
257.66s - 261.14s |  or the synergy between LLMs and the knowledge data sources
261.14s - 262.42s |  that they're going to refer.
262.42s - 265.74s |  And that's what will be used for manipulating the model
265.74s - 269.06s |  responses.
269.06s - 271.14s |  There have been research already done.
271.14s - 273.26s |  These are the two prominent ones that you
273.26s - 275.74s |  can go look up in archive.
275.74s - 277.30s |  A ton of research is also happening
277.30s - 281.02s |  in this space around how easy it is
281.02s - 284.10s |  and what should be done to tackle this problem.
286.82s - 289.02s |  The next, building on why should anyone
289.02s - 294.34s |  care, for folks who had been to Hacker Summer Camp this year,
294.34s - 296.90s |  this was from a security research
296.90s - 298.78s |  who was there at Black Hat, who we worked very
298.78s - 301.34s |  closely with at Microsoft, around 15 ways
301.34s - 302.66s |  to break copilot.
302.66s - 304.90s |  Gained a lot of media attention as well,
304.90s - 307.02s |  and definitely made us rethink about how
307.02s - 310.26s |  we handle our data sources.
310.26s - 313.02s |  There was another talk at DEF CON around confused copilot.
313.02s - 316.18s |  So they were focusing, again, on data poisoning
316.18s - 319.90s |  and how an insider within a company that
319.90s - 323.14s |  has access to this enterprise data, enterprise context,
323.14s - 327.10s |  can easily go ahead and result in contentious results
327.10s - 331.18s |  and make an LLM generate outputs that
331.18s - 337.76s |  won't be in line with its safety alignment and safety training.
337.76s - 339.68s |  So it's not just specific to Microsoft.
339.68s - 343.80s |  Just wanted to show this, another piece of news
343.80s - 347.44s |  out there that we have other Gen-AI platforms
347.44s - 349.72s |  and other Gen-AI applications as well, which
349.72s - 352.04s |  are susceptible to this problem.
352.04s - 355.32s |  This is a popular one that came out a few weeks ago
355.32s - 359.28s |  around Gemini for work and how researchers
359.28s - 361.08s |  were able to manipulate the underlying data
361.08s - 367.52s |  source in the enterprise context and have Gemini result spit
367.52s - 371.51s |  out manipulated output.
371.51s - 375.07s |  So building on top of it, we saw,
375.07s - 379.51s |  OK, we know what rack poisoning is.
379.51s - 380.83s |  Why should we care?
380.83s - 385.55s |  The first and foremost reason is that system prompts,
385.55s - 389.27s |  the guardrails that define an LLM application,
389.27s - 391.23s |  you can assume that it is public information.
391.23s - 393.83s |  It's like a lot of articles out there
393.83s - 397.11s |  that, oh, we were able to trip the information but give out
397.11s - 399.07s |  an LLM system prompt.
399.07s - 401.91s |  But we should assume that it is already public information.
401.91s - 404.67s |  It's fairly trivial to get that information out as well.
404.67s - 407.51s |  The second one is people's email IDs.
407.51s - 411.15s |  There's automated tools to find email addresses,
411.15s - 413.07s |  the combination for users.
413.07s - 415.19s |  And then you have the system prompt.
415.19s - 416.31s |  You have the email ID.
416.31s - 421.99s |  So you can basically send an email, which we have seen
421.99s - 424.79s |  this in the wild as well, that people
424.79s - 427.63s |  will have white text on white.
427.63s - 429.91s |  And they'll have markdown instructions
429.91s - 434.23s |  as part of the email that you send to corporate users.
435.23s - 438.19s |  The third part is around the M365 context.
438.19s - 443.23s |  So when you consider M365, the graph for a particular user
443.23s - 446.55s |  or the interactions that they do within the enterprise context,
446.55s - 447.99s |  the documents that they work with,
447.99s - 450.43s |  the people they interact with, the blast radius
450.43s - 452.79s |  just increases manifold.
452.79s - 456.67s |  The problem amplifies at scale.
456.67s - 459.99s |  We do not have any good way.
459.99s - 461.91s |  Like, we have protections for checking
461.91s - 465.07s |  external, internal emails, team messages.
465.07s - 467.51s |  But there are too many avenues.
467.51s - 470.31s |  And we feel that there's not enough controls where
470.31s - 472.47s |  we can check any and all interactions that
472.47s - 476.15s |  are coming in from external, at least at the LLM interaction
476.15s - 479.35s |  layer, or do the handoff between all interactions that are
479.35s - 481.39s |  happening in the external context.
481.39s - 484.63s |  So that awareness within the graph needs to be,
484.63s - 490.03s |  I would say, has to be built in or has to be written.
490.35s - 492.59s |  And as you already may be familiar,
492.59s - 496.51s |  or we are getting into this agentic phase with Gen AI,
496.51s - 500.63s |  where it's very fairly trivial to build
500.63s - 504.79s |  an agent on top of well-known copilots
504.79s - 507.23s |  or with data sets that users are already
507.23s - 509.83s |  working off of as a copilot studio
509.83s - 512.47s |  or with agent force, with sales force.
512.47s - 514.87s |  It becomes very easy.
514.87s - 517.43s |  And the data sets are interconnected.
517.43s - 519.47s |  The dependencies are too many.
519.47s - 522.47s |  And just increases the possibility for entry points
522.47s - 527.75s |  and how you can infect the RAG knowledge sources.
527.75s - 531.39s |  So based on what we have seen, a couple
531.39s - 535.65s |  of things that we feel within Microsoft security
535.65s - 538.23s |  that we feel where the research is,
538.23s - 540.79s |  and we are seeing research happen in this area as well.
540.79s - 543.55s |  The detection and mitigation around these
543.55s - 546.75s |  would involve some sort of behavior-based anomaly
546.75s - 550.47s |  detection that we had with our traditional products
550.47s - 554.51s |  in identity, in endpoint device monitoring.
554.51s - 556.83s |  That needs to happen in the interaction
556.83s - 558.87s |  with these LLM applications as well,
558.87s - 562.67s |  because otherwise it's very hard to sort of scale and track
562.67s - 567.07s |  each and every signal that the user is interacting with.
567.07s - 570.63s |  The second one, which will involve more research
570.63s - 574.99s |  and more thinking about how to solve the problem,
574.99s - 576.39s |  would be around sandboxing.
576.55s - 581.55s |  So achieve this with attachment filtering in Office.
581.71s - 583.55s |  Something similar, we feel, should
583.55s - 587.91s |  be done in interaction with the Gen-AI co-pilot,
587.91s - 589.59s |  and especially when they're interacting
589.59s - 591.91s |  with knowledge sources as well.
591.91s - 594.11s |  Which knowledge source you're interacting with
594.11s - 596.37s |  and when to sandbox the interactions
596.37s - 599.47s |  with the knowledge sources would be critical decision points
599.47s - 602.43s |  in application performance and application design phases.
603.43s - 608.99s |  For using, when you're using custom-built Gen-AI
608.99s - 610.99s |  applications built off of platforms
610.99s - 614.71s |  like Azure, OpenAI, or Amazon Bedrock,
614.71s - 618.59s |  you would need to define strict onboarding processes
618.59s - 621.15s |  as to how you onboard these data sources
621.15s - 623.87s |  before you connect it to your LLM application.
623.87s - 627.23s |  And then for pre-built or embedded Gen-AI applications,
627.23s - 630.19s |  or think of things like co-pilots,
630.23s - 635.43s |  we need to have a staggered approach for rollout
635.43s - 638.39s |  where you can go ahead and have like a,
638.39s - 640.87s |  we feel that if we have a ring deployment strategy,
640.87s - 642.27s |  that will work best.
642.27s - 643.59s |  Obviously, that didn't happen.
643.59s - 645.83s |  I mean, enterprises are just in the rush
645.83s - 649.15s |  to get co-pilots out there, but we
649.15s - 653.23s |  feel that it's just diving head into the problem
653.23s - 657.75s |  if you don't take this staggered or ring-based approach.
657.83s - 663.23s |  And the underlying solution to the whole problem
663.23s - 666.83s |  is if you have robust data security and governance,
666.83s - 670.83s |  then there's no taking away from it.
670.83s - 674.43s |  The foundation for data security still apply.
674.43s - 680.21s |  It's just that Gen-AI is a layer on top of it.
680.21s - 684.33s |  So with that, I'll conclude by saying
684.33s - 687.29s |  that it is definitely a hard problem to solve.
687.29s - 689.21s |  The industry and the research, we
689.21s - 695.05s |  see people are investing time in trying
695.05s - 698.33s |  to figure out what would be the best way to solve this problem
698.33s - 702.17s |  so that we are not taking an end-user experience or a compute
702.17s - 704.17s |  handoff as well.
704.17s - 708.09s |  Because imagine you're already having a delayed response
708.09s - 709.55s |  and interacting with the co-pilot,
709.55s - 712.73s |  and then you build in things like sandboxing and real-time
712.73s - 716.25s |  detection that will just make the user experience a nightmare.
716.25s - 721.45s |  So work has to be done there.
721.45s - 725.37s |  We also saw that it's not a problem specific to Microsoft.
725.37s - 728.85s |  It's basically a Gen-AI platform problem.
728.85s - 732.89s |  We feel that behavior-based and sandboxing
732.89s - 737.69s |  would be where solutions may arise.
737.69s - 741.73s |  So think of it like either Microsoft product or a way
741.73s - 744.37s |  where our signals coming in from co-pilots
744.37s - 746.93s |  or from the product telemetry, end-user, device identity,
746.93s - 748.51s |  et cetera, have to be stitched together
748.51s - 750.93s |  and understand the behavior in which they are interacting
750.93s - 753.49s |  with the Gen-AI application.
753.49s - 756.01s |  And again, back from the last slide,
756.01s - 759.85s |  no taking away anything that the underlying data sources need
759.85s - 763.87s |  to be secure and hardened before we
763.87s - 766.77s |  decide that it is going to be used
766.77s - 772.97s |  as a knowledge source for Gen-AI applications as a database.
772.97s - 776.85s |  So with that, you see the AI robot handing out
776.85s - 779.29s |  muffins for people who are attending the talk.
779.29s - 781.85s |  And I promise there will be muffins at the end of the talk.
781.85s - 784.01s |  So you just need to wait for a couple of hours,
784.01s - 788.71s |  and the Blue Hat folks will put out muffins in the corridor.
788.71s - 790.17s |  So with that, thank you, everyone,
790.17s - 791.33s |  for listening to the talk.
791.33s - 794.61s |  And hope you have a great rest of the Blue Hat.