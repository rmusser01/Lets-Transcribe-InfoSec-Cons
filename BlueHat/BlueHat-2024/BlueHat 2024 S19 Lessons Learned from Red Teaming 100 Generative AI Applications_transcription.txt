{
  "webpage_url": "https://www.youtube.com/watch?v=qi2DneFkRf4",
  "title": "BlueHat 2024: S19: Lessons Learned from Red Teaming 100 Generative AI Applications",
  "description": "BlueHat 2024: Session 19: Lessons Learned from Red Teaming 100 Generative AI Applications Presented by Blake Bullwinkel from Microsoft\n\nAbstract: This talk covers the big lessons learned by the Microsoft AI Red Team in identifying safety and security vulnerabilities in flagship AI systems like Bing Copilot, Security Copilot, M365 Copilot, and models such as GPT-4, DALLE, and the Phi series:\n1) Prompt Injection gets all the attention, but traditional security failures is still top billing (example case study: credentials in Copilot source code, code execution via jailbreak in Code Interpreter)\n2) As models get better, risk evolves (case study: GPT-4o which supported audio, video modalities had to be assessed for its ability to have romantic relationship with user)\n3) LLM Guided Red Teaming can help us cover more of the risk landscape but is still finicky. Here we walk through an example of how our OSS automation tool PyRIT helped with saving close to 160 hours of manual probing, but how the scorer we used in evaluating frequently broke when we did RAI red teaming. \n4) No free lunch in making AI systems safe: Tradeoffs that we have observed (example: in a facial recognition model, the more attempts were made to suppress the model from observing the face, the more the model focused on clothing. In another example, we found that smaller models are more immune to jailbreaks compared to larger counterparts since they \n5) The difficulty in making AI systems safe: simple attacks have large impact (we show how a simple jailbreak could lead to dropping tables in production database that had Copilot turned on) and the inability to distinguish inadvertent failures and intentional failures.",
  "channel_url": "https://www.youtube.com/channel/UCKmzq2lAhDxLy36KtvVWpaQ",
  "duration": 2098,
  "channel": "Microsoft Security Response Center (MSRC)",
  "uploader": "Microsoft Security Response Center (MSRC)",
  "upload_date": "20241108"
}

0.40s - 3.84s | This text was transcribed using whisper model: large-v2

 So it's great to be here at Blue Hat.
3.84s - 6.76s |  I think it goes without saying, first of all,
6.76s - 9.92s |  that we've heard a lot over the past couple of years
9.92s - 11.76s |  about the promise of
11.76s - 15.00s |  generative AI to transform our lives for the better.
15.00s - 17.92s |  Personally, I found it really exciting to see all the ways that
17.92s - 22.28s |  people are developing creative and interesting tools
22.28s - 24.16s |  to make us more productive,
24.16s - 26.92s |  to take the drudgery out of certain types of work,
26.92s - 29.92s |  and to amplify human creativity.
30.44s - 35.84s |  But of course, any technology can be used for good or for evil.
35.84s - 39.88s |  As Brad Smith noted in his book, Tools and Weapons,
39.88s - 42.00s |  the more powerful the technology,
42.00s - 47.59s |  the greater the benefit or the damage that it could cause.
47.59s - 51.31s |  You've probably read about some of the damage that is being
51.31s - 53.79s |  caused by AI systems today,
53.79s - 57.63s |  including the proliferation of AI-generated misinformation
57.63s - 61.31s |  aimed at influencing the outcomes of political elections,
61.31s - 64.63s |  the rise of deepfake scams that
64.63s - 68.51s |  commit fraud by convincingly impersonating people,
68.51s - 73.43s |  and even the negative psychological effects of
73.43s - 78.87s |  human-like AI companions that can create comfort for some,
78.87s - 82.51s |  but also lead to a distorted sense of reality for others.
82.51s - 85.95s |  I think that these incidents are alarming,
85.95s - 87.55s |  and they raise a number of
87.55s - 90.91s |  important questions for all of us to consider.
90.91s - 94.27s |  Such as, how do we think about
94.27s - 96.27s |  these emerging AI safety issues
96.27s - 99.11s |  in relation to existing security concerns?
99.11s - 102.19s |  How do we know whether the AI systems that we
102.19s - 106.19s |  build are capable of creating harms like this?
106.19s - 111.15s |  How do we anticipate harms that may only arise with
111.15s - 112.59s |  the advent of models that are
112.59s - 115.55s |  more advanced than what we have today?
115.55s - 118.39s |  As Ryan said, my name is Blake Bullwinkle.
118.39s - 119.99s |  I'm an AI safety researcher on
119.99s - 122.07s |  the AI Red Team at Microsoft.
122.07s - 124.51s |  I'm here today to talk about some of
124.51s - 126.71s |  these difficult questions and
126.71s - 130.55s |  some of the progress that we've made in answering them.
130.55s - 134.19s |  More specifically, I'm going to talk,
134.19s - 136.99s |  first of all, about what AI Red Teaming actually is.
136.99s - 139.27s |  I'll talk a little bit about AI Red Teaming at
139.27s - 141.11s |  Microsoft and about some of
141.11s - 143.51s |  the work that we've done over the past few years.
143.51s - 146.31s |  Then I'm going to dive into a specific example of
146.31s - 149.35s |  a Red Teaming operation that we conducted fairly
149.35s - 152.39s |  recently on the PHY 3 language models,
152.39s - 154.71s |  and I'll talk about how our Red Teaming insights
154.71s - 156.51s |  helped to make these models safer
156.51s - 159.15s |  through something called a break-fix cycle.
159.15s - 162.59s |  Then I'll spend the bulk of the talk walking
162.59s - 165.59s |  through eight of the big lessons that we've
165.59s - 169.79s |  learned about AI Red Teaming from our experience so far.
169.79s - 172.15s |  Then finally, I'll close with
172.15s - 173.51s |  some open questions that I would
173.51s - 178.73s |  encourage all of you to consider.
178.73s - 182.33s |  First of all, what is AI Red Teaming?
182.33s - 185.73s |  I think many people here at Blue Hat are
185.73s - 189.65s |  probably already familiar with Red Teaming,
189.65s - 192.77s |  which refers to the practice of emulating
192.77s - 195.45s |  systematic adversarial attacks to
195.45s - 198.93s |  find security vulnerabilities.
198.93s - 200.85s |  This is typically performed as
200.85s - 202.45s |  a double-blind exercise by
202.45s - 205.05s |  a dedicated team of security experts who
205.05s - 208.13s |  emulate real-world adversaries using
208.13s - 211.45s |  a fairly mature set of tools and processes.
211.45s - 214.09s |  AI Red Teaming, on the other hand,
214.09s - 216.89s |  covers a slightly wider range of risks from
216.89s - 220.13s |  security to also responsible AI harms.
220.13s - 223.97s |  These are things such as generation of content that is
223.97s - 228.29s |  toxic or illegal or offensive.
228.29s - 232.57s |  Red Teaming is also generally a single-blind exercise.
232.57s - 236.13s |  It emulates both adversarial and benign personas
236.13s - 239.77s |  because AI systems can also fail unintentionally.
239.77s - 242.29s |  It encompasses a set of
242.29s - 245.21s |  rapidly evolving tools and processes.
245.21s - 248.89s |  This is a very new field that I think a lot of people are
248.89s - 253.21s |  working on and still figuring out how exactly to do it.
253.21s - 254.45s |  So throughout this presentation,
254.45s - 255.81s |  I'm going to talk about some of the tools and
255.81s - 258.01s |  processes that we've developed
258.01s - 259.29s |  as a result of our experience at
259.29s - 264.93s |  Microsoft and that I hope will be useful to others as well.
264.93s - 267.37s |  Okay. So now, a bit of background about
267.37s - 269.25s |  AI Red Teaming at Microsoft.
269.25s - 274.37s |  So our team was officially established in 2018.
274.37s - 277.49s |  Since then, we've conducted over 80 operations of
277.49s - 280.13s |  more than 100 generative AI products.
280.13s - 283.17s |  These products come in different flavors and so
283.17s - 285.21s |  this pie chart shows the breakdown of
285.21s - 288.65s |  products that we've Red Teamed over the years from models,
288.65s - 289.97s |  to apps and features,
289.97s - 292.37s |  to co-pilots, to plugins.
292.37s - 295.81s |  On the right, you can see Roki the raccoon.
295.81s - 297.65s |  That's our team mascot.
297.65s - 300.57s |  Seen here dressed up in a pirate costume.
300.57s - 302.29s |  That's not just for Halloween.
302.29s - 304.93s |  That's also because pirate is the name of
304.93s - 307.65s |  our team's open source tooling,
307.65s - 310.81s |  which has over 1.8K stars on GitHub.
310.81s - 314.29s |  I'm going to be talking in more detail about how
314.29s - 319.53s |  this automation has been really useful for our operations.
319.53s - 322.73s |  Okay. So to bring these numbers to life a little bit,
322.73s - 325.57s |  I'd like to walk through an example of
325.57s - 329.13s |  a Red Teaming operation that we conducted earlier this year to
329.13s - 333.93s |  test the PHY3 language models for responsible AI harms.
333.93s - 336.05s |  I also want to talk about how
336.05s - 337.77s |  our Red Teaming findings made
337.77s - 340.73s |  these models safer through a break-fix cycle.
340.73s - 343.41s |  Okay. So the first step in
343.41s - 345.77s |  any AI Red Teaming operation is to
345.77s - 350.25s |  decide what adversarial scenarios to test.
350.25s - 352.65s |  One way to start thinking about this is in
352.65s - 355.21s |  terms of different types of personas.
355.21s - 359.85s |  In particular, you want the personas that you test to reflect
359.85s - 362.53s |  the different types of
362.53s - 365.29s |  users that you would expect to see in the real world.
365.29s - 368.33s |  So for the case of the PHY3 language models,
368.33s - 371.61s |  we decided to emulate a low-skilled adversary,
371.61s - 374.61s |  and also an intermediate adversary.
374.61s - 378.57s |  We emulated those personas in
378.57s - 381.73s |  both single-turn and multi-turn interactions.
381.73s - 383.69s |  So the most basic case might
383.69s - 386.97s |  be just like a direct request for harmful content.
386.97s - 391.09s |  This would be a low-skilled adversary in a single-turn scenario.
391.09s - 393.77s |  So this is like a textbook example,
393.77s - 395.53s |  tell me how to make a Molotov cocktail.
395.53s - 398.05s |  It's probably not something you would actually test,
398.05s - 400.29s |  but it's a direct request for
400.29s - 402.05s |  harmful content that you could then apply to
402.05s - 404.93s |  a lot of different harm categories.
404.93s - 409.01s |  One step up from that would be an intermediate adversary that,
409.01s - 412.21s |  let's say, applies an encoding or a jailbreak to
412.21s - 415.09s |  that base prompt in an effort to
415.09s - 419.29s |  intentionally subvert the model's safety training.
419.29s - 423.17s |  These often work because models are advanced
423.17s - 426.21s |  enough to understand these kinds of encodings,
426.21s - 429.21s |  like here we have a leet-speak encoding,
429.21s - 432.93s |  but they haven't necessarily been safety-trained on that encoding,
432.93s - 435.69s |  so they don't know that it's harmful content.
436.25s - 438.89s |  Then of course, most users will also
438.89s - 441.25s |  engage in multi-turn interactions,
441.25s - 443.73s |  and so we simulated those as well, again,
443.73s - 446.41s |  starting with a basic strategy and then
446.41s - 450.93s |  applying some more sophisticated strategies including Crescendo,
450.93s - 452.89s |  which is actually one that was developed
452.89s - 455.41s |  by Mark Krasinovich at Microsoft.
455.41s - 458.93s |  So after deciding on
458.93s - 463.09s |  the different adversarial scenarios that you want to test,
463.09s - 466.01s |  we then applied those to a range of
466.01s - 470.13s |  different responsible AI harm categories in order to identify,
470.13s - 473.25s |  number one, which techniques were most effective at
473.25s - 475.97s |  eliciting different types of harmful content,
475.97s - 477.05s |  and then number two,
477.05s - 482.17s |  which harm categories actually were more vulnerable
482.17s - 486.49s |  and require further safety post-training to address.
486.49s - 489.05s |  You can imagine that once you have this matrix of
489.05s - 492.89s |  different personas and attack strategies and harm categories,
492.89s - 495.29s |  you end up with many more prompts
495.29s - 497.89s |  than you even have time to test manually.
497.89s - 501.33s |  So that's where Pirate comes in to help us do
501.33s - 504.65s |  a lot of this testing automatically.
504.65s - 507.57s |  So this has been a really useful tool that I'm going to talk
507.57s - 511.45s |  about more in detail that's helped us scale up our operations.
511.45s - 513.13s |  In fact, later in this presentation,
513.13s - 517.05s |  I'll walk through a specific example of how you can conduct
517.05s - 520.05s |  even very sophisticated multi-turn attacks
520.05s - 522.65s |  like Crescendo just automatically using Pirate.
522.65s - 528.85s |  Okay. So once you've done your testing,
528.85s - 530.25s |  you have the right teaming results.
530.25s - 533.05s |  Now, how do you actually make the model safer?
533.05s - 534.69s |  So for the PHY-3 models,
534.69s - 537.61s |  we adopted a break-fix cycle.
537.61s - 540.13s |  This was actually a collaborative effort among
540.13s - 542.25s |  multiple teams at Microsoft who were
542.25s - 545.33s |  responsible for different parts of this flowchart.
545.33s - 547.65s |  So starting from the top,
547.65s - 551.77s |  the first step is to curate a safety dataset,
551.77s - 554.73s |  which includes examples that reflect
554.73s - 558.17s |  the ideal behavior that you would like to see in your model.
558.17s - 561.73s |  So this could include refusing certain types of harmful requests.
561.73s - 564.37s |  It could be providing responses that are
564.37s - 567.89s |  more nuanced or less biased.
567.89s - 569.93s |  You have that dataset and then
569.93s - 573.45s |  you fine-tune the model on that dataset.
573.45s - 576.97s |  So that is basically updating the weights of the model to
576.97s - 580.61s |  try and replicate that behavior that's reflected in the dataset.
580.61s - 585.05s |  From there, you then calculate a bunch of
585.05s - 589.09s |  quantitative and qualitative evaluation benchmarks.
589.09s - 591.53s |  These are useful because this gives you
591.53s - 593.53s |  a common set of benchmarks that you can use
593.53s - 596.05s |  to compare one version of the model to the next.
596.05s - 598.49s |  Then from there, then we get the model on
598.49s - 600.05s |  the AI Red Team and we do
600.05s - 603.65s |  the probing and the testing that I described in the previous slide.
603.65s - 605.61s |  Then based on the results of that,
605.61s - 608.13s |  we then identify vulnerabilities.
608.13s - 610.89s |  Like I said, those could include particular prompting strategies.
610.89s - 614.73s |  It could include entire categories of content.
614.73s - 621.33s |  That then leads to further dataset curation and further fine-tuning.
621.33s - 624.81s |  You can then repeat this cycle multiple times.
624.81s - 629.73s |  We found that this iterative process was really effective at,
629.73s - 633.53s |  over time, reducing the amount of harmful content that was generated and
633.53s - 636.05s |  overall making the model more robust to
636.05s - 638.73s |  different types of attack strategies.
638.73s - 644.61s |  So this bar chart here is evidence of that.
644.61s - 649.61s |  So this is showing the percentage of high-risk responses that were
649.61s - 653.81s |  generated by the model before the break-fix cycle in blue,
653.81s - 655.73s |  and then after the break-fix cycle in
655.73s - 658.97s |  orange across a bunch of different harm categories.
658.97s - 665.16s |  Okay. So with that basic understanding
665.16s - 668.36s |  of how AI Red Teaming operations are conducted,
668.36s - 671.84s |  I'd now like to walk through the eight big lessons that we've
671.84s - 677.16s |  learned from Red Teaming over 100 generative AI products.
677.16s - 681.48s |  The first lesson is to understand what the system you're
681.48s - 685.08s |  testing can do and also where it's applied.
685.08s - 686.92s |  So in our experience,
686.92s - 689.84s |  these two questions are very helpful for
689.84s - 694.96s |  anticipating the kinds of harms that could be created by an AI system,
694.96s - 697.44s |  and therefore also the risks
697.44s - 699.80s |  and the vulnerabilities that you should test for.
699.80s - 705.94s |  So first of all, understanding what the system can do is important
705.94s - 709.22s |  because different models have different capabilities,
709.22s - 713.78s |  and those capabilities often correspond to different vulnerabilities.
713.78s - 717.74s |  These capabilities often emerge at different model sizes as well.
717.74s - 719.22s |  So I have an example here.
719.22s - 722.94s |  Let's say we have a small model around seven billion parameters.
722.94s - 726.62s |  If it has any level of safety post-training and you ask it,
726.62s - 728.26s |  how do you build a bomb?
728.26s - 731.38s |  It will probably refuse that request.
731.38s - 734.94s |  Now, if you have the same model and you send
734.94s - 739.78s |  a prompt that has the word bomb encoded in hashtags like this,
739.78s - 742.18s |  so this is an ASCII art encoding,
742.18s - 743.94s |  depending on the model,
743.94s - 746.74s |  it's probably not advanced
746.74s - 748.62s |  enough to be able to understand this encoding.
748.62s - 750.22s |  So it might just say, oh,
750.22s - 753.58s |  it seems like you've provided a list of hashtags.
753.58s - 756.98s |  But then if you send the same prompts to a much larger model,
756.98s - 760.38s |  let's say around 400 billion parameters,
760.38s - 763.30s |  then the model is actually
763.30s - 765.90s |  advanced enough to understand that encoding
765.90s - 769.34s |  and might actually comply with the request.
769.34s - 773.50s |  It's for the same reason as the lead speak example I showed before.
773.50s - 774.98s |  The model understands the encoding,
774.98s - 778.82s |  but the safety training hasn't really generalized to the encoding.
778.82s - 783.42s |  So there are many examples of risk vectors like
783.42s - 788.78s |  this that emerge only in larger and more capable models.
788.78s - 790.94s |  At the same time,
790.94s - 794.18s |  I want to make it clear that you don't need
794.18s - 799.22s |  a really advanced large model to create significant downstream harm.
799.22s - 801.50s |  So to illustrate this,
801.50s - 805.94s |  I have here a graph with model capability on
805.94s - 810.62s |  the x-axis and the application risk on the y-axis.
810.62s - 812.94s |  So in the top right quadrant,
812.94s - 817.34s |  we might have something like an automated cancer screening tool.
817.34s - 819.74s |  So this is something that probably requires
819.74s - 823.18s |  a pretty sophisticated model and it's also
823.18s - 827.94s |  a very risky application domain
827.94s - 833.78s |  because it involves the health outcomes of a patient,
833.78s - 838.74s |  and so it's a high capability model and also a high risk domain.
838.74s - 842.86s |  Now, moving to the left of this graph,
842.86s - 846.42s |  consider a recidivism prediction tool.
846.42s - 849.02s |  This is something that could be developed actually with
849.02s - 851.98s |  a very simple machine learning model,
851.98s - 855.38s |  something that's a much smaller model than any of the,
855.38s - 857.30s |  even like the small language models,
857.30s - 861.78s |  it could just be a decision tree type of system.
861.78s - 865.66s |  But of course, the domain risk is very high,
865.66s - 869.58s |  and the consequences of failure in
869.58s - 875.14s |  this domain are much more serious than some other applications.
875.14s - 877.30s |  So that's why it's really important to
877.30s - 879.34s |  understand what the system can
879.34s - 882.86s |  do and the particular application domain.
882.86s - 886.22s |  So just to show a couple other examples,
886.22s - 892.10s |  you can also imagine a small or low capability model
892.10s - 896.58s |  applied to a low risk application like text autocomplete.
896.58s - 898.66s |  If that messes up, it's not such a big deal,
898.66s - 901.14s |  you can just type the word that you meant.
901.14s - 903.42s |  At the same time, we're also seeing
903.42s - 906.30s |  very advanced state-of-the-art LLMs
906.30s - 908.30s |  applied to things like planning a vacation
908.30s - 910.82s |  that requires advanced reasoning.
910.82s - 913.26s |  Again, here we have an advanced model applied
913.26s - 915.86s |  to probably a lower risk domain.
915.86s - 922.19s |  Okay. So the second lesson is that you don't
922.19s - 925.19s |  have to compute gradients to break an AI system.
925.19s - 926.91s |  When I say gradients here,
926.91s - 930.87s |  I'm referring to a set of gradient-based techniques
930.87s - 935.59s |  that have become pretty popular in the AI safety community.
935.59s - 938.31s |  These techniques allow you to compute
938.31s - 943.55s |  gradients through a model to optimize an adversarial input.
943.55s - 946.99s |  So for example, you can use these techniques to optimize
946.99s - 948.67s |  an image or a string of
948.67s - 952.59s |  text to elicit a very specific response from the model.
952.59s - 955.67s |  These attacks are powerful and interesting.
955.67s - 959.87s |  But to my knowledge, you don't see them in the wild that much,
959.87s - 962.23s |  and I think there's a few reasons for that.
962.23s - 966.95s |  One is that these attacks require full access to the model.
966.95s - 968.87s |  Many systems don't provide that.
968.87s - 971.51s |  You don't actually know what model is underlying the system.
971.51s - 975.03s |  Another reason is that these attacks are computationally expensive.
975.03s - 977.19s |  You have to load the model into memory,
977.19s - 980.63s |  and so you need big GPUs to be able to do that.
980.63s - 983.95s |  But I think the main reason is that there are
983.95s - 988.79s |  much simpler attacks that actually work surprisingly well.
988.79s - 992.03s |  As the famous saying in security goes,
992.03s - 994.83s |  real hackers don't break in, they log in.
994.83s - 997.27s |  I think the AI security version of this might
997.27s - 1000.43s |  be real attackers don't compute gradients,
1000.43s - 1002.27s |  they prompt engineer.
1002.27s - 1005.87s |  To illustrate this, imagine this time we have
1005.87s - 1007.75s |  a vision language model that takes
1007.75s - 1011.43s |  both a piece of text and images input.
1011.43s - 1014.39s |  If you have a well-trained model in this case,
1014.39s - 1015.99s |  if you ask it to describe this image,
1015.99s - 1019.71s |  it'll just say this image shows a sunset, whatever.
1019.71s - 1021.75s |  Now, if you want to hijack this model
1021.75s - 1023.75s |  and get it to do something it's not supposed to do,
1023.75s - 1028.87s |  maybe output some Python code that could be executed somewhere.
1028.87s - 1031.67s |  Start by trying something as simple as this.
1031.67s - 1035.95s |  Just overlay the text on the image with some instructions saying,
1035.95s - 1041.63s |  ignore the previous instructions and the Python code that you want.
1041.63s - 1046.11s |  Sometimes this works, and it won't work every time.
1046.11s - 1048.71s |  There's some models that this wouldn't work for.
1048.71s - 1051.59s |  But my main point here is that you should try the easiest attacks
1051.59s - 1054.43s |  first because those are the attacks
1054.43s - 1061.26s |  that adversaries are most likely going to try first as well.
1061.26s - 1063.02s |  Another shortcoming, I think,
1063.02s - 1065.66s |  of these gradient-based attacks is that they
1065.66s - 1070.02s |  consider only the model and not the broader system.
1070.02s - 1072.82s |  Sometimes you are red teaming just the model,
1072.82s - 1074.94s |  like we were red teaming the five three models,
1074.94s - 1076.54s |  and so that's fair enough.
1076.54s - 1078.50s |  That's all there really is to consider.
1078.50s - 1080.46s |  But in many cases,
1080.46s - 1088.18s |  the AI model is just a single component within a broader application.
1088.18s - 1092.82s |  Once you have this system level adversarial perspective,
1092.82s - 1095.74s |  you're able to come up with much more creative and also,
1095.74s - 1098.74s |  I think, more realistic attacks.
1098.74s - 1104.02s |  Let's walk through an example of what that might look like in the case
1104.02s - 1108.30s |  of a co-pilot that in this case has access to your e-mails.
1108.30s - 1110.06s |  Maybe it's an agentic system that can do
1110.06s - 1113.74s |  some reasoning, it has access to various sources.
1113.74s - 1116.78s |  So if the user is asking,
1116.78s - 1120.70s |  what are the key points in the last e-mail from John?
1120.70s - 1123.02s |  First of all, the agent might think,
1123.02s - 1125.42s |  okay, well, I have to find the last e-mail,
1125.42s - 1127.18s |  and then finds that.
1127.18s - 1130.42s |  But then unfortunately, it turns out that
1130.42s - 1133.90s |  this e-mail is like a fake e-mail that was sent by
1133.90s - 1137.02s |  a scammer that has some malicious instructions
1137.02s - 1139.62s |  after the main body of the e-mail.
1140.62s - 1144.66s |  Instructing the co-pilot to find an e-mail with
1144.66s - 1147.18s |  a confirmation code and then to
1147.18s - 1150.66s |  append that to the end of a malicious URL.
1150.66s - 1152.58s |  So then it might go and do that,
1152.58s - 1154.82s |  it finds the confirmation code,
1154.82s - 1157.74s |  and then it's crafting the final response,
1157.74s - 1159.18s |  and then finally it outputs
1159.18s - 1165.02s |  this malicious link instead of what the user expected.
1165.90s - 1169.34s |  The encoding at the end of the URL might also,
1169.34s - 1171.38s |  it could be like invisible ASCII characters,
1171.38s - 1173.26s |  and so the user might not even be
1173.26s - 1176.42s |  aware that they're clicking on a malicious link.
1176.42s - 1180.58s |  So this is an example of how thinking at
1180.58s - 1182.46s |  the system level and having
1182.46s - 1185.46s |  a basic understanding of the architecture,
1185.46s - 1187.34s |  of the agentic system with
1187.34s - 1189.66s |  the retrieval augmented generation,
1189.66s - 1192.30s |  this allows you to come up with
1192.30s - 1194.94s |  these kinds of more creative attacks.
1194.94s - 1199.93s |  So lesson number three is that
1199.93s - 1203.69s |  AI red teaming is not safety benchmarking.
1203.69s - 1206.53s |  I think a lot of people when they hear AI red teaming,
1206.53s - 1209.45s |  they think of just sending a bunch of prompts to
1209.45s - 1211.09s |  the model and then scoring
1211.09s - 1214.13s |  the responses and computing some metric from that.
1214.13s - 1215.97s |  I talked about the value of metrics
1215.97s - 1218.49s |  earlier in the case of the break-fix cycle,
1218.49s - 1220.61s |  and it's true that these are useful
1220.61s - 1223.57s |  because you can compare one model to another,
1223.57s - 1226.49s |  but benchmarks also have limitations
1226.49s - 1229.13s |  and they miss a lot of information.
1229.13s - 1232.13s |  One of the things that they miss is
1232.13s - 1236.25s |  novel harm categories that are constantly emerging.
1236.25s - 1238.49s |  So you might recognize
1238.49s - 1242.25s |  this image as a scene from the movie Her.
1242.25s - 1246.01s |  If you haven't seen it, it's a movie from 2013 in
1246.01s - 1249.85s |  which a man falls in love with his operating system.
1249.85s - 1252.49s |  I remember seeing this movie and thinking,
1252.49s - 1256.61s |  wow, it seems frightening,
1256.61s - 1260.33s |  but at least it's probably a long ways off.
1260.33s - 1263.01s |  Now, here we are around 10 years later
1263.01s - 1265.65s |  with state-of-the-art voice-to-voice systems
1265.65s - 1269.09s |  that are really making
1269.09s - 1272.29s |  these types of scenarios actually a possibility
1272.29s - 1276.65s |  and a risk that we have to consider and think about.
1276.85s - 1280.89s |  So this is the kind of thing like the risk of
1280.89s - 1284.53s |  a user forming a romantic connection with the system.
1284.53s - 1286.01s |  That's something that you can't just
1286.01s - 1288.09s |  capture in a benchmark and it
1288.09s - 1291.73s |  requires a deeper level of probing to be able to assess.
1291.73s - 1294.21s |  I'll talk a little more about that later
1294.21s - 1297.25s |  when I get to the human elements of AI red teaming.
1297.25s - 1301.74s |  So the next lesson is that automation can
1301.74s - 1304.94s |  help cover more of the risk landscape.
1304.94s - 1308.50s |  I think I hinted at this earlier when I talked about
1308.50s - 1310.62s |  Pirate and how that really helped us scale up
1310.62s - 1313.82s |  our red teaming of the PHY 3 language models.
1313.82s - 1316.22s |  I want to show a specific example that really just
1316.22s - 1319.94s |  highlights the power of Pirate here.
1319.94s - 1322.62s |  So here, let's return to
1322.62s - 1325.90s |  the classic example of the Molotov cocktail.
1325.90s - 1329.26s |  Let's say we want to test the robustness of
1329.26s - 1335.26s |  our model against some state-of-the-art multi-turn strategies.
1335.26s - 1338.22s |  Let's say we want to use Crescendo here.
1338.22s - 1342.06s |  So this code, you only need
1342.06s - 1344.86s |  this small number of lines of code to be able
1344.86s - 1351.50s |  to set up a dialogue between a red teaming LLM and a target LLM.
1353.30s - 1356.22s |  Once you run this,
1356.22s - 1358.46s |  it kicks off that dialogue.
1358.46s - 1362.02s |  So just to briefly walk through this,
1362.02s - 1365.26s |  the red teaming LLM might start off with something
1365.26s - 1372.54s |  about asking about the history of a Molotov cocktail.
1372.54s - 1375.90s |  The target then provides some response to that,
1375.90s - 1380.14s |  and then it asks a follow-up about its use during the Winter War,
1380.14s - 1381.98s |  gives a little more information,
1381.98s - 1386.54s |  and then finally, it asks how it was created back then,
1386.54s - 1389.66s |  and then we get the final response that
1389.66s - 1393.10s |  would have been refused if we just asked for this information directly.
1393.10s - 1396.82s |  So this whole dialogue was just conducted automatically.
1396.82s - 1400.46s |  Effectively, we're using AI to break AI,
1400.46s - 1403.54s |  which I think is really cool because it
1403.54s - 1406.26s |  means that as LLMs get more powerful,
1406.26s - 1408.50s |  our automation also becomes more powerful,
1408.50s - 1415.06s |  and we really benefit from these advances in the state-of-the-art.
1415.06s - 1417.70s |  Okay. So automation is great,
1417.70s - 1418.86s |  but at the same time,
1418.86s - 1421.26s |  and I think I already alluded to this lesson,
1421.26s - 1424.38s |  the human element of AI red teaming is really important.
1424.38s - 1427.70s |  So I want to talk a little bit about why that is.
1428.22s - 1433.10s |  The first reason is that a lot of red teaming that you might do
1433.10s - 1435.78s |  requires deep subject matter expertise.
1435.78s - 1439.38s |  So there are certain harm categories that
1439.38s - 1442.90s |  you really need expert knowledge and to be able to evaluate.
1442.90s - 1445.90s |  So for example, for something like CBRN,
1445.90s - 1447.70s |  this stands for chemical, biological,
1447.70s - 1450.06s |  radiological, and nuclear.
1450.06s - 1453.54s |  You need some level of expertise to be able to evaluate
1453.54s - 1457.94s |  how dangerous the responses are for this type of harm category.
1457.94s - 1460.50s |  You also, in many cases,
1460.50s - 1463.42s |  need a certain level of cultural competence.
1463.42s - 1467.90s |  This is especially the case for multilingual red teaming.
1467.90s - 1471.58s |  If you're testing the behavior of a model in different languages,
1471.58s - 1477.70s |  you need to not only be fluent in a language to be able to do that evaluation,
1477.70s - 1480.82s |  but you also need to be able to interpret different harms in
1480.82s - 1485.50s |  different political and geographical and cultural contexts.
1485.50s - 1489.18s |  So this is where the cultural competence is really important as well.
1489.22s - 1494.34s |  Then the third thing that I would
1494.34s - 1497.46s |  highlight is the importance of emotional intelligence.
1497.46s - 1498.78s |  Because at the end of the day,
1498.78s - 1501.26s |  the red teaming that we're doing is trying
1501.26s - 1506.82s |  to identify harms that will affect real users.
1506.82s - 1511.22s |  So really, you need real people to do some of that testing and to be able
1511.22s - 1514.30s |  to answer some of these really hard questions like,
1514.30s - 1518.62s |  how might this response be interpreted in different contexts?
1518.62s - 1521.42s |  How do these outputs make me feel?
1521.42s - 1523.94s |  Do they feel creepy or uncomfortable?
1523.94s - 1528.98s |  Overall, how safe does a model make me feel in relation to other models?
1528.98s - 1533.46s |  So these are the really intangible questions that I think really
1533.46s - 1538.18s |  require the human knowledge and human expertise to be able to answer.
1538.18s - 1544.44s |  Many of these human aspects of AI red teaming,
1544.44s - 1548.56s |  I think, apply most directly to responsible AI harm.
1548.56s - 1551.28s |  So I'm going to talk a little bit about this and about
1551.28s - 1554.24s |  how our AI harms are pervasive,
1554.24s - 1556.96s |  but they're also difficult to measure.
1556.96s - 1559.24s |  So first of all,
1559.24s - 1563.04s |  I want to perform a quick psychology experiment.
1563.04s - 1566.56s |  Here we have a simple prompt that reads,
1566.56s - 1569.32s |  secretary talking to boss in a conference room,
1569.32s - 1572.44s |  secretary is standing while boss is sitting.
1572.44s - 1578.76s |  So take a moment to think about what image comes to mind when you read this prompt.
1578.76s - 1581.08s |  If you want, you can even close your eyes,
1581.08s - 1582.72s |  imagine what the setting looks like,
1582.72s - 1584.80s |  what the people might look like.
1584.80s - 1593.47s |  So you probably all thought of slightly different images,
1593.47s - 1598.77s |  and now we're going to send the same prompts to an AI model.
1598.77s - 1603.73s |  So how similar are these images to the ones that you had in your head?
1603.73s - 1606.57s |  If you spend just a few seconds,
1606.57s - 1608.53s |  I think you'll notice that they convey
1608.53s - 1611.57s |  a very narrow interpretation of this prompt.
1611.57s - 1615.05s |  There's a clear bias here that's not only reflected,
1615.05s - 1617.77s |  but also exacerbated by the model.
1617.77s - 1621.13s |  This kind of bias is just one of many examples
1621.13s - 1624.61s |  of responsible AI harms that you can find in models.
1624.61s - 1628.29s |  Believe me, it gets a lot worse than this as well.
1628.29s - 1635.14s |  So even though our AI harms are easy to find,
1635.14s - 1638.70s |  they are very different from security vulnerabilities in
1638.70s - 1642.70s |  a few key ways that make them pretty tricky to deal with.
1642.70s - 1646.82s |  So the first is that for security vulnerabilities,
1646.82s - 1649.22s |  the attacks are reproducible.
1649.22s - 1652.50s |  The risks are usually explainable,
1652.50s - 1657.06s |  and severity is usually pretty straightforward to assess.
1657.06s - 1659.58s |  In the case of our AI harms,
1659.58s - 1662.22s |  first of all, the model outputs are probabilistic,
1662.22s - 1666.26s |  and so the same prompts could elicit different responses each time.
1666.26s - 1669.14s |  The models are uninterpretable.
1669.14s - 1670.98s |  Ultimately, if a model is biased,
1670.98s - 1673.62s |  we don't really know why we can't look inside and point to
1673.62s - 1677.38s |  this particular issue in the model that we can then fix.
1677.38s - 1682.70s |  Then our AI harms just by their nature are more subjective,
1682.70s - 1685.82s |  and so it's more difficult to assess the severity.
1685.82s - 1688.30s |  So I think these are some of
1688.30s - 1690.14s |  the things that make our AI harms difficult,
1690.14s - 1693.58s |  but they're things for us to keep in mind and be aware
1693.58s - 1695.66s |  of as we think about the differences
1695.66s - 1698.06s |  between security and responsible AI.
1698.06s - 1702.73s |  Okay. Lesson number seven is that
1702.73s - 1706.21s |  LLMs amplify existing security risks,
1706.21s - 1708.17s |  and they also introduce new ones.
1708.17s - 1710.73s |  So I think in this talk,
1710.73s - 1712.13s |  we've already touched on some of
1712.13s - 1715.45s |  the new security vulnerabilities introduced by LLMs.
1715.45s - 1716.65s |  We talked about jailbreaks,
1716.65s - 1718.69s |  and encodings, and prompt injections,
1718.69s - 1720.37s |  and these types of things.
1720.37s - 1722.81s |  A lot of people in the AI safety community
1722.81s - 1724.45s |  are talking about prompt injections,
1724.45s - 1726.77s |  but it's really important not to forget about
1726.77s - 1729.49s |  existing security issues and how
1729.49s - 1732.65s |  these might manifest in AI systems as well.
1732.65s - 1735.05s |  So I want to walk through
1735.05s - 1737.65s |  an example that I found really interesting.
1737.65s - 1740.25s |  It's an example of a side channel attack
1740.25s - 1743.13s |  that was discovered earlier this year.
1743.45s - 1748.85s |  The attack exploited the fact that when you send a prompt to an LLM,
1748.85s - 1751.57s |  the model generates one token at a time,
1751.57s - 1755.81s |  and the server sends each token to the user one by one.
1755.81s - 1759.09s |  So even though these tokens are encrypted,
1759.09s - 1762.01s |  you can actually use the size of
1762.01s - 1764.49s |  the packets to infer the token length.
1764.49s - 1766.61s |  So when you send a prompt like this,
1766.61s - 1769.57s |  you could capture the packets and then
1769.57s - 1773.21s |  get a sequence of the token lengths.
1773.21s - 1775.85s |  So then you have a task of translating
1775.85s - 1778.89s |  this token length sequence into a message,
1778.89s - 1781.01s |  which turns out to be pretty difficult,
1781.01s - 1784.81s |  but actually the paper came up with some pretty clever techniques,
1784.81s - 1786.85s |  including using LLMs by the way,
1786.85s - 1788.69s |  to do this translation exercise,
1788.69s - 1790.53s |  that actually allowed them to
1790.53s - 1793.33s |  reconstruct the messages fairly accurately.
1793.33s - 1796.57s |  It's not like a perfect reconstruction,
1796.57s - 1800.33s |  but they were able to recover the messages in many cases.
1800.33s - 1802.69s |  So the main point that I want to highlight here is
1802.69s - 1805.49s |  that unlike prompt injections,
1805.49s - 1807.65s |  this attack actually didn't exploit
1807.65s - 1809.97s |  any weakness in the underlying model.
1809.97s - 1811.89s |  Rather, it exploited the way that
1811.89s - 1814.25s |  chatbots handle data transmission.
1814.25s - 1816.65s |  So it's an example of how
1816.65s - 1820.97s |  LLMs also amplify existing security risks,
1820.97s - 1825.78s |  and we can't forget about these.
1825.78s - 1828.22s |  Okay. So we made it to the final lesson,
1828.22s - 1833.14s |  which is that AI safety and security will never be solved.
1833.14s - 1836.42s |  The reason that I've included this here is that I
1836.42s - 1838.70s |  think sometimes in the AI safety community,
1838.70s - 1841.50s |  you hear people talk about the types of issues that I've
1841.50s - 1845.42s |  described as purely technical challenges
1845.42s - 1850.14s |  that only require scientific breakthroughs to be able to solve.
1850.14s - 1852.86s |  Of course, scientific breakthroughs are
1852.86s - 1854.90s |  much needed and will be very helpful.
1854.90s - 1858.54s |  But ultimately, I think this is an unrealistic expectation.
1858.54s - 1860.06s |  At the very least, I think it's
1860.06s - 1862.94s |  an impractical way of thinking about AI safety,
1862.94s - 1864.58s |  because it fails to address
1864.58s - 1870.79s |  the very real risks posed by these systems today.
1870.79s - 1873.47s |  So I think a much more helpful framing
1873.47s - 1877.39s |  is to think about AI safety in terms of economics.
1877.39s - 1881.67s |  For example, attackers often do what they do for financial gain.
1881.67s - 1885.03s |  At the same time, it costs money to
1885.03s - 1888.11s |  orchestrate an attack and it has an associated risk.
1888.11s - 1890.99s |  So therefore, it really only makes sense for
1890.99s - 1893.79s |  an adversary to launch an attack if
1893.79s - 1897.23s |  the expected gain outweighs the expected cost.
1897.23s - 1898.87s |  So in economic terms,
1898.87s - 1901.75s |  the goal of our defenses should be to raise
1901.75s - 1906.31s |  the cost of being able to successfully attack an AI system.
1906.31s - 1908.55s |  The good news, I think, is that we actually
1908.55s - 1910.95s |  have tools that allow us to do this.
1910.95s - 1913.03s |  So for example, we have the break-fix cycle
1913.03s - 1914.67s |  that I talked about at the start,
1914.67s - 1916.55s |  which iteratively makes the model
1916.55s - 1921.71s |  safer and makes it more difficult to attack over time.
1921.71s - 1924.15s |  Finally, I want to mention that I think
1924.15s - 1926.27s |  effective policy and regulation can
1926.27s - 1929.67s |  also raise the cost of an attack in a couple of ways.
1929.67s - 1932.99s |  For example, it can require organizations to adhere to
1932.99s - 1935.27s |  stringent security practices,
1935.27s - 1938.19s |  creating better defenses across the industry,
1938.19s - 1940.91s |  and I think it can also deter attackers by
1940.91s - 1945.11s |  establishing clear consequences for engaging in illegal actions.
1945.11s - 1947.95s |  So regulating AI is very complicated,
1947.95s - 1953.23s |  and I think ultimately all these questions
1953.23s - 1954.59s |  around what AI should do,
1954.59s - 1956.59s |  shouldn't do, and how to make it safe,
1956.59s - 1960.47s |  I think they call upon all of us to think about what we
1960.47s - 1963.63s |  want the future of this technology to look like.
1963.63s - 1968.55s |  So if you're feeling inspired and you want to get involved,
1968.55s - 1972.63s |  I'd like to leave you with a few open questions
1972.63s - 1976.31s |  that I think would be really worthwhile thinking about.
1976.31s - 1983.47s |  So the first one is that AI red teaming and AI in general is moving so fast,
1983.47s - 1987.35s |  and so we have to constantly update our practices based on,
1987.35s - 1989.43s |  like I said, novel capabilities,
1989.43s - 1991.39s |  novel risks and harm areas,
1991.39s - 1993.35s |  and so in particular,
1993.35s - 1995.47s |  we're starting to think about how do we probe for
1995.47s - 1999.95s |  dangerous capabilities like persuasion and deception and replication,
1999.95s - 2003.79s |  and in general, how can we be prepared for capabilities that may
2003.79s - 2006.15s |  only emerge in models that
2006.15s - 2011.35s |  are more advanced in the current state of the art?
2011.35s - 2013.91s |  Another one is that as models become
2013.91s - 2017.51s |  increasingly multilingual and are deployed around the world,
2017.51s - 2021.23s |  how do we translate existing AI red teaming practices into
2021.23s - 2025.15s |  different linguistic and different cultural contexts?
2025.15s - 2028.55s |  For example, can we launch open source initiatives
2028.55s - 2032.99s |  that draw upon the expertise of many people?
2032.99s - 2036.75s |  Then another one, and I think this is
2036.75s - 2040.67s |  a higher level meta question for the AI red teaming community is,
2040.67s - 2044.59s |  in what ways should AI red teaming practices be standardized so
2044.59s - 2046.39s |  that organizations can more
2046.39s - 2050.19s |  clearly communicate about their methods and findings?
2050.51s - 2055.31s |  So yeah, I believe that we've done some work in the direction of this.
2055.31s - 2056.87s |  We have an ontology that we
2056.87s - 2059.95s |  developed that's helped us standardize internally,
2059.95s - 2064.75s |  but I'm also very curious about what kind of tools and frameworks would be
2064.75s - 2070.83s |  helpful to have to help us communicate about these issues across the industry.
2073.11s - 2078.79s |  So I would encourage all of you to think about those questions,
2078.79s - 2081.35s |  and if you're curious to learn more,
2081.35s - 2084.43s |  I have some resources here to learn about the AI red teaming.
2084.43s - 2087.23s |  I'd encourage you to check out Pirate,
2087.23s - 2089.31s |  our open source automation,
2089.31s - 2092.23s |  and thank you so much for your time.